import { Problem } from '../../types';

export const DM_137_STABLE_SOFTMAX_CROSS_ENTROPY_LOSS_AND_GRADIENT: Problem = {
  "id": "dm_137_stable_softmax_cross_entropy_loss_and_gradient",
  "title": "Stable Softmax Cross-Entropy Loss and Gradient",
  "difficulty": "Medium",
  "tags": [
    "Probability",
    "Loss Functions",
    "Activation Functions",
    "Backpropagation",
    "Matrix Operations"
  ],
  "descriptionMarkdown": "Implement a numerically stable Softmax and Cross-Entropy loss for a batch of classification logits, and derive the gradient with respect to the logits without using autograd.\n\nGiven:\n- A logits tensor of shape (N, C) where N is the batch size and C is the number of classes.\n- A target tensor of shape (N,) with integer class indices in [0, C-1].\n\nTasks:\n1. Compute the softmax probabilities in a numerically stable way (use the log-sum-exp trick).\n2. Compute the average cross-entropy loss over the batch.\n3. Compute the gradient of the average loss with respect to the logits. Do not rely on autograd for this gradient; derive it analytically using vectorized PyTorch tensor operations.\n\nReturn the scalar loss and the gradient tensor with the same shape as the logits.",
  "solutionExplanation": "The softmax function converts logits z_i into probabilities p_i = exp(z_i) / sum_j exp(z_j). Directly exponentiating logits can be numerically unstable, so we use the log-sum-exp trick: subtract the per-row maximum m = max_j z_j before exponentiating. This preserves the probabilities while preventing overflow. In log form, log p_i = z_i - logsumexp(z), where logsumexp(z) = m + log(sum_j exp(z_j - m)).\n\nThe cross-entropy loss for a single example with true class y is L = -log p_y. For a batch, we take the mean over N samples. The gradient of the average cross-entropy loss with respect to the logits is well-known: dL/dz = (p - one_hot(y)) / N, where p is the softmax probability matrix and one_hot(y) is the one-hot encoding of the targets. This result follows from differentiating the softmax and the negative log-likelihood; the derivation collapses to p - y_one_hot due to convenient cancellations.\n\nWe implement this entirely with PyTorch tensor operations without using autograd to compute the analytical gradient. We also provide a validation snippet comparing the analytical gradient to PyTorch's autograd gradient for correctness.",
  "solutionCode": "import torch\nimport torch.nn.functional as F\n\n\ndef softmax_cross_entropy_loss_and_grad(logits: torch.Tensor, targets: torch.Tensor):\n    \"\"\"\n    Compute numerically stable softmax probabilities, average cross-entropy loss,\n    and the gradient of the loss w.r.t. logits without using autograd.\n\n    Args:\n        logits: Float tensor of shape (N, C)\n        targets: Long tensor of shape (N,), values in [0, C-1]\n\n    Returns:\n        loss: Scalar tensor, mean cross-entropy loss over the batch\n        grad: Float tensor of shape (N, C), gradient d(loss)/d(logits)\n        probs: Float tensor of shape (N, C), softmax probabilities (optional for inspection)\n    \"\"\"\n    if logits.dim() != 2:\n        raise ValueError(f\"logits must be 2D (N, C); got shape {tuple(logits.shape)}\")\n    if targets.dim() != 1:\n        raise ValueError(f\"targets must be 1D (N,); got shape {tuple(targets.shape)}\")\n    if logits.size(0) != targets.size(0):\n        raise ValueError(\"batch size mismatch between logits and targets\")\n    if targets.dtype != torch.long:\n        raise TypeError(\"targets must be of dtype torch.long with class indices\")\n\n    N, C = logits.shape\n\n    # Numerically stable log-softmax using log-sum-exp trick\n    # Subtract per-row max for stability\n    max_per_row = logits.max(dim=1, keepdim=True).values\n    shifted = logits - max_per_row\n    # logsumexp = m + log(sum(exp(z - m)))\n    logsumexp = max_per_row + torch.log(torch.sum(torch.exp(shifted), dim=1, keepdim=True))\n    log_probs = logits - logsumexp  # equivalent to log_softmax\n    probs = torch.exp(log_probs)\n\n    # Negative log-likelihood for the true class\n    nll = -log_probs[torch.arange(N, device=logits.device), targets]\n    loss = nll.mean()\n\n    # Gradient: (probs - one_hot(target)) / N\n    grad = probs.clone()\n    grad[torch.arange(N, device=logits.device), targets] -= 1.0\n    grad /= N\n\n    return loss, grad, probs\n\n\ndef solution():\n    \"\"\"\n    Example usage and validation against PyTorch's CrossEntropyLoss + autograd.\n    \"\"\"\n    torch.manual_seed(42)\n\n    N, C = 4, 5\n    logits = torch.randn(N, C, dtype=torch.float64)  # use float64 for tighter numerical check\n    targets = torch.randint(low=0, high=C, size=(N,), dtype=torch.long)\n\n    # Analytical computation (no autograd)\n    loss_analytical, grad_analytical, probs = softmax_cross_entropy_loss_and_grad(logits, targets)\n\n    # Autograd reference\n    logits_ref = logits.clone().detach().requires_grad_(True)\n    loss_ref = F.cross_entropy(logits_ref, targets, reduction='mean')\n    loss_ref.backward()\n\n    # Compare losses and gradients\n    loss_diff = (loss_analytical - loss_ref.detach()).abs().item()\n    grad_diff = (grad_analytical - logits_ref.grad).abs().max().item()\n\n    print(\"Targets:\", targets.tolist())\n    print(\"Loss (analytical):\", float(loss_analytical))\n    print(\"Loss (autograd):   \", float(loss_ref.detach()))\n    print(\"Max |grad diff|:   \", grad_diff)\n    print(\"|loss diff|:       \", loss_diff)\n\n    # Return values for potential downstream checks\n    return {\n        'loss_analytical': loss_analytical.detach(),\n        'loss_autograd': loss_ref.detach(),\n        'max_grad_abs_diff': grad_diff,\n        'probs': probs.detach(),\n    }\n\n\nif __name__ == \"__main__\":\n    _ = solution()\n",
  "timeComplexity": "O(N*C)",
  "spaceComplexity": "O(N*C)",
  "platform": "deepml"
};
