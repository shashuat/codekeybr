import { Problem } from '../../types';

export const DM_57_GAUSS_SEIDEL_METHOD_FOR_SOLVING_LINEAR_SYSTEMS: Problem = {
  "id": "dm_57_gauss_seidel_method_for_solving_linear_systems",
  "title": "Gauss-Seidel Method for Solving Linear Systems",
  "difficulty": "Medium",
  "tags": [
    "Linear Algebra",
    "Matrix Operations",
    "Optimization"
  ],
  "descriptionMarkdown": "Task: Implement the Gauss-Seidel Method\n\nYour task is to implement the Gauss-Seidel method, an iterative technique for solving a system of linear equations (Ax = b). The function should iteratively update the solution vector x by using the most recent values available during the iteration process.\n\nWrite a function `gauss_seidel(A, b, n, x_ini=None)` where:\n- A is a square matrix of coefficients,\n- b is the right-hand side vector,\n- n is the number of iterations,\n- x_ini is an optional initial guess for x (if not provided, assume a vector of zeros).\n\nThe function should return the approximated solution vector x after performing the specified number of iterations.\n\nExample:\n\nInput:\n- A = [[4, 1, 2], [3, 5, 1], [1, 1, 3]]\n- b = [4, 7, 3]\n- n = 100\n\nOutput (approximate):\n- [0.2, 1.4, 0.8]  (Approximate, values may vary depending on iterations)\n\nReasoning:\nThe Gauss-Seidel method iteratively updates the solution vector until convergence. The output is an approximate solution to the linear system.",
  "solutionExplanation": "The Gauss-Seidel method solves Ax = b by iteratively refining x using the most recent updates within each iteration. For each variable i, the update uses the newest values x_j for j < i (updated earlier in the same iteration) and the previous iteration's values for j > i. The update rule is:\n\nx_i^(k+1) = (b_i - sum_{j<i} a_ij x_j^(k+1) - sum_{j>i} a_ij x_j^(k)) / a_ii\n\nThis leads to faster convergence than the Jacobi method in many cases because it immediately incorporates fresh information. Convergence is guaranteed for certain matrix classes, such as strictly diagonally dominant or symmetric positive definite matrices. In practice, you either run a fixed number of iterations or stop early when the change between successive iterates falls below a tolerance.\n\nThe implementation carefully handles tensor types and devices with PyTorch, checks dimensions and diagonal elements to avoid division by zero, and performs in-place updates per row using torch.dot for efficient dense operations. Although we use a fixed number of iterations as required, the structure allows easy extension to add tolerances or residual checks.",
  "solutionCode": "import torch\nimport torch.nn as nn\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solve Ax = b using the Gauss-Seidel iterative method for a fixed number of iterations.\n\n    Args:\n        A (Tensor or array-like): Square coefficient matrix of shape (N, N).\n        b (Tensor or array-like): Right-hand side vector of shape (N,).\n        n (int): Number of Gauss-Seidel iterations to perform.\n        x_ini (Tensor or array-like, optional): Initial guess for x. If None, zeros are used.\n\n    Returns:\n        Tensor: Approximated solution vector x of shape (N,) on the same device/dtype as A.\n    \"\"\"\n    # Convert inputs to tensors and align dtype/device\n    A = torch.as_tensor(A)\n    b = torch.as_tensor(b)\n\n    if A.dim() != 2 or A.size(0) != A.size(1):\n        raise ValueError(\"A must be a square 2D tensor of shape (N, N)\")\n    if b.dim() != 1 or b.numel() != A.size(0):\n        raise ValueError(\"b must be a 1D tensor with length equal to A.size(0)\")\n\n    # Ensure floating types\n    if not A.is_floating_point():\n        A = A.to(dtype=torch.get_default_dtype())\n    # Match b to A's dtype/device\n    b = b.to(device=A.device, dtype=A.dtype)\n\n    N = A.size(0)\n\n    # Initialize x\n    if x_ini is None:\n        x = torch.zeros(N, device=A.device, dtype=A.dtype)\n    else:\n        x = torch.as_tensor(x_ini, device=A.device, dtype=A.dtype).clone()\n        if x.dim() != 1 or x.numel() != N:\n            raise ValueError(\"x_ini must be a 1D tensor with length equal to A.size(0)\")\n\n    # Check for zeros on the diagonal to prevent division by zero\n    if torch.any(A.diag() == 0):\n        raise ValueError(\"Zero detected on the diagonal of A; Gauss-Seidel cannot proceed.\")\n\n    # Perform iterations (no autograd required)\n    with torch.no_grad():\n        for _ in range(int(n)):\n            x_old = x.clone()\n            for i in range(N):\n                # Compute sums using latest x for j < i and previous x for j > i\n                if i > 0:\n                    sum_lower = torch.dot(A[i, :i], x[:i])\n                else:\n                    sum_lower = torch.tensor(0.0, device=A.device, dtype=A.dtype)\n\n                if i + 1 < N:\n                    sum_upper = torch.dot(A[i, i+1:], x_old[i+1:])\n                else:\n                    sum_upper = torch.tensor(0.0, device=A.device, dtype=A.dtype)\n\n                x[i] = (b[i] - sum_lower - sum_upper) / A[i, i]\n    return x\n\n# Example usage\nif __name__ == \"__main__\":\n    A = torch.tensor([[4.0, 1.0, 2.0],\n                      [3.0, 5.0, 1.0],\n                      [1.0, 1.0, 3.0]])\n    b = torch.tensor([4.0, 7.0, 3.0])\n    n = 100\n    x = gauss_seidel(A, b, n)\n    print(\"Approximate solution:\", x)\n    # Check residual (should be small for sufficient iterations)\n    r = A @ x - b\n    print(\"Residual norm:\", torch.norm(r).item())\n",
  "timeComplexity": "O(T * N^2) where N is the number of variables and T is the number of iterations",
  "spaceComplexity": "O(N) additional space (excluding storage for A and b)",
  "platform": "deepml"
};
