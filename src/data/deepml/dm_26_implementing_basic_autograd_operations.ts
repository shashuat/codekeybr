import { Problem } from '../../types';

export const DM_26_IMPLEMENTING_BASIC_AUTOGRAD_OPERATIONS: Problem = {
  "id": "dm_26_implementing_basic_autograd_operations",
  "title": "Implementing Basic Autograd Operations",
  "difficulty": "Medium",
  "tags": [
    "Neural Networks",
    "Activation Functions",
    "Backpropagation"
  ],
  "descriptionMarkdown": "Implement a Python class similar to the \"Value\" class (inspired by micrograd) that supports scalar automatic differentiation for basic operations: addition, multiplication, and ReLU activation. The class should build a computation graph during the forward pass and compute gradients via backpropagation.\n\nExample:\n\nInput:\n\na = Value(2)\n\nb = Value(-3)\n\nc = Value(10)\n\nd = a + b * c\n\ne = d.relu()\n\ne.backward()\n\nprint(a, b, c, d, e)\n\nOutput:\n\nValue(data=2, grad=0) Value(data=-3, grad=0) Value(data=10, grad=0)\n\nReasoning:\n\nb * c = -30, then d = a + (-30) = -28. The ReLU on d produces 0, so its output and gradient are zero, causing all upstream gradients to be zero.",
  "solutionExplanation": "We construct a lightweight scalar autograd engine by defining a Value class that stores a scalar (implemented as a 0-D PyTorch tensor), its gradient, and a reference to the operation that created it. Each forward operation (addition, multiplication, ReLU) creates a new Value node and records a local backward function that captures how to propagate gradients to its parents.\n\nTo compute gradients, we perform a reverse-mode automatic differentiation pass. We first topologically sort the computation graph (via DFS) to ensure children are visited before their parents. We initialize the target node\u2019s gradient to 1 and then traverse the nodes in reverse topological order, invoking each node\u2019s stored _backward function. The local gradient rules follow the chain rule: for addition, gradients pass unchanged to both inputs; for multiplication, they are scaled by the other operand; and for ReLU, gradients pass through only if the input was positive (zero otherwise).\n\nBy storing scalar data as 0-D PyTorch tensors and using torch operations (e.g., torch.relu), the implementation remains consistent with PyTorch semantics while keeping the autograd logic explicit and educational. This mirrors micrograd\u2019s design but leverages PyTorch tensors for numerical operations.",
  "solutionCode": "import torch\n\ndef solution():\n    class Value:\n        def __init__(self, data, _children=(), _op=''):\n            # Store data as a 0-D PyTorch tensor\n            if isinstance(data, torch.Tensor):\n                assert data.dim() == 0, \"Value expects a 0-D tensor (scalar).\"\n                self.data = data.clone().detach().to(dtype=torch.float32)\n            else:\n                self.data = torch.tensor(float(data), dtype=torch.float32)\n            self.grad = 0.0  # gradient as Python float for readability\n            self._backward = lambda: None\n            self._prev = set(_children)\n            self._op = _op  # name of the operation that produced this node (for debugging)\n\n        def __repr__(self):\n            v = float(self.data.item())\n            g = float(self.grad)\n            # Pretty print ints when possible\n            def fmt(x):\n                ix = int(round(x))\n                return ix if abs(x - ix) < 1e-9 else x\n            return f\"Value(data={fmt(v)}, grad={fmt(g)})\"\n\n        def __add__(self, other):\n            other = other if isinstance(other, Value) else Value(other)\n            out = Value(self.data + other.data, (self, other), '+')\n\n            def _backward():\n                # d(out)/d(self) = 1, d(out)/d(other) = 1\n                self.grad += 1.0 * out.grad\n                other.grad += 1.0 * out.grad\n            out._backward = _backward\n            return out\n\n        def __radd__(self, other):\n            return self + other\n\n        def __mul__(self, other):\n            other = other if isinstance(other, Value) else Value(other)\n            out = Value(self.data * other.data, (self, other), '*')\n\n            def _backward():\n                # d(self*other)/d(self) = other, and vice versa\n                self.grad += float(other.data.item()) * out.grad\n                other.grad += float(self.data.item()) * out.grad\n            out._backward = _backward\n            return out\n\n        def __rmul__(self, other):\n            return self * other\n\n        def relu(self):\n            out = Value(torch.relu(self.data), (self,), 'ReLU')\n\n            def _backward():\n                self.grad += (1.0 if self.data.item() > 0.0 else 0.0) * out.grad\n            out._backward = _backward\n            return out\n\n        def backward(self):\n            # Topological order (post-order DFS)\n            topo = []\n            visited = set()\n\n            def build_topo(v):\n                if v not in visited:\n                    visited.add(v)\n                    for child in v._prev:\n                        build_topo(child)\n                    topo.append(v)\n\n            build_topo(self)\n            # Seed gradient\n            self.grad = 1.0\n            # Backpropagate\n            for node in reversed(topo):\n                node._backward()\n\n    # Example usage from the problem statement\n    a = Value(2)\n    b = Value(-3)\n    c = Value(10)\n    d = a + b * c\n    e = d.relu()\n    e.backward()\n    print(a, b, c, d, e)\n\nif __name__ == \"__main__\":\n    solution()",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(N)",
  "platform": "deepml"
};
