import { Problem } from '../../types';

export const DM_43_IMPLEMENT_RIDGE_REGRESSION_LOSS_FUNCTION: Problem = {
  "id": "dm_43_implement_ridge_regression_loss_function",
  "title": "Implement Ridge Regression Loss Function",
  "difficulty": "Easy",
  "tags": [
    "Linear Algebra",
    "Loss Functions",
    "Regularization",
    "Matrix Operations",
    "Optimization"
  ],
  "descriptionMarkdown": "Write a Python function `ridge_loss` that implements the Ridge Regression loss function. The function should take:\n- `X`: a 2D array representing the feature matrix (shape: n \u00d7 d)\n- `w`: a 1D array representing the coefficients (shape: d)\n- `y_true`: a 1D array representing the true labels (shape: n)\n- `alpha`: a float representing the regularization parameter\n\nReturn the Ridge loss, which combines the Mean Squared Error (MSE) and an L2 regularization term:\n\nL(w) = MSE(Xw, y) + alpha * ||w||^2\n\nExample:\n\nInput:\n\n```\nimport numpy as np\nX = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\nw = np.array([0.2, 2])\ny_true = np.array([2, 3, 4, 5])\nalpha = 0.1\nloss = ridge_loss(X, w, y_true, alpha)\nprint(loss)\n```\n\nOutput:\n\n```\n2.204\n```\n\nReasoning: The Ridge loss is computed as the mean squared error between predictions and true labels plus the L2 penalty on the weights scaled by `alpha`.",
  "solutionExplanation": "Ridge regression augments the standard mean squared error (MSE) objective with an L2 penalty on the weights. Given feature matrix X (n\u00d7d), weights w (d), and target y (n), predictions are y_hat = Xw. The MSE is mean((y_hat \u2212 y)^2). The L2 penalty is alpha * ||w||^2, where ||w||^2 is the squared Euclidean norm of w. The total Ridge loss is MSE + alpha * ||w||^2.\n\nThis penalty discourages large weights, improving generalization and mitigating multicollinearity. In the provided example, the loss is computed as MSE = 1.8 and regularization = 0.1 * 4.04 = 0.404, yielding 2.204. The implementation uses vectorized PyTorch operations for efficiency and converts NumPy inputs to tensors as needed.",
  "solutionCode": "import torch\nfrom typing import Union\n\nTensorLike = Union[torch.Tensor, \"numpy.ndarray\", list]\n\ndef ridge_loss(X: TensorLike, w: TensorLike, y_true: TensorLike, alpha: float) -> float:\n    \"\"\"Compute Ridge Regression loss: MSE(Xw, y) + alpha * ||w||^2.\n\n    Args:\n        X: Feature matrix of shape (n, d). Accepts torch.Tensor, numpy array, or list.\n        w: Coefficient vector of shape (d,).\n        y_true: Target vector of shape (n,).\n        alpha: Non-negative regularization strength.\n\n    Returns:\n        Float loss value.\n    \"\"\"\n    if alpha < 0:\n        raise ValueError(\"alpha must be non-negative\")\n\n    # Convert inputs to float64 tensors for numerical stability and reproducibility\n    X_t = torch.as_tensor(X, dtype=torch.float64)\n    w_t = torch.as_tensor(w, dtype=torch.float64)\n    y_t = torch.as_tensor(y_true, dtype=torch.float64)\n\n    # Validate dimensions\n    if X_t.dim() != 2:\n        raise ValueError(f\"X must be 2D, got shape {tuple(X_t.shape)}\")\n    if w_t.dim() != 1:\n        raise ValueError(f\"w must be 1D, got shape {tuple(w_t.shape)}\")\n    if y_t.dim() != 1:\n        raise ValueError(f\"y_true must be 1D, got shape {tuple(y_t.shape)}\")\n    n, d = X_t.shape\n    if w_t.shape[0] != d:\n        raise ValueError(f\"w shape {tuple(w_t.shape)} is incompatible with X shape {tuple(X_t.shape)}\")\n    if y_t.shape[0] != n:\n        raise ValueError(f\"y_true length {y_t.shape[0]} must match number of rows in X ({n})\")\n\n    # Predictions and MSE\n    y_pred = X_t.matmul(w_t)  # shape: (n,)\n    mse = torch.mean((y_pred - y_t).pow(2))\n\n    # L2 regularization on all weights (including bias if present in w)\n    reg = alpha * torch.sum(w_t.pow(2))\n\n    loss = mse + reg\n    return float(loss.item())\n\n\ndef solution():\n    # Example usage reproducing the prompt example\n    import numpy as np\n    X = np.array([[1, 1], [2, 1], [3, 1], [4, 1]], dtype=float)\n    w = np.array([0.2, 2.0], dtype=float)\n    y_true = np.array([2, 3, 4, 5], dtype=float)\n    alpha = 0.1\n\n    loss = ridge_loss(X, w, y_true, alpha)\n    print(loss)  # Expected: 2.204\n",
  "timeComplexity": "O(n*d)",
  "spaceComplexity": "O(n)",
  "platform": "deepml"
};
