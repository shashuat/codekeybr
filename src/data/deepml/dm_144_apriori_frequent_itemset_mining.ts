import { Problem } from '../../types';

export const DM_144_APRIORI_FREQUENT_ITEMSET_MINING: Problem = {
  "id": "dm_144_apriori_frequent_itemset_mining",
  "title": "Apriori Frequent Itemset Mining",
  "difficulty": "Medium",
  "tags": [
    "Probability",
    "Matrix Operations"
  ],
  "descriptionMarkdown": "Implement the Apriori algorithm to discover all frequent itemsets in a set of transactions, given a minimum support threshold. Your function should return all frequent itemsets (of any size) whose support (fraction of transactions containing the itemset) is at least the given minimum.\n\nReturn the frequent itemsets as a dictionary mapping frozenset of items to their support (float in [0, 1]). Optionally, allow limiting the maximum size of itemsets via a max_length parameter.\n\nExample\n- Input:\n  - transactions = [\n    {'bread', 'milk'},\n    {'bread', 'diaper', 'beer', 'eggs'},\n    {'milk', 'diaper', 'beer', 'cola'},\n    {'bread', 'milk', 'diaper', 'beer'},\n    {'bread', 'milk', 'diaper', 'cola'}\n  ]\n  - min_support = 0.6\n- Output (one possible ordering):\n  - ['bread'] 0.8\n  - ['diaper'] 0.8\n  - ['milk'] 0.8\n  - ['bread', 'diaper'] 0.6\n  - ['bread', 'milk'] 0.6\n  - ['milk', 'diaper'] 0.6\n\nReasoning: Bread, Milk, and Diaper each appear in 4 out of 5 transactions (support = 0.8), and the 2-itemsets {'bread','milk'}, {'bread','diaper'}, and {'milk','diaper'} each appear in 3 out of 5 (support = 0.6). Only these satisfy the min_support threshold.",
  "solutionExplanation": "Apriori is a classical level-wise algorithm for frequent itemset mining. It works by iteratively generating candidate itemsets of increasing size k, starting from singletons. At level k, candidates are formed by joining frequent (k\u22121)-itemsets that share a common (k\u22122)-length prefix. Then, using the Apriori property (all subsets of a frequent itemset must be frequent), we prune any candidate whose (k\u22121)-subsets are not all frequent, greatly reducing the number of candidates to count.\n\nSupport counting is the main computational step. We map items to column indices and build a boolean transaction matrix X of shape [num_transactions, num_items]. For a candidate k-itemset, its support count equals the number of rows in which all k corresponding columns are True. We implement this efficiently with PyTorch by gathering the relevant columns for many candidates at once and applying a boolean all operation across the item dimension, then summing across transactions. This leverages vectorized tensor operations for speed while preserving the exact Apriori logic.\n\nThe algorithm stops when no new frequent itemsets are found or when a user-specified maximum size is reached. The final result aggregates all frequent itemsets across sizes and returns their fractional supports.",
  "solutionCode": "import math\nfrom itertools import combinations\nfrom typing import Dict, FrozenSet, Iterable, List, Set, Any\n\nimport torch\n\n\ndef apriori(transactions: Iterable[Iterable[Any]], min_support: float = 0.5, max_length: int = None) -> Dict[FrozenSet[Any], float]:\n    \"\"\"\n    Apriori frequent itemset mining using PyTorch for efficient support counting.\n\n    Args:\n        transactions: Iterable of transactions, each transaction is an iterable of hashable items.\n        min_support: Minimum support threshold in [0, 1]. Itemsets with support >= min_support are kept.\n        max_length: Optional maximum size (k) of itemsets to mine. If None, mine all sizes.\n\n    Returns:\n        Dictionary mapping frozenset of items to fractional support (float).\n    \"\"\"\n    # Convert transactions into list of sets for consistency\n    transactions = [set(t) for t in transactions]\n    num_transactions = len(transactions)\n    if num_transactions == 0:\n        return {}\n\n    # Build item vocabulary\n    items_sorted = sorted({item for t in transactions for item in t})\n    num_items = len(items_sorted)\n    if num_items == 0:\n        return {}\n\n    item_to_idx = {item: i for i, item in enumerate(items_sorted)}\n    idx_to_item = {i: item for i, item in enumerate(items_sorted)}\n\n    # Build boolean transaction matrix X: [T, I]\n    X = torch.zeros((num_transactions, num_items), dtype=torch.bool)\n    for t_idx, t in enumerate(transactions):\n        if t:\n            cols = [item_to_idx[it] for it in t if it in item_to_idx]\n            if cols:\n                X[t_idx, torch.tensor(cols, dtype=torch.long)] = True\n\n    # Minimum support count (use ceil to ensure >= threshold fraction)\n    min_count = int(math.ceil(min_support * num_transactions - 1e-12))\n\n    # Helper: count supports for a batch of candidates (2D tensor [C, k])\n    def count_candidates(candidate_idx: torch.Tensor, chunk_size: int = 4096) -> torch.Tensor:\n        # candidate_idx: [C, k] long tensor of item indices\n        C = candidate_idx.shape[0]\n        if C == 0:\n            return torch.zeros((0,), dtype=torch.long)\n        T = X.shape[0]\n        k = candidate_idx.shape[1]\n        counts = torch.zeros((C,), dtype=torch.long)\n        # Process in chunks to control memory usage\n        for start in range(0, C, chunk_size):\n            end = min(C, start + chunk_size)\n            idx_chunk = candidate_idx[start:end]  # [c, k]\n            # Build gather indices of shape [T, c, k]\n            idx = idx_chunk.unsqueeze(0).expand(T, -1, -1)\n            # Expand X to [T, c, I] view, then gather -> [T, c, k]\n            X_exp = X.unsqueeze(1).expand(-1, idx_chunk.shape[0], -1)\n            cols = X_exp.gather(2, idx)\n            present = cols.all(dim=2)  # [T, c]\n            counts[start:end] = present.sum(dim=0)\n        return counts\n\n    # Result accumulator: map frozenset(items) -> fractional support\n    result: Dict[FrozenSet[Any], float] = {}\n\n    # Level 1 (singletons)\n    col_counts = X.sum(dim=0).to(torch.long)  # [I]\n    L1_indices = [i for i, c in enumerate(col_counts.tolist()) if c >= min_count]\n    L_prev = [tuple([i]) for i in sorted(L1_indices)]  # list of tuples of indices\n\n    # Record supports for singletons\n    for i in L1_indices:\n        item = idx_to_item[i]\n        result[frozenset([item])] = col_counts[i].item() / float(num_transactions)\n\n    if max_length is not None and max_length <= 1:\n        return result\n\n    k = 2\n    while L_prev and (max_length is None or k <= max_length):\n        # Generate candidate k-itemsets via join and prune\n        L_prev_sorted = sorted(L_prev)\n        L_prev_set = set(L_prev_sorted)\n        candidates = []\n        n_prev = len(L_prev_sorted)\n        for i in range(n_prev):\n            for j in range(i + 1, n_prev):\n                a, b = L_prev_sorted[i], L_prev_sorted[j]\n                # Join if first k-2 items match\n                if a[: k - 2] == b[: k - 2]:\n                    cand = tuple(sorted(set(a) | set(b)))\n                    if len(cand) != k:\n                        continue\n                    # Prune: all (k-1)-subsets must be frequent\n                    all_subsets_frequent = True\n                    for subset in combinations(cand, k - 1):\n                        if subset not in L_prev_set:\n                            all_subsets_frequent = False\n                            break\n                    if all_subsets_frequent:\n                        candidates.append(cand)\n                else:\n                    break  # due to sorting, no further j will match prefix\n        # Deduplicate candidates\n        candidates = sorted(set(candidates))\n        if not candidates:\n            break\n\n        # Count supports for candidates using PyTorch\n        cand_tensor = torch.tensor(candidates, dtype=torch.long)\n        cand_counts = count_candidates(cand_tensor)\n\n        # Filter by min_count\n        L_curr = []\n        for cand, cnt in zip(candidates, cand_counts.tolist()):\n            if cnt >= min_count:\n                L_curr.append(cand)\n                items = [idx_to_item[idx] for idx in cand]\n                result[frozenset(items)] = cnt / float(num_transactions)\n\n        L_prev = L_curr\n        k += 1\n\n    return result\n\n\nif __name__ == \"__main__\":\n    # Example usage\n    transactions = [\n        {'bread', 'milk'},\n        {'bread', 'diaper', 'beer', 'eggs'},\n        {'milk', 'diaper', 'beer', 'cola'},\n        {'bread', 'milk', 'diaper', 'beer'},\n        {'bread', 'milk', 'diaper', 'cola'}\n    ]\n    res = apriori(transactions, min_support=0.6)\n    for k in sorted(res, key=lambda x: (len(x), sorted(x))):\n        print(sorted(list(k)), round(res[k], 2))\n",
  "timeComplexity": "O(I*T + sum_{k>=2} |C_k| * T * k), where T is the number of transactions, I is the number of distinct items, and |C_k| is the number of candidate k-itemsets after pruning.",
  "spaceComplexity": "O(T*I + sum_{k} |L_k| * k), for the boolean transaction matrix and storage of frequent itemsets at each level.",
  "platform": "deepml"
};
