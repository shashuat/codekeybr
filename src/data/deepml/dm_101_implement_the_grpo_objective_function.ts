import { Problem } from '../../types';

export const DM_101_IMPLEMENT_THE_GRPO_OBJECTIVE_FUNCTION: Problem = {
  "id": "dm_101_implement_the_grpo_objective_function",
  "title": "Implement the GRPO Objective Function",
  "difficulty": "Hard",
  "tags": [
    "Optimization",
    "Loss Functions",
    "Probability"
  ],
  "descriptionMarkdown": "Implement the GRPO (Group Relative Policy Optimization) objective function as defined in the DeepSeekMath paper. Given likelihood ratios rhos, advantages A, and policy probabilities for the current (pi_theta) and reference (pi_theta_ref) policies, compute the GRPO objective.\n\nRequirements:\n- Clipped surrogate objective (PPO-style): use epsilon to clip likelihood ratios and take the appropriate min/max depending on the sign of the advantage.\n- Unbiased KL divergence estimator used in GRPO: KL = r - log(r) - 1 where r = pi_ref / pi_theta.\n- Final objective = mean(clipped_surrogate - beta * KL).\n\nExample:\nInput:\n- rhos = [1.2, 0.8, 1.1]\n- A = [1.0, 1.0, 1.0]\n- pi_theta = [0.9, 1.1, 1.0]\n- pi_theta_ref = [1.0, 0.5, 1.5]\n- epsilon = 0.2, beta = 0.01\n\nOutput:\n- 1.032700 (approximate reference)\n",
  "solutionExplanation": "GRPO combines PPO-style clipping with a KL penalty measured against a reference policy. The surrogate objective promotes updates that increase likelihood of advantageous actions while controlling instability via clipping. Specifically, for each sample with advantage A and likelihood ratio rho = pi_theta / pi_theta_old, PPO uses the clipped surrogate: min(rho, clip(rho, 1 - eps, 1 + eps)) * A for positive advantages and max(...) for negative ones. This ensures conservative updates in directions that would otherwise excessively inflate or deflate the ratio.\n\nUnlike a standard KL regularizer, GRPO uses an unbiased estimator of the KL between the reference and current policies computed under the current policy samples: with r = pi_ref / pi_theta, KL is estimated as r - log(r) - 1. This yields an unbiased estimate of D_KL(pi_ref || pi_theta) when taking expectation under pi_theta. The final objective is the mean over samples of the clipped surrogate minus beta times this KL estimate. This balances improvement in the objective with a penalty for deviating from the reference policy.",
  "solutionCode": "import torch\n\ndef grpo_objective(rhos, A, pi_theta, pi_theta_ref, epsilon=0.2, beta=0.01, eps=1e-12):\n    \"\"\"\n    Compute the GRPO objective.\n\n    Args:\n        rhos: Iterable or torch.Tensor of likelihood ratios (rho_i).\n        A: Iterable or torch.Tensor of advantage estimates (A_i).\n        pi_theta: Iterable or torch.Tensor of current policy probabilities/densities for the taken actions.\n        pi_theta_ref: Iterable or torch.Tensor of reference policy probabilities/densities for the taken actions.\n        epsilon: PPO clipping parameter.\n        beta: KL divergence penalty coefficient.\n        eps: numerical stability epsilon to avoid log(0).\n\n    Returns:\n        A scalar torch.Tensor representing the GRPO objective (to maximize).\n    \"\"\"\n    # Convert inputs to tensors with a consistent dtype\n    rhos_t = torch.as_tensor(rhos, dtype=torch.get_default_dtype())\n    A_t = torch.as_tensor(A, dtype=torch.get_default_dtype())\n    pi_theta_t = torch.as_tensor(pi_theta, dtype=torch.get_default_dtype())\n    pi_ref_t = torch.as_tensor(pi_theta_ref, dtype=torch.get_default_dtype())\n\n    if not (rhos_t.shape == A_t.shape == pi_theta_t.shape == pi_ref_t.shape):\n        raise ValueError(\"All input tensors/lists must have the same shape.\")\n\n    # PPO-style clipped surrogate\n    clipped_rhos = torch.clamp(rhos_t, 1.0 - epsilon, 1.0 + epsilon)\n    surr1 = rhos_t * A_t\n    surr2 = clipped_rhos * A_t\n    # Correct handling for signs of advantages\n    clipped_surrogate = torch.where(A_t >= 0.0, torch.minimum(surr1, surr2), torch.maximum(surr1, surr2))\n\n    # GRPO unbiased KL estimator: KL = r - log(r) - 1, where r = pi_ref / pi_theta\n    r = pi_ref_t / (pi_theta_t + eps)\n    r = torch.clamp(r, min=eps)\n    kl_est = r - torch.log(r) - 1.0\n\n    # Final objective: average over samples of (clipped_surrogate - beta * KL)\n    objective = (clipped_surrogate - beta * kl_est).mean()\n    return objective\n\n\ndef solution():\n    # Example usage\n    rhos = [1.2, 0.8, 1.1]\n    A = [1.0, 1.0, 1.0]\n    pi_theta = [0.9, 1.1, 1.0]\n    pi_theta_ref = [1.0, 0.5, 1.5]\n    obj = grpo_objective(rhos, A, pi_theta, pi_theta_ref, epsilon=0.2, beta=0.01)\n    # Return the scalar value for display; keep tensor if needed for autograd\n    return float(obj.item())\n\nif __name__ == \"__main__\":\n    print(f\"GRPO objective: {solution():.6f}\")\n",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(N)",
  "platform": "deepml"
};
