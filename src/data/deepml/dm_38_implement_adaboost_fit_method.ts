import { Problem } from '../../types';

export const DM_38_IMPLEMENT_ADABOOST_FIT_METHOD: Problem = {
  "id": "dm_38_implement_adaboost_fit_method",
  "title": "Implement AdaBoost Fit Method",
  "difficulty": "Hard",
  "tags": [
    "Optimization",
    "Probability",
    "Loss Functions"
  ],
  "descriptionMarkdown": "Write a Python function `adaboost_fit` that implements the fit method for an AdaBoost classifier using decision stumps (single-feature threshold classifiers). The function should:\n\n- Take a 2D array `X` of shape `(n_samples, n_features)`, a 1D array `y` of shape `(n_samples,)` with labels in `{-1, 1}`, and an integer `n_clf` (number of weak classifiers).\n- Initialize uniform sample weights.\n- For each boosting round, search across all features, thresholds, and polarities to find the stump with the lowest weighted classification error.\n- Compute the classifier weight `alpha` from the weighted error and update the sample weights accordingly.\n- Return a list of weak classifiers, each as a dictionary containing: `polarity`, `threshold`, `feature_index`, and `alpha`.\n\nExample:\n\nInput:\n- `X = [[1, 2], [2, 3], [3, 4], [4, 5]]`\n- `y = [1, 1, -1, -1]`\n- `n_clf = 3`\n\nOutput (example format, actual values may vary):\n- `[\n  {'polarity': 1, 'threshold': 2.0, 'feature_index': 0, 'alpha': 0.5},\n  {'polarity': -1, 'threshold': 3.0, 'feature_index': 1, 'alpha': 0.3},\n  {'polarity': 1, 'threshold': 4.0, 'feature_index': 0, 'alpha': 0.2}\n]`",
  "solutionExplanation": "AdaBoost builds a strong classifier as a weighted sum of weak classifiers. Here, each weak classifier is a decision stump defined by a feature index, a threshold, and a polarity indicating which side of the threshold predicts +1 or -1. At each boosting round, we select the stump that minimizes the weighted classification error with respect to the current sample weights.\n\nWe initialize uniform sample weights. For each stump candidate, we compute predictions and the weighted error. The best stump is chosen and assigned a weight alpha = 0.5 * log((1 - error) / (error + eps)), where eps is a small constant for numerical stability. We then update the sample weights multiplicatively by exp(-alpha * y_i * h_i(x)), which increases weights on misclassified samples and decreases them on correctly classified samples. Finally, we normalize the weights and repeat for the specified number of classifiers, returning the list of selected stump parameters.",
  "solutionCode": "import torch\n\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Fit AdaBoost with decision stumps using PyTorch ops.\n\n    Args:\n        X: array-like of shape (n_samples, n_features) (list, numpy array, or torch tensor)\n        y: array-like of shape (n_samples,) with labels in {-1, 1}\n        n_clf: int, number of weak classifiers (stumps)\n\n    Returns:\n        List[dict]: Each dict has keys: 'polarity', 'threshold', 'feature_index', 'alpha'\n    \"\"\"\n    X_t = torch.as_tensor(X, dtype=torch.float64)\n    y_t = torch.as_tensor(y, dtype=torch.float64)\n    n_samples, n_features = X_t.shape\n\n    # Ensure labels are -1 and 1\n    if not torch.all((y_t == -1) | (y_t == 1)):\n        raise ValueError(\"Labels y must be in {-1, 1}.\")\n\n    # Initialize uniform sample weights\n    w = torch.full((n_samples,), 1.0 / n_samples, dtype=torch.float64)\n\n    clfs = []\n    eps = 1e-12\n\n    for _ in range(int(n_clf)):\n        best_err = float('inf')\n        best_feature = None\n        best_threshold = None\n        best_polarity = None\n        best_pred = None\n\n        for j in range(n_features):\n            X_col = X_t[:, j]\n            # Candidate thresholds: unique feature values (sorted)\n            thresholds = torch.unique(X_col, sorted=True)\n            if thresholds.numel() == 0:\n                continue\n\n            for t in thresholds:\n                for p in (1.0, -1.0):\n                    # Prediction rule:\n                    # pred = +1 by default, -1 if p * x < p * t\n                    pred = torch.ones(n_samples, dtype=torch.float64)\n                    mask = (p * X_col) < (p * t)\n                    pred[mask] = -1.0\n\n                    misclassified = pred != y_t\n                    err = torch.sum(w[misclassified])\n\n                    if err.item() < best_err:\n                        best_err = err.item()\n                        best_feature = j\n                        best_threshold = float(t.item())\n                        best_polarity = int(1 if p > 0 else -1)\n                        best_pred = pred.clone()\n\n        # Compute alpha and update weights\n        err = max(min(best_err, 1.0 - eps), eps)  # clamp error to (eps, 1-eps)\n        alpha = 0.5 * torch.log(torch.tensor((1.0 - err) / err, dtype=torch.float64))\n\n        # w_i <- w_i * exp(-alpha * y_i * h_i(x))\n        w = w * torch.exp(-alpha * y_t * best_pred)\n        w = w / (w.sum() + eps)\n\n        clfs.append({\n            'polarity': best_polarity,\n            'threshold': best_threshold,\n            'feature_index': int(best_feature),\n            'alpha': float(alpha.item())\n        })\n\n    return clfs\n\n\ndef solution():\n    # Example usage\n    X = [[1.0, 2.0], [2.0, 3.0], [3.0, 4.0], [4.0, 5.0]]\n    y = [1.0, 1.0, -1.0, -1.0]\n    n_clf = 3\n    clfs = adaboost_fit(X, y, n_clf)\n    print(clfs)\n",
  "timeComplexity": "O(T * d * n^2), where T is the number of classifiers, d is the number of features, and n is the number of samples",
  "spaceComplexity": "O(n + T)",
  "platform": "deepml"
};
