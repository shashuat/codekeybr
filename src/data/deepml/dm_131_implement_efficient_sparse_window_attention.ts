import { Problem } from '../../types';

export const DM_131_IMPLEMENT_EFFICIENT_SPARSE_WINDOW_ATTENTION: Problem = {
  "id": "dm_131_implement_efficient_sparse_window_attention",
  "title": "Implement Efficient Sparse Window Attention",
  "difficulty": "Medium",
  "tags": [
    "Transformers",
    "Attention",
    "Neural Networks",
    "Matrix Operations"
  ],
  "descriptionMarkdown": "Create a function named sparse_window_attention that computes sparse attention over long sequences by sliding a fixed-radius window across the sequence.\n\n- The parameter window_size represents the radius w of the window. For a token at index i, attend only to tokens whose indices are within max(0, i \u2212 w) through min(seq_len \u2212 1, i + w), inclusive.\n- Tokens near the beginning or end of the sequence simply have smaller windows; no padding is added.\n\nInputs:\n- Q, K, V: arrays with shapes (seq_len, d_k) for Q and K, and (seq_len, d_v) for V.\n- window_size: integer window radius.\n- scale_factor (optional): value used to scale dot-product scores; if None, default to sqrt(d_k).\n\nOutput:\n- An array of shape (seq_len, d_v) containing the attention results.\n\nExample:\n```\nQ = [[1.0], [1.0], [1.0]]\nK = [[1.0], [1.0], [1.0]]\nV = [[1.0], [2.0], [3.0]]\nwindow_size = 1\noutput = [[1.5], [2.0], [2.5]]\n```\nReasoning: For each query, compute attention scores only with keys within a radius-1 window, apply softmax, and form a weighted sum of the corresponding values.",
  "solutionExplanation": "The sparse window attention restricts each query position i to attend only to keys and values within a fixed-radius window [max(0, i \u2212 w), min(L \u2212 1, i + w)], where L is the sequence length and w is window_size. This reduces the computational cost from quadratic in L to linear in L times the window size. For each position, we compute scaled dot-product attention: scores = (q_i \u00b7 K_window) / scale, where scale defaults to sqrt(d_k) for numerical stability, then apply softmax over the window, and finally take a weighted sum over the corresponding V_window.\n\nImplementationally, we iterate over sequence positions and, for each index, slice the local key/value windows. We use PyTorch tensor operations (matmul and softmax) to compute the local attention efficiently and numerically stably. Since each window size can vary near sequence boundaries, the code dynamically adapts the slice limits. The function accepts NumPy arrays and converts them to torch tensors internally, returning a NumPy array to match the problem specification.\n\nThis design maintains O(L \u00b7 w \u00b7 (d_k + d_v)) runtime without forming a dense L\u00d7L attention matrix, making it suitable for long sequences when w is much smaller than L. The memory footprint is modest, dominated by the output and small per-step slices.",
  "solutionCode": "import math\nimport numpy as np\nimport torch\n\ndef sparse_window_attention(Q, K, V, window_size, scale_factor=None):\n    \"\"\"\n    Compute sparse windowed scaled dot-product attention.\n\n    Args:\n        Q (np.ndarray or torch.Tensor): shape (L, d_k)\n        K (np.ndarray or torch.Tensor): shape (L, d_k)\n        V (np.ndarray or torch.Tensor): shape (L, d_v)\n        window_size (int): window radius w. For position i, attend to [max(0, i-w), min(L-1, i+w)]\n        scale_factor (float or None): scale for dot-product scores; defaults to sqrt(d_k)\n\n    Returns:\n        np.ndarray: shape (L, d_v) attention output.\n    \"\"\"\n    # Convert inputs to torch tensors (CPU); do not copy if already tensors\n    q = torch.as_tensor(Q)\n    k = torch.as_tensor(K)\n    v = torch.as_tensor(V)\n\n    if q.dim() != 2 or k.dim() != 2 or v.dim() != 2:\n        raise ValueError(\"Q, K, V must be 2D arrays/tensors.\")\n\n    L, d_k = q.shape\n    if k.shape != (L, d_k):\n        raise ValueError(f\"K must have shape {(L, d_k)}, but got {tuple(k.shape)}\")\n    if v.shape[0] != L:\n        raise ValueError(f\"V must have L={L} rows, but got {v.shape[0]}\")\n\n    # Promote to floating type for attention math\n    if not torch.is_floating_point(q):\n        q = q.float()\n    if not torch.is_floating_point(k):\n        k = k.float()\n    if not torch.is_floating_point(v):\n        v = v.float()\n\n    # Determine scale\n    if scale_factor is None:\n        scale = math.sqrt(d_k) if d_k > 0 else 1.0\n    else:\n        scale = float(scale_factor) if not isinstance(scale_factor, torch.Tensor) else float(scale_factor.item())\n        if scale == 0.0:\n            raise ValueError(\"scale_factor must be non-zero\")\n\n    if window_size < 0:\n        raise ValueError(\"window_size must be non-negative\")\n\n    outputs = []\n    for i in range(L):\n        left = max(0, i - window_size)\n        right = min(L - 1, i + window_size)\n        # Window slices\n        k_win = k[left:right + 1]        # (M, d_k)\n        v_win = v[left:right + 1]        # (M, d_v)\n\n        # (1, d_k) @ (d_k, M) -> (1, M)\n        scores = (q[i:i+1] @ k_win.T) / scale\n        attn = torch.softmax(scores, dim=-1)  # (1, M)\n        out_i = attn @ v_win                  # (1, d_v)\n        outputs.append(out_i.squeeze(0))\n\n    out = torch.stack(outputs, dim=0)  # (L, d_v)\n    # Return NumPy array per problem statement\n    return out.detach().cpu().numpy()\n\n# Example usage\nif __name__ == \"__main__\":\n    Q = np.array([[1.0], [1.0], [1.0]])\n    K = np.array([[1.0], [1.0], [1.0]])\n    V = np.array([[1.0], [2.0], [3.0]])\n    print(sparse_window_attention(Q, K, V, window_size=1))\n",
  "timeComplexity": "O(L * w * (d_k + d_v))",
  "spaceComplexity": "O(L * d_v)",
  "platform": "deepml"
};
