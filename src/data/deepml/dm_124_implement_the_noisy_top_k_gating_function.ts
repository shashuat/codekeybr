import { Problem } from '../../types';

export const DM_124_IMPLEMENT_THE_NOISY_TOP_K_GATING_FUNCTION: Problem = {
  "id": "dm_124_implement_the_noisy_top_k_gating_function",
  "title": "Implement the Noisy Top-K Gating Function",
  "difficulty": "Medium",
  "tags": [
    "Neural Networks",
    "Probability",
    "Matrix Operations",
    "Linear Algebra"
  ],
  "descriptionMarkdown": "Implement the Noisy Top-K gating mechanism used in Mixture-of-Experts (MoE) models. Given an input matrix X, gating weights W_g, noise weights W_noise, pre-sampled noise N, and a sparsity level k, compute the final gating probabilities per example.\n\nInputs:\n- X: Input data of shape (batch_size, features)\n- W_g: Gating weight matrix of shape (features, num_experts)\n- W_noise: Noise weight matrix of shape (features, num_experts)\n- N: Pre-sampled noise of shape (batch_size, num_experts)\n- k: Number of experts to keep per example (Top-K)\n\nProcedure:\n1. Compute clean logits: logits = X @ W_g\n2. Compute input-dependent noise scale via a softplus: noise_std = softplus(X @ W_noise)\n3. Add noise: noisy_logits = logits + noise_std * N\n4. Select Top-K experts per example from noisy_logits and mask others to -inf\n5. Apply softmax over the masked logits to obtain sparse gating probabilities\n\nExample:\n- X = [[1.0, 2.0]]\n- W_g = [[1.0, 0.0], [0.0, 1.0]]\n- W_noise = [[0.5, 0.5], [0.5, 0.5]]\n- N = [[1.0, -1.0]]\n- k = 2\n\nOutput: [[0.917, 0.0825]] (approximately)\n\nThis demonstrates that after noise perturbation, the gating distribution becomes sparse and favors the higher-scoring expert.",
  "solutionExplanation": "Noisy Top-K gating stochastically selects a subset of experts by adding input-dependent noise to the gating logits. First, we compute the clean gating logits as a linear projection of the input, logits = X @ W_g. To modulate the amount of exploration per expert, we learn a noise scale via another linear projection followed by softplus to ensure positivity: noise_std = softplus(X @ W_noise). The pre-sampled noise N (e.g., standard normal) is then scaled and added to the clean logits to form the noisy logits.\n\nTo enforce sparsity, we take the top-k experts for each example based on the noisy logits and mask the rest to negative infinity. Finally, we apply a softmax over the masked noisy logits to obtain valid probabilities that sum to one, with non-zero mass on at most k experts. This aligns with the Noisy Top-K mechanism commonly used in Mixture-of-Experts to balance exploration (via noise) and sparsity (via Top-K). The provided example matches closely when using softplus for the noise standard deviation, yielding the reported probabilities.",
  "solutionCode": "import torch\nimport torch.nn.functional as F\n\ndef noisy_topk_gating(X, W_g, W_noise, N, k):\n    \"\"\"\n    Compute Noisy Top-K gating probabilities.\n\n    Args:\n        X (Tensor): Input tensor of shape (B, F)\n        W_g (Tensor): Gating weights of shape (F, E)\n        W_noise (Tensor): Noise weights of shape (F, E)\n        N (Tensor): Pre-sampled noise of shape (B, E)\n        k (int): Number of experts to keep per example\n\n    Returns:\n        Tensor: Gating probabilities of shape (B, E), with at most k non-zeros per row.\n    \"\"\"\n    X = torch.as_tensor(X, dtype=torch.float32)\n    W_g = torch.as_tensor(W_g, dtype=torch.float32, device=X.device)\n    W_noise = torch.as_tensor(W_noise, dtype=torch.float32, device=X.device)\n    N = torch.as_tensor(N, dtype=torch.float32, device=X.device)\n\n    B, F_in = X.shape\n    Fw, E = W_g.shape\n    assert Fw == F_in, \"W_g has incompatible shape with X\"\n    assert W_noise.shape == W_g.shape, \"W_noise must match W_g shape\"\n    assert N.shape == (B, E), \"N must have shape (batch_size, num_experts)\"\n    assert 1 <= k <= E, \"k must be in [1, num_experts]\"\n\n    # Clean logits\n    logits = X @ W_g  # (B, E)\n\n    # Input-dependent noise scale (ensure positivity via softplus)\n    noise_std = F.softplus(X @ W_noise) + 1e-9  # (B, E)\n\n    # Add noise\n    noisy_logits = logits + noise_std * N  # (B, E)\n\n    # Top-k masking on noisy logits\n    if k < E:\n        topk_vals, topk_idx = torch.topk(noisy_logits, k, dim=-1)\n        masked_logits = torch.full_like(noisy_logits, float(\"-inf\"))\n        masked_logits.scatter_(1, topk_idx, topk_vals)\n    else:\n        masked_logits = noisy_logits\n\n    # Softmax over masked logits to produce probabilities\n    probs = F.softmax(masked_logits, dim=-1)\n    return probs\n\n\ndef solution():\n    # Example usage\n    X = torch.tensor([[1.0, 2.0]])\n    W_g = torch.tensor([[1.0, 0.0], [0.0, 1.0]])\n    W_noise = torch.tensor([[0.5, 0.5], [0.5, 0.5]])\n    N = torch.tensor([[1.0, -1.0]])\n    k = 2\n\n    probs = noisy_topk_gating(X, W_g, W_noise, N, k)\n    # Print with rounding similar to the example\n    print(torch.round(probs * 1e3) / 1e3)\n    return probs\n\nif __name__ == \"__main__\":\n    _ = solution()\n",
  "timeComplexity": "O(B*F*E + B*E*log k)",
  "spaceComplexity": "O(B*E)",
  "platform": "deepml"
};
