import { Problem } from '../../types';

export const DM_87_ADAM_OPTIMIZER: Problem = {
  "id": "dm_87_adam_optimizer",
  "title": "Adam Optimizer",
  "difficulty": "Medium",
  "tags": [
    "Optimization",
    "Gradient Descent",
    "Neural Networks"
  ],
  "descriptionMarkdown": "Implement the Adam optimizer update step function. Your function should take the current parameter value, gradient, and moving averages as inputs, and return the updated parameter value and new moving averages. The function must handle scalar and array/tensor inputs and include bias correction for the moving averages.\n\nExample:\n- Input: parameter = 1.0, grad = 0.1, m = 0.0, v = 0.0, t = 1\n- Output: (0.999, 0.01, 0.00001)\n\nReasoning: Adam updates the first moment (m) and second moment (v), applies bias correction to obtain m_hat and v_hat, and then updates the parameter using the corrected moments. With the defaults (learning_rate=0.001, beta1=0.9, beta2=0.999), the updated parameter becomes 0.999 for the given inputs.",
  "solutionExplanation": "Adam maintains exponentially decaying moving averages of the gradient (first moment m) and the squared gradient (second moment v). At step t with gradient g, the raw moment updates are m = beta1 * m + (1 - beta1) * g and v = beta2 * v + (1 - beta2) * g^2. Because these moving averages are biased towards zero in the early steps, Adam applies bias correction: m_hat = m / (1 - beta1^t) and v_hat = v / (1 - beta2^t).\n\nThe parameter update then uses an adaptive step size for each parameter dimension: parameter_new = parameter - learning_rate * m_hat / (sqrt(v_hat) + epsilon). This yields stable and adaptive updates that work well across a variety of deep learning tasks. The implementation below handles both scalars and tensors, preserves dtype/device, and uses pure PyTorch tensor operations.",
  "solutionCode": "import torch\n\ndef adam_optimizer_update(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Perform one Adam optimizer update step.\n\n    Args:\n        parameter: float or torch.Tensor - current parameter value(s).\n        grad: float or torch.Tensor - current gradient(s) w.r.t. parameter.\n        m: float or torch.Tensor - first moment (moving average of gradients).\n        v: float or torch.Tensor - second moment (moving average of squared gradients).\n        t: int - current timestep (1-indexed).\n        learning_rate: float - step size (default 0.001).\n        beta1: float - decay for first moment (default 0.9).\n        beta2: float - decay for second moment (default 0.999).\n        epsilon: float - small constant for numerical stability.\n\n    Returns:\n        (new_parameter, new_m, new_v): tuple of torch.Tensor with updated values.\n    \"\"\"\n    if t <= 0:\n        raise ValueError(\"timestep t must be >= 1 for bias correction\")\n\n    # Convert to tensors and ensure shared dtype/device\n    param_t = torch.as_tensor(parameter)\n    grad_t = torch.as_tensor(grad, dtype=param_t.dtype, device=param_t.device)\n    m_t = torch.as_tensor(m, dtype=param_t.dtype, device=param_t.device)\n    v_t = torch.as_tensor(v, dtype=param_t.dtype, device=param_t.device)\n\n    # Adam moment updates\n    new_m = beta1 * m_t + (1.0 - beta1) * grad_t\n    new_v = beta2 * v_t + (1.0 - beta2) * (grad_t * grad_t)\n\n    # Bias correction\n    beta1_pow_t = beta1 ** t\n    beta2_pow_t = beta2 ** t\n    m_hat = new_m / (1.0 - beta1_pow_t)\n    v_hat = new_v / (1.0 - beta2_pow_t)\n\n    # Parameter update (no autograd tracking needed for manual optimizer step)\n    with torch.no_grad():\n        new_param = param_t - learning_rate * m_hat / (torch.sqrt(v_hat) + epsilon)\n\n    return new_param, new_m, new_v\n\n\ndef solution():\n    # Example 1: Scalar inputs\n    parameter = 1.0\n    grad = 0.1\n    m = 0.0\n    v = 0.0\n    t = 1\n    new_param, new_m, new_v = adam_optimizer_update(parameter, grad, m, v, t)\n    print(\"Scalar example:\")\n    print(\"new_param=\", float(new_param), \"new_m=\", float(new_m), \"new_v=\", float(new_v))\n    # Expected: new_param \u2248 0.999, new_m = 0.01, new_v = 1e-05 with default betas.\n\n    # Example 2: Tensor inputs\n    param_tensor = torch.tensor([1.0, 2.0, -3.0])\n    grad_tensor = torch.tensor([0.1, -0.2, 0.05])\n    m_tensor = torch.zeros_like(param_tensor)\n    v_tensor = torch.zeros_like(param_tensor)\n    t = 1\n    new_param_t, new_m_t, new_v_t = adam_optimizer_update(param_tensor, grad_tensor, m_tensor, v_tensor, t)\n    print(\"\\nTensor example:\")\n    print(\"new_param=\", new_param_t)\n    print(\"new_m=\", new_m_t)\n    print(\"new_v=\", new_v_t)\n",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(1)",
  "platform": "deepml"
};
