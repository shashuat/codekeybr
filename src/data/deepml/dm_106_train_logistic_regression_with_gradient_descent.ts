import { Problem } from '../../types';

export const DM_106_TRAIN_LOGISTIC_REGRESSION_WITH_GRADIENT_DESCENT: Problem = {
  "id": "dm_106_train_logistic_regression_with_gradient_descent",
  "title": "Train Logistic Regression with Gradient Descent",
  "difficulty": "Hard",
  "tags": [
    "Optimization",
    "Loss Functions",
    "Activation Functions",
    "Gradient Descent",
    "Matrix Operations",
    "Linear Algebra"
  ],
  "descriptionMarkdown": "Implement a gradient descent training algorithm for binary logistic regression. Your task is to compute model parameters using Binary Cross Entropy (BCE) loss and return the optimized coefficients along with the loss values collected over iterations.\n\nSpecifications:\n- Add a bias term (column of ones) as the FIRST column of the feature matrix\n- Initialize all coefficients to ZERO\n- Use SUM-based BCE loss (not mean): L = -\u2211[y \u00b7 log(p) + (1 - y) \u00b7 log(1 - p)]\n- Update rule: \u03b2 = \u03b2 - lr \u00d7 X^T \u00d7 (predictions - y)\n\nExample:\n- Input:\n  - X = [[1.0, 0.5], [-0.5, -1.5], [2.0, 1.5], [-2.0, -1.0]]\n  - y = [1, 0, 1, 0]\n  - learning_rate = 0.01\n  - iterations = 5\n- Output (approx.):\n  - coefficients = [-0.0001, 0.128, 0.1053]\n  - losses = [2.7726, 2.6485, 2.5330, 2.4254, 2.3250]\n\nThe function should add the bias column to X (making it n\u00d7(d+1)), initialize d+1 coefficients to zero, then iteratively update them using gradient descent. Track and return the loss after each iteration.",
  "solutionExplanation": "Logistic regression models the probability of the positive class as p = \u03c3(X\u03b2), where \u03c3 is the sigmoid function. Given binary labels y \u2208 {0,1}, the appropriate loss is the Binary Cross Entropy (BCE). Following the problem specification, we use the sum-reduction BCE: L(\u03b2) = -\u2211[y log p + (1 \u2212 y) log(1 \u2212 p)]. The gradient of this loss with respect to \u03b2 is X^T (p \u2212 y), where p = \u03c3(X\u03b2). This yields a simple and efficient closed-form expression for the full-batch gradient.\n\nWe prepend a bias column of ones to the feature matrix to incorporate the intercept as the first coefficient. Parameters are initialized to zeros, and at each iteration we compute predictions via the sigmoid of the linear logits, evaluate the BCE loss, and update \u03b2 with gradient descent: \u03b2 \u2190 \u03b2 \u2212 lr \u00b7 X^T (p \u2212 y). We also clamp probabilities in the log to maintain numerical stability. The losses should monotonically decrease for a suitable learning rate, demonstrating convergence.",
  "solutionCode": "import torch\n\ndef train_logreg(X, y, learning_rate: float, iterations: int):\n    \"\"\"\n    Train binary logistic regression with full-batch gradient descent using SUM-based BCE loss.\n\n    Args:\n        X: Feature matrix (numpy array or torch tensor) of shape (n_samples, n_features)\n        y: Binary labels (numpy array or torch tensor) of shape (n_samples,)\n        learning_rate: Step size for gradient descent\n        iterations: Number of training iterations\n\n    Returns:\n        coefficients: List[float] of learned weights with bias as the first coefficient\n        losses: List[float] of BCE-sum losses over iterations\n    \"\"\"\n    # Convert inputs to float32 tensors\n    X_t = torch.as_tensor(X, dtype=torch.float32)\n    y_t = torch.as_tensor(y, dtype=torch.float32).view(-1)\n\n    n_samples, n_features = X_t.shape\n\n    # Add bias as the FIRST column\n    ones = torch.ones(n_samples, 1, dtype=X_t.dtype)\n    Xb = torch.cat([ones, X_t], dim=1)  # shape: (n_samples, n_features + 1)\n\n    # Initialize coefficients to zeros (bias included)\n    beta = torch.zeros(n_features + 1, dtype=X_t.dtype)\n\n    losses = []\n    eps = 1e-12  # numerical stability for log\n\n    for _ in range(iterations):\n        # Forward pass: logits and probabilities\n        logits = Xb @ beta  # shape: (n_samples,)\n        p = torch.sigmoid(logits)\n\n        # SUM-based BCE loss\n        loss = -(y_t * torch.log(torch.clamp(p, min=eps)) +\n                 (1.0 - y_t) * torch.log(torch.clamp(1.0 - p, min=eps))).sum()\n        losses.append(float(loss.item()))\n\n        # Gradient: X^T (p - y)\n        grad = Xb.t().matmul(p - y_t)\n\n        # Update rule: beta = beta - lr * grad\n        beta = beta - learning_rate * grad\n\n    return beta.tolist(), losses\n\n\ndef solution():\n    # Example usage (reproducing the prompt's example)\n    import numpy as np\n\n    X = np.array([[1.0, 0.5],\n                  [-0.5, -1.5],\n                  [2.0, 1.5],\n                  [-2.0, -1.0]], dtype=np.float32)\n    y = np.array([1, 0, 1, 0], dtype=np.float32)\n\n    learning_rate = 0.01\n    iterations = 5\n\n    coefs, losses = train_logreg(X, y, learning_rate, iterations)\n\n    # Print results\n    print(\"Coefficients (bias first):\", [round(c, 4) for c in coefs])\n    print(\"Losses:\", [round(l, 4) for l in losses])\n\n    return coefs, losses\n\nif __name__ == \"__main__\":\n    solution()",
  "timeComplexity": "O(T * N * D)",
  "spaceComplexity": "O(N * D)",
  "platform": "deepml"
};
