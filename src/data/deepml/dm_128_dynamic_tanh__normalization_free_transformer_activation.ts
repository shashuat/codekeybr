import { Problem } from '../../types';

export const DM_128_DYNAMIC_TANH__NORMALIZATION_FREE_TRANSFORMER_ACTIVATION: Problem = {
  "id": "dm_128_dynamic_tanh_normalization_free_transformer_activation",
  "title": "Dynamic Tanh: Normalization-Free Transformer Activation",
  "difficulty": "Easy",
  "tags": [
    "Neural Networks",
    "Activation Functions",
    "Transformers"
  ],
  "descriptionMarkdown": "Implement the Dynamic Tanh (DyT) function, a normalization-free transformation inspired by the Tanh function. DyT replaces layer normalization in Transformer architectures while preserving squashing behavior and enabling stable training.\n\nGiven an input tensor x, a scalar scaling factor alpha, and per-feature affine parameters gamma and beta, compute:\n\n- y = gamma * tanh(alpha * x) + beta\n\nThis applies elementwise squashing via tanh followed by a feature-wise affine transform, mimicking the effect of normalization without relying on batch or layer statistics.\n\nExample:\n- Input:\n  - x = [[[0.14115588, 0.00372817, 0.24126647, 0.22183601]]]\n  - gamma = [1, 1, 1, 1]\n  - beta = [0, 0, 0, 0]\n  - alpha = 0.5\n- Output:\n  - y \u2248 [[[0.0705, 0.0019, 0.1201, 0.1105]]]  ",
  "solutionExplanation": "Dynamic Tanh (DyT) applies a simple elementwise transformation that combines a scaled tanh nonlinearity with a per-feature affine mapping: y = gamma \u00b7 tanh(alpha \u00b7 x) + beta. The scalar alpha controls the input range before squashing, which can help stabilize training by preventing extreme activations. The gamma and beta parameters provide flexibility similar to the affine step in normalization layers, enabling per-feature scaling and shifting without computing statistics.\n\nThis approach is attractive for normalization-free architectures such as certain Transformer variants, where avoiding explicit mean/variance computations can reduce overhead and simplify training dynamics. Broadcasting gamma and beta across the last (feature) dimension allows DyT to work with inputs of arbitrary leading dimensions (e.g., batch and sequence). The function is fully differentiable and works seamlessly with PyTorch autograd.\n\nImplementation-wise, we ensure that gamma and beta are either scalars or 1D tensors matching the last dimension of x. They are reshaped for broadcasting across all leading dimensions, and all operations are performed with native PyTorch tensor ops for performance and GPU compatibility.",
  "solutionCode": "import torch\nimport torch.nn as nn\nfrom typing import Union\n\ndef _to_broadcast_param(param: Union[float, torch.Tensor], x: torch.Tensor, name: str) -> torch.Tensor:\n    \"\"\"Convert a scalar or 1D parameter to a tensor broadcastable over x's last dimension.\n    - Scalars are returned as 0D tensors on x's device/dtype.\n    - 1D tensors must match x.size(-1) and are reshaped to [1, ..., 1, F].\n    \"\"\"\n    if isinstance(param, torch.Tensor):\n        p = param.to(dtype=x.dtype, device=x.device)\n    else:\n        p = torch.tensor(param, dtype=x.dtype, device=x.device)\n\n    if p.ndim == 0:\n        return p\n    if p.ndim == 1:\n        if p.numel() != x.size(-1):\n            raise ValueError(f\"{name} must have length equal to the last dimension of x (got {p.numel()} vs {x.size(-1)}).\")\n        view_shape = [1] * x.ndim\n        view_shape[-1] = p.numel()\n        return p.view(*view_shape)\n    # For safety, enforce only scalar or 1D inputs for gamma/beta\n    raise ValueError(f\"{name} must be a scalar or 1D tensor; got shape {tuple(p.shape)}.\")\n\n\ndef dynamic_tanh(\n    x: torch.Tensor,\n    alpha: Union[float, torch.Tensor],\n    gamma: Union[float, torch.Tensor],\n    beta: Union[float, torch.Tensor],\n) -> torch.Tensor:\n    \"\"\"Dynamic Tanh (DyT): y = gamma * tanh(alpha * x) + beta\n\n    Args:\n        x: Input tensor of shape (..., F) where F is the feature dimension.\n        alpha: Scalar scale applied before tanh (stabilizes the input range).\n        gamma: Scalar or 1D tensor (F,) for per-feature scaling after tanh.\n        beta: Scalar or 1D tensor (F,) for per-feature bias after tanh.\n\n    Returns:\n        Tensor with the same shape as x.\n    \"\"\"\n    if not isinstance(x, torch.Tensor):\n        raise TypeError(\"x must be a torch.Tensor\")\n\n    # Convert parameters to broadcastable tensors on the same device/dtype\n    # alpha can be scalar or broadcastable to x; typically scalar\n    if isinstance(alpha, torch.Tensor):\n        a = alpha.to(dtype=x.dtype, device=x.device)\n    else:\n        a = torch.tensor(alpha, dtype=x.dtype, device=x.device)\n    if a.ndim > 0 and a.shape != ():\n        # Allow broadcasting but encourage scalar use; rely on PyTorch broadcasting\n        a = a\n\n    g = _to_broadcast_param(gamma, x, name=\"gamma\")\n    b = _to_broadcast_param(beta, x, name=\"beta\")\n\n    # Core operation: elementwise tanh with affine transform\n    y = g * torch.tanh(a * x) + b\n    return y\n\n\nclass DynamicTanh(nn.Module):\n    \"\"\"Module wrapper for Dynamic Tanh.\n\n    Optionally exposes learnable gamma and beta; alpha can be fixed or learnable as well.\n    \"\"\"\n    def __init__(\n        self,\n        num_features: int,\n        alpha: float = 1.0,\n        gamma_init: float = 1.0,\n        beta_init: float = 0.0,\n        learnable: bool = False,\n        learnable_alpha: bool = False,\n    ):\n        super().__init__()\n        self.num_features = num_features\n        if learnable:\n            self.gamma = nn.Parameter(torch.full((num_features,), gamma_init))\n            self.beta = nn.Parameter(torch.full((num_features,), beta_init))\n        else:\n            self.register_buffer(\"gamma\", torch.full((num_features,), gamma_init))\n            self.register_buffer(\"beta\", torch.full((num_features,), beta_init))\n        if learnable_alpha:\n            self.alpha = nn.Parameter(torch.tensor(float(alpha)))\n        else:\n            self.register_buffer(\"alpha\", torch.tensor(float(alpha)))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return dynamic_tanh(x, self.alpha, self.gamma, self.beta)\n\n\ndef solution():\n    # Example usage matching the problem statement\n    x = torch.tensor([[[0.14115588, 0.00372817, 0.24126647, 0.22183601]]], dtype=torch.float32)\n    gamma = torch.ones(4, dtype=torch.float32)\n    beta = torch.zeros(4, dtype=torch.float32)\n    alpha = 0.5\n\n    y = dynamic_tanh(x, alpha, gamma, beta)\n    print(y)\n\n    # Optional check against the expected approximate output\n    expected = torch.tensor([[[0.0705, 0.0019, 0.1201, 0.1105]]], dtype=torch.float32)\n    assert torch.allclose(y, expected, atol=5e-4), f\"Output deviates from expected: {y} vs {expected}\"\n\n    return y\n",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(1)",
  "platform": "deepml"
};
