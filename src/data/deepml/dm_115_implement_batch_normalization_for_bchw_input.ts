import { Problem } from '../../types';

export const DM_115_IMPLEMENT_BATCH_NORMALIZATION_FOR_BCHW_INPUT: Problem = {
  "id": "dm_115_implement_batch_normalization_for_bchw_input",
  "title": "Implement Batch Normalization for BCHW Input",
  "difficulty": "Easy",
  "tags": [
    "Neural Networks",
    "CNNs",
    "Matrix Operations"
  ],
  "descriptionMarkdown": "Implement a function that performs Batch Normalization on a 4D array representing a batch of feature maps in BCHW format (batch, channels, height, width). For each channel, normalize the input across the batch and spatial dimensions, then apply scale (gamma) and shift (beta). Use the provided epsilon value to ensure numerical stability.\n\nExample:\n\n- Input:\n  - `B, C, H, W = 2, 2, 2, 2`\n  - `np.random.seed(42)`\n  - `X = np.random.randn(B, C, H, W)`\n  - `gamma = np.ones(C).reshape(1, C, 1, 1)`\n  - `beta = np.zeros(C).reshape(1, C, 1, 1)`\n\n- Output:\n```\n[[[[ 0.42859934, -0.51776438], [ 0.65360963,  1.95820707]], [[ 0.02353721,  0.02355215], [ 1.67355207,  0.93490043]]], [[[-1.01139563,  0.49692747], [-1.00236882, -1.00581468]], [[ 0.45676349, -1.50433085], [-1.33293647, -0.27503802]]]]\n```\n\nNormalize each channel c using:\n\n- `mean_c = mean over (B, H, W)`\n- `var_c = variance over (B, H, W)`\n- `X_hat = (X - mean_c) / sqrt(var_c + epsilon)`\n- `Y = gamma * X_hat + beta`",
  "solutionExplanation": "Batch Normalization standardizes activations per channel to have zero mean and unit variance, then applies a learned affine transform. For BCHW tensors, we compute the mean and variance across the batch and spatial dimensions (B, H, W) independently for each channel C. This ensures that each feature map channel is normalized consistently across the mini-batch and all spatial locations.\n\nGiven input X of shape [B, C, H, W], we compute mean and variance as mean = E[X] and var = E[(X \u2212 mean)^2] along dimensions (0, 2, 3). We use an epsilon term to maintain numerical stability when taking the square root. The normalized tensor is X_hat = (X \u2212 mean) / sqrt(var + eps). Finally, we apply per-channel scale and shift: Y = gamma * X_hat + beta, where gamma and beta are broadcastable to [1, C, 1, 1]. We use unbiased=False for variance to match the population variance used in typical BatchNorm forward passes.",
  "solutionCode": "import torch\nimport torch.nn as nn\nfrom typing import Tuple\n\n\ndef batch_normalization_bchw(x: torch.Tensor, gamma: torch.Tensor, beta: torch.Tensor, epsilon: float = 1e-5) -> torch.Tensor:\n    \"\"\"\n    Perform Batch Normalization over BCHW tensor per-channel across (B, H, W).\n\n    Args:\n        x (torch.Tensor): Input tensor of shape [B, C, H, W].\n        gamma (torch.Tensor): Scale parameter, shape [C] or broadcastable to [1, C, 1, 1].\n        beta (torch.Tensor): Shift parameter, shape [C] or broadcastable to [1, C, 1, 1].\n        epsilon (float): Small constant for numerical stability.\n\n    Returns:\n        torch.Tensor: Batch-normalized tensor with same shape as input.\n    \"\"\"\n    if x.dim() != 4:\n        raise ValueError(f\"Input x must be 4D BCHW, got shape {tuple(x.shape)}\")\n\n    B, C, H, W = x.shape\n\n    # Ensure gamma and beta are broadcastable to [1, C, 1, 1]\n    def _to_bchw(param: torch.Tensor, name: str) -> torch.Tensor:\n        if param.dim() == 1:\n            if param.numel() != C:\n                raise ValueError(f\"{name} must have length C={C}, got {param.numel()}\")\n            param = param.view(1, C, 1, 1)\n        elif param.shape != (1, C, 1, 1):\n            try:\n                param = param.view(1, C, 1, 1)\n            except Exception as e:\n                raise ValueError(f\"{name} must be shape [C] or [1, C, 1, 1], got {tuple(param.shape)}\") from e\n        return param.to(dtype=x.dtype, device=x.device)\n\n    gamma = _to_bchw(gamma, \"gamma\")\n    beta = _to_bchw(beta, \"beta\")\n\n    # Compute per-channel mean and variance across (B, H, W)\n    mean = x.mean(dim=(0, 2, 3), keepdim=True)  # [1, C, 1, 1]\n    var = x.var(dim=(0, 2, 3), unbiased=False, keepdim=True)  # population variance\n\n    # Normalize\n    x_hat = (x - mean) / torch.sqrt(var + epsilon)\n\n    # Scale and shift\n    y = x_hat * gamma + beta\n    return y\n\n\ndef solution() -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Demonstrates batch normalization over a toy BCHW input.\n\n    Returns:\n        A tuple (out, params) where out is the normalized output and params is a tensor\n        concatenating per-channel mean and variance for quick inspection.\n    \"\"\"\n    # Example usage similar to the prompt (sizes only); values will differ due to torch RNG\n    B, C, H, W = 2, 2, 2, 2\n    torch.manual_seed(42)\n    x = torch.randn(B, C, H, W)\n    gamma = torch.ones(C)\n    beta = torch.zeros(C)\n\n    out = batch_normalization_bchw(x, gamma, beta, epsilon=1e-5)\n\n    # Also return per-channel mean/var for verification if desired\n    mean = x.mean(dim=(0, 2, 3), keepdim=False)\n    var = x.var(dim=(0, 2, 3), unbiased=False, keepdim=False)\n    params = torch.cat([mean, var])  # just for demonstration\n    return out, params\n\n\nif __name__ == \"__main__\":\n    out, params = solution()\n    print(\"Normalized output shape:\", out.shape)\n    print(\"Output sample:\\n\", out)\n",
  "timeComplexity": "O(B * C * H * W)",
  "spaceComplexity": "O(B * C * H * W)",
  "platform": "deepml"
};
