import { Problem } from '../../types';

export const DM_174_TRAIN_A_SIMPLE_GAN_ON_1D_GAUSSIAN_DATA: Problem = {
  "id": "dm_174_train_a_simple_gan_on_1d_gaussian_data",
  "title": "Train a Simple GAN on 1D Gaussian Data",
  "difficulty": "Hard",
  "tags": [
    "Neural Networks",
    "Probability",
    "Loss Functions",
    "Activation Functions",
    "Gradient Descent",
    "Backpropagation",
    "Optimization"
  ],
  "descriptionMarkdown": "In this task, you will train a Generative Adversarial Network (GAN) to learn a one-dimensional Gaussian distribution. The GAN consists of a generator that maps latent noise to 1D samples and a discriminator that estimates the probability that a given sample is real. Both networks should have one hidden layer with ReLU activation in the hidden layer. The generator's output layer is linear, while the discriminator's output layer uses a sigmoid activation.\n\nTraining must use the standard non-saturating GAN objective for the generator and binary cross-entropy loss for the discriminator. In the PyTorch version, parameters must be updated using stochastic gradient descent (SGD) with the specified learning rate. The training loop should alternate between updating the discriminator and the generator in each iteration.\n\nYour function must return the trained generator forward function `gen_forward(z)`, which produces generated samples given latent noise. The function should accept a NumPy array or a PyTorch tensor and return a tuple `(x_gen, _, _)`, where `x_gen` is a NumPy array of generated samples.\n\nFunction signature:\n- `train_gan(mean_real: float, std_real: float, latent_dim: int = 1, hidden_dim: int = 16, learning_rate: float = 0.001, epochs: int = 5000, batch_size: int = 128, seed: int = 42)`\n\nExample:\n```\ngen_forward = train_gan(4.0, 1.25, epochs=1000, seed=42)\nz = np.random.normal(0, 1, (500, 1))\nx_gen, _, _ = gen_forward(z)\n(round(np.mean(x_gen), 4), round(np.std(x_gen), 4))\n```",
  "solutionExplanation": "We model a simple GAN with two small multilayer perceptrons (MLPs). The generator G takes latent noise z ~ N(0, I) of dimension `latent_dim` and produces a 1D output via a network with one hidden ReLU layer and a linear output layer. The discriminator D takes a 1D input and outputs a probability via a network with one hidden ReLU layer and a sigmoid output. We train the discriminator to distinguish real samples (drawn from N(mean_real, std_real^2)) from fake samples G(z), using binary cross-entropy (BCE) loss.\n\nWe use the non-saturating GAN objective for the generator: rather than minimizing log(1 - D(G(z))), we maximize log D(G(z)), which corresponds to minimizing BCE with target 1 on discriminator outputs for fake samples. Training alternates per iteration: first update D with a batch of real and fake samples (detaching fake samples to avoid backprop through G), then update G to fool D. Parameters are updated with SGD at the specified learning rate. After training, we return a `gen_forward(z)` callable that accepts NumPy arrays or PyTorch tensors and returns generated samples as a NumPy array (with two placeholder Nones to match the example API).\n\nThis setup allows the generator to learn the mean and variance of the target 1D Gaussian by matching D's decision boundary. Alternating updates and the non-saturating loss help stabilize training and avoid vanishing gradients for the generator.",
  "solutionCode": "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom typing import Callable, Tuple, Union\n\n\nclass Generator(nn.Module):\n    def __init__(self, latent_dim: int, hidden_dim: int):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(latent_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, 1)  # linear output\n        )\n        self._init_weights()\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                # He init for ReLU layers, Xavier for last is fine via same rule\n                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n\n    def forward(self, z: torch.Tensor) -> torch.Tensor:\n        return self.net(z)\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, hidden_dim: int):\n        super().__init__()\n        self.feature = nn.Sequential(\n            nn.Linear(1, hidden_dim),\n            nn.ReLU(inplace=True)\n        )\n        self.out = nn.Sequential(\n            nn.Linear(hidden_dim, 1),\n            nn.Sigmoid()  # sigmoid output for BCE\n        )\n        self._init_weights()\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        h = self.feature(x)\n        return self.out(h)\n\n\ndef train_gan(\n    mean_real: float,\n    std_real: float,\n    latent_dim: int = 1,\n    hidden_dim: int = 16,\n    learning_rate: float = 0.001,\n    epochs: int = 5000,\n    batch_size: int = 128,\n    seed: int = 42,\n) -> Callable[[Union[np.ndarray, torch.Tensor]], Tuple[np.ndarray, None, None]]:\n    \"\"\"\n    Train a simple GAN to learn a 1D Gaussian distribution using SGD.\n\n    Returns a callable gen_forward(z) that produces generated samples as a NumPy array\n    and two placeholder None values to match the expected (x_gen, _, _) signature.\n    \"\"\"\n    # Reproducibility\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n    device = torch.device('cpu')\n\n    # Models\n    G = Generator(latent_dim, hidden_dim).to(device)\n    D = Discriminator(hidden_dim).to(device)\n\n    # Losses and optimizers (SGD as required)\n    bce = nn.BCELoss()\n    opt_G = optim.SGD(G.parameters(), lr=learning_rate)\n    opt_D = optim.SGD(D.parameters(), lr=learning_rate)\n\n    # Training loop\n    G.train()\n    D.train()\n\n    for _ in range(epochs):\n        # ======== Discriminator update ========\n        # Real samples from target Gaussian\n        x_real = torch.normal(mean=torch.full((batch_size, 1), float(mean_real)),\n                              std=float(std_real)).to(device)\n        y_real = torch.ones((batch_size, 1), device=device)\n\n        # Fake samples\n        z = torch.randn(batch_size, latent_dim, device=device)\n        with torch.no_grad():\n            x_fake_detached = G(z).detach()\n        y_fake = torch.zeros((batch_size, 1), device=device)\n\n        opt_D.zero_grad(set_to_none=True)\n        p_real = D(x_real)\n        p_fake = D(x_fake_detached)\n        loss_D = 0.5 * (bce(p_real, y_real) + bce(p_fake, y_fake))\n        loss_D.backward()\n        opt_D.step()\n\n        # ======== Generator update (non-saturating) ========\n        z2 = torch.randn(batch_size, latent_dim, device=device)\n        opt_G.zero_grad(set_to_none=True)\n        x_fake = G(z2)\n        p_fake_for_G = D(x_fake)\n        # Non-saturating: maximize log D(G(z)) -> minimize BCE with target 1\n        loss_G = bce(p_fake_for_G, y_real)\n        loss_G.backward()\n        opt_G.step()\n\n    # Switch to eval for inference\n    G.eval()\n\n    def gen_forward(z_in: Union[np.ndarray, torch.Tensor]) -> Tuple[np.ndarray, None, None]:\n        \"\"\"Generate samples given latent noise z. Accepts NumPy or torch input.\n        Returns (x_gen_numpy, None, None) to match expected API.\n        \"\"\"\n        with torch.no_grad():\n            if isinstance(z_in, np.ndarray):\n                z_t = torch.from_numpy(z_in.astype(np.float32))\n            elif isinstance(z_in, torch.Tensor):\n                z_t = z_in.to(dtype=torch.float32)\n            else:\n                raise TypeError(\"z must be a NumPy array or torch.Tensor\")\n            x_t = G(z_t)\n            x_np = x_t.cpu().numpy()\n        return x_np, None, None\n\n    return gen_forward\n\n\ndef solution():\n    # Return the train_gan function as the primary entry point\n    return train_gan\n\n\nif __name__ == \"__main__\":\n    # Example usage\n    tg = train_gan(4.0, 1.25, epochs=1000, seed=42)\n    z = np.random.normal(0, 1, (500, 1)).astype(np.float32)\n    x_gen, _, _ = tg(z)\n    print(round(float(np.mean(x_gen)), 4), round(float(np.std(x_gen)), 4))\n",
  "timeComplexity": "O(epochs * batch_size * hidden_dim)",
  "spaceComplexity": "O(hidden_dim + model_params)",
  "platform": "deepml"
};
