import { Problem } from '../../types';

export const DM_159_INCREMENTAL_MEAN_FOR_ONLINE_REWARD_ESTIMATION: Problem = {
  "id": "dm_159_incremental_mean_for_online_reward_estimation",
  "title": "Incremental Mean for Online Reward Estimation",
  "difficulty": "Easy",
  "tags": [
    "Probability"
  ],
  "descriptionMarkdown": "Implement an efficient method to update the mean reward for a k-armed bandit action after receiving each new reward, without storing the full history of rewards. Given the previous mean estimate (Q_prev), the number of times the action has been selected including the current selection (k), and a new reward (R), compute the updated mean using the incremental formula:\n\nnew_Q = Q_prev + (1 / k) * (R - Q_prev)\n\nExample:\n- Input: Q_prev = 2.0, k = 2, R = 6.0\n- Output: 4.0 (since 2.0 + (1/2) * (6.0 - 2.0) = 4.0)\n\nThis approach avoids storing all past rewards and updates the estimate in constant time and space.",
  "solutionExplanation": "The incremental mean update leverages the identity for updating the average when a new sample arrives. If Q_prev is the mean of the previous k-1 samples and R is the new sample, then the new mean after k samples is Q_new = Q_prev + (R \u2212 Q_prev) / k. This can be derived from the definition of the mean by expressing the new sum as the old sum plus the new sample and dividing by k, then simplifying to avoid explicitly storing or recomputing the sum.\n\nThis method is memory-efficient (O(1) space) and numerically stable for streaming updates. It is well-suited for online reinforcement learning scenarios like k-armed bandits where action-value estimates must be updated after each reward without retaining the full reward history. Note that k must represent the total count after incorporating the new reward, matching the standard bandit update convention.",
  "solutionCode": "import torch\nimport torch.nn as nn\n\ndef incremental_mean(Q_prev, k, R):\n    \"\"\"\n    Incrementally update the mean reward for a k-armed bandit action.\n\n    Args:\n        Q_prev (float or torch.Tensor): Previous mean estimate (before receiving R).\n        k (int, float, or torch.Tensor): Total count of selections AFTER receiving the new reward (must be > 0).\n        R (float or torch.Tensor): Newly observed reward.\n\n    Returns:\n        float or torch.Tensor: Updated mean estimate. Returns a Python float if scalar inputs are provided.\n    \"\"\"\n    # Convert inputs to tensors for vectorized, numerically stable computation.\n    Q_prev_t = torch.as_tensor(Q_prev, dtype=torch.float32)\n    k_t = torch.as_tensor(k, dtype=torch.float32)\n    R_t = torch.as_tensor(R, dtype=torch.float32)\n\n    # Validate k to avoid division by zero or invalid updates.\n    if torch.any(k_t <= 0):\n        raise ValueError(\"k must be positive (count after receiving the new reward).\")\n\n    # Incremental mean update: Q_new = Q_prev + (R - Q_prev) / k\n    new_Q = Q_prev_t + (R_t - Q_prev_t) / k_t\n\n    # Return a Python float if the result is scalar for convenience (e.g., with round()).\n    return new_Q.item() if new_Q.numel() == 1 else new_Q\n\n\ndef solution():\n    # Example usage matching the prompt\n    Q_prev = 2.0\n    k = 2  # total count after receiving R\n    R = 6.0\n    new_Q = incremental_mean(Q_prev, k, R)\n    print(round(new_Q, 2))  # Expected: 4.0\n    return new_Q\n",
  "timeComplexity": "O(1)",
  "spaceComplexity": "O(1)",
  "platform": "deepml"
};
