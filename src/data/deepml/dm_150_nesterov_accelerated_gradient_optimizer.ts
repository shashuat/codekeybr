import { Problem } from '../../types';

export const DM_150_NESTEROV_ACCELERATED_GRADIENT_OPTIMIZER: Problem = {
  "id": "dm_150_nesterov_accelerated_gradient_optimizer",
  "title": "Nesterov Accelerated Gradient Optimizer",
  "difficulty": "Easy",
  "tags": [
    "Optimization",
    "Gradient Descent"
  ],
  "descriptionMarkdown": "Implement the Nesterov Accelerated Gradient (NAG) optimizer update step function. Your function should take the current parameter value, a gradient function, and the current velocity as inputs, and return the updated parameter value and new velocity. Use the \"look-ahead\" approach where momentum is applied before computing the gradient. The implementation should handle both scalar and array (vector) inputs.\n\nExample:\n- Input: parameter = 1.0, grad_fn = lambda x: x, velocity = 0.1\n- Output: (0.9009, 0.0991)\n\nReasoning:\nUsing NAG with learning_rate=0.01 and momentum=0.9, we compute the gradient at the look-ahead point (parameter - momentum * velocity). The new velocity and parameter are then updated accordingly, yielding 0.9009 and 0.0991.",
  "solutionExplanation": "Nesterov Accelerated Gradient (NAG) improves upon classical momentum by computing the gradient at a look-ahead point instead of the current parameter. Intuitively, we first move in the direction of the previous momentum, then evaluate the gradient, and finally update with this more informative gradient.\n\nUsing the sign convention that updates subtract velocity from parameters, the NAG update can be written as:\n- Look-ahead: x_look = x - \u03bc v\n- Gradient at look-ahead: g = \u2207f(x_look)\n- Velocity update: v_new = \u03bc v + \u03b7 g\n- Parameter update: x_new = x - v_new\nwhere \u03b7 is the learning rate and \u03bc is the momentum. This matches the example: with x=1.0, v=0.1, \u03b7=0.01, \u03bc=0.9, we get x_look=0.91, g=0.91, v_new=0.0991, and x_new=0.9009. The implementation uses PyTorch tensors, supports scalars and arrays, and avoids autograd because the gradient is provided by grad_fn.",
  "solutionCode": "import torch\nfrom typing import Callable, Tuple, Union\n\nTensorOrScalar = Union[torch.Tensor, float, int]\n\ndef nag_optimizer(parameter: TensorOrScalar,\n                  grad_fn: Callable[[torch.Tensor], torch.Tensor],\n                  velocity: TensorOrScalar,\n                  learning_rate: float = 0.01,\n                  momentum: float = 0.9) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Perform one Nesterov Accelerated Gradient (NAG) update step.\n\n    Uses the look-ahead point: x_look = x - momentum * v\n    Then updates:\n      g = grad_fn(x_look)\n      v_new = momentum * v + learning_rate * g\n      x_new = x - v_new\n\n    Args:\n        parameter: Current parameter value (scalar or tensor-like)\n        grad_fn: Function mapping a tensor to its gradient tensor at that point\n        velocity: Current velocity (same shape as parameter)\n        learning_rate: Learning rate (eta)\n        momentum: Momentum coefficient (mu)\n\n    Returns:\n        (updated_parameter, updated_velocity) as torch.Tensors\n    \"\"\"\n    # Convert inputs to tensors while preserving device/dtype of parameter when possible\n    param_t = torch.as_tensor(parameter)\n    vel_t = torch.as_tensor(velocity, dtype=param_t.dtype, device=param_t.device)\n\n    # Look-ahead point\n    lookahead = param_t - momentum * vel_t\n\n    # Compute gradient at the look-ahead point. Ensure tensor type/device consistency.\n    grad = grad_fn(lookahead)\n    grad_t = torch.as_tensor(grad, dtype=param_t.dtype, device=param_t.device)\n\n    # NAG velocity and parameter updates\n    new_velocity = momentum * vel_t + learning_rate * grad_t\n    updated_parameter = param_t - new_velocity\n\n    return updated_parameter, new_velocity\n\n\ndef solution():\n    # Example usage replicating the prompt\n    parameter = 1.0\n    velocity = 0.1\n    grad_fn = lambda x: x  # gradient of f(x) = 0.5 * x^2\n\n    updated_param, updated_vel = nag_optimizer(parameter, grad_fn, velocity,\n                                               learning_rate=0.01, momentum=0.9)\n\n    # Return Python scalars for readability in this example\n    return float(updated_param), float(updated_vel)\n",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(1)",
  "platform": "deepml"
};
