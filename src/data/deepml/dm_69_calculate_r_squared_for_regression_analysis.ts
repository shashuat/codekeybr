import { Problem } from '../../types';

export const DM_69_CALCULATE_R_SQUARED_FOR_REGRESSION_ANALYSIS: Problem = {
  "id": "dm_69_calculate_r_squared_for_regression_analysis",
  "title": "Calculate R-squared for Regression Analysis",
  "difficulty": "Easy",
  "tags": [
    "Linear Algebra",
    "Loss Functions",
    "Matrix Operations"
  ],
  "descriptionMarkdown": "Task: Compute the R-squared Value in Regression Analysis\n\nR-squared, also known as the coefficient of determination, measures how well the independent variables explain the variability of the dependent variable in a regression model.\n\nYour task: Implement the function r_squared(y_true, y_pred) that calculates the R-squared value, given arrays (or tensors) of true values y_true and predicted values y_pred.\n\nExample:\n\nInput:\n\n```\nimport numpy as np\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1.1, 2.1, 2.9, 4.2, 4.8])\nprint(r_squared(y_true, y_pred))\n```\n\nOutput:\n\n```\n0.989\n```\n\nReasoning: The R-squared value is calculated to be 0.989, indicating that the regression model explains 98.9% of the variance in the dependent variable.",
  "solutionExplanation": "R-squared (coefficient of determination) is defined as R^2 = 1 - SS_res / SS_tot, where SS_res = \u03a3(y_true - y_pred)^2 is the residual sum of squares, and SS_tot = \u03a3(y_true - mean(y_true))^2 is the total sum of squares. Intuitively, SS_tot measures the total variance in the target, while SS_res measures the variance left unexplained by the model. A higher R^2 (closer to 1) means the model explains more variance.\n\nIn implementation, we compute the mean of y_true, then use vectorized tensor operations to compute SS_res and SS_tot. We include a robust edge-case handling: if the target has zero variance (SS_tot = 0), R^2 is undefined mathematically. Following common practice, we return 1.0 if predictions are perfectly equal to the targets (SS_res = 0), otherwise 0.0. This mirrors practical behavior in popular libraries and avoids division by zero.\n\nThe function accepts lists, NumPy arrays, or PyTorch tensors and converts them to float tensors. We flatten inputs to 1D and validate matching sizes. The computation uses efficient PyTorch reductions and dot products.",
  "solutionCode": "import torch\nimport torch.nn as nn\n\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Compute R-squared (coefficient of determination) between true and predicted values.\n\n    Args:\n        y_true: array-like or torch.Tensor of shape (N, ...) with ground-truth values\n        y_pred: array-like or torch.Tensor of shape (N, ...) with predicted values\n\n    Returns:\n        float: R^2 score in [-inf, 1]. Returns 1.0 if both y_true and y_pred are constant and equal,\n               else 0.0 when y_true is constant but predictions are not equal.\n    \"\"\"\n    # Convert inputs to float tensors\n    yt = torch.as_tensor(y_true, dtype=torch.float32)\n    yp = torch.as_tensor(y_pred, dtype=torch.float32)\n\n    # Ensure same number of elements\n    if yt.numel() != yp.numel():\n        raise ValueError(f\"y_true and y_pred must contain the same number of elements: {yt.numel()} != {yp.numel()}\")\n\n    # Flatten to 1D for simple reduction operations\n    yt = yt.reshape(-1)\n    yp = yp.reshape(-1)\n\n    # Residual sum of squares: sum((y_true - y_pred)^2)\n    residual = yt - yp\n    ss_res = torch.dot(residual, residual)\n\n    # Total sum of squares: sum((y_true - mean(y_true))^2)\n    yt_mean = yt.mean()\n    diff = yt - yt_mean\n    ss_tot = torch.dot(diff, diff)\n\n    # Handle edge case: zero variance in y_true\n    if torch.isclose(ss_tot, torch.tensor(0.0, dtype=ss_tot.dtype)):\n        # Perfect prediction -> 1.0, otherwise 0.0 (practical convention)\n        return 1.0 if torch.isclose(ss_res, torch.tensor(0.0, dtype=ss_res.dtype)) else 0.0\n\n    r2 = 1.0 - (ss_res / ss_tot)\n    return float(r2.item())\n\n\ndef solution():\n    # Example usage (matches the problem statement)\n    y_true = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32)\n    y_pred = torch.tensor([1.1, 2.1, 2.9, 4.2, 4.8], dtype=torch.float32)\n    r2 = r_squared(y_true, y_pred)\n    # Print rounded to 3 decimals to match the example output format\n    print(round(r2, 3))\n\nif __name__ == \"__main__\":\n    solution()\n",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(1)",
  "platform": "deepml"
};
