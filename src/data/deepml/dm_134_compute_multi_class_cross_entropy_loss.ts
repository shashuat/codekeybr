import { Problem } from '../../types';

export const DM_134_COMPUTE_MULTI_CLASS_CROSS_ENTROPY_LOSS: Problem = {
  "id": "dm_134_compute_multi_class_cross_entropy_loss",
  "title": "Compute Multi-class Cross-Entropy Loss",
  "difficulty": "Easy",
  "tags": [
    "Loss Functions",
    "Probability",
    "Neural Networks"
  ],
  "descriptionMarkdown": "Implement a function that computes the average cross-entropy loss for a batch of predictions in a multi-class classification task. The function should take a batch of predicted probabilities and one-hot encoded true labels, and return the average cross-entropy loss. For numerical stability, clip probabilities by a small epsilon.\n\nExample:\n- Input:\n  - predicted_probs = [[0.7, 0.2, 0.1], [0.3, 0.6, 0.1]]\n  - true_labels = [[1, 0, 0], [0, 1, 0]]\n- Output: 0.4338\n\nReasoning:\nThe predicted probabilities for the correct classes are 0.7 and 0.6. The cross-entropy is computed as -mean(log(0.7), log(0.6)), which is approximately 0.4338.",
  "solutionExplanation": "For multi-class classification with one-hot targets, the cross-entropy loss for each sample is defined as L = -\u2211_c y_c log(p_c), where y is the one-hot label and p is the predicted probability distribution over classes. Because only the correct class has y = 1, this reduces to -log(p_correct). The batch loss is the mean of per-sample losses.\n\nTo ensure numerical stability and avoid log(0), we clip predicted probabilities to [epsilon, 1 - epsilon] before taking the logarithm. We then compute log probabilities, select the correct class log-probabilities via elementwise multiplication with the one-hot labels and summation over the class dimension, and finally take the negative mean across the batch.\n\nThis direct computation assumes inputs are probabilities (not logits). If logits were provided, one should instead use log-softmax plus NLL or PyTorch's built-in CrossEntropyLoss. Here we follow the problem statement and operate on probability inputs.",
  "solutionCode": "import torch\nimport torch.nn.functional as F\n\ndef compute_cross_entropy_loss(predicted_probs, true_labels, epsilon: float = 1e-15) -> torch.Tensor:\n    \"\"\"\n    Compute the average multi-class cross-entropy loss for a batch.\n\n    Args:\n        predicted_probs: Tensor or array-like of shape [batch_size, num_classes], probabilities per class.\n        true_labels: Tensor or array-like of shape [batch_size, num_classes], one-hot labels.\n        epsilon: Small constant for numerical stability when taking logs.\n\n    Returns:\n        A scalar torch.Tensor representing the average cross-entropy loss over the batch.\n    \"\"\"\n    probs = torch.as_tensor(predicted_probs, dtype=torch.float32)\n    targets = torch.as_tensor(true_labels, dtype=torch.float32)\n\n    if probs.ndim != 2 or targets.ndim != 2 or probs.shape != targets.shape:\n        raise ValueError(\"predicted_probs and true_labels must be 2D and have the same shape: [batch_size, num_classes].\")\n\n    # Clip to avoid log(0) and overly confident probabilities\n    probs = probs.clamp(min=epsilon, max=1.0 - epsilon)\n\n    # Compute log probabilities and pick correct-class log-prob via one-hot masking\n    log_probs = probs.log()\n    correct_log_probs = (targets * log_probs).sum(dim=1)  # shape: [batch_size]\n\n    loss = -correct_log_probs.mean()\n    return loss\n\n\ndef solution():\n    # Example usage\n    predicted_probs = [[0.7, 0.2, 0.1], [0.3, 0.6, 0.1]]\n    true_labels = [[1, 0, 0], [0, 1, 0]]\n\n    loss = compute_cross_entropy_loss(predicted_probs, true_labels)\n    # Should be approximately 0.4338\n    print(\"Loss:\", float(loss))\n    return float(loss)\n",
  "timeComplexity": "O(B*C)",
  "spaceComplexity": "O(1)",
  "platform": "deepml"
};
