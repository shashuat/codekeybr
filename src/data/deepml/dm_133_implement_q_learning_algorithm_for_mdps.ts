import { Problem } from '../../types';

export const DM_133_IMPLEMENT_Q_LEARNING_ALGORITHM_FOR_MDPS: Problem = {
  "id": "dm_133_implement_q_learning_algorithm_for_mdps",
  "title": "Implement Q-Learning Algorithm for MDPs",
  "difficulty": "Medium",
  "tags": [
    "Probability",
    "Optimization"
  ],
  "descriptionMarkdown": "Write a function that implements the Q-Learning algorithm to learn the optimal Q-table for a given Markov Decision Process (MDP).\n\nInputs:\n- num_states: integer >= 1\n- num_actions: integer >= 1\n- P: a 3D NumPy array of shape (num_states, num_actions, num_states) with valid transition probabilities (each distribution sums to 1)\n- R: a 2D NumPy array of shape (num_states, num_actions) with rewards\n- terminal_states: a list/array of unique integers in [0, num_states - 1]\n- alpha: learning rate in [0, 1]\n- gamma: discount factor in [0, 1]\n- epsilon: exploration rate in [0, 1]\n- num_episodes: integer >= 1\n\nRequirements:\n- Initialize a Q-table with zeros and update it using the Q-Learning rule.\n- Use epsilon-greedy action selection.\n- Start each episode from a random non-terminal state and terminate when a terminal state is reached.\n\nReturn a 2D NumPy array of shape (num_states, num_actions) representing the learned Q-table.\n\nExample:\nInput:\nimport numpy as np; np.random.seed(42); P = np.array([[[0, 1], [1, 0]], [[1, 0], [1, 0]]]); R = np.array([[1, 0], [0, 0]]); terminal_states = [1]; print(q_learning(2, 2, P, R, terminal_states, 0.1, 0.9, 0.1, 10))\n\nOutput:\n[[0.65132156, 0.052902  ], [0., 0.]]",
  "solutionExplanation": "Q-Learning is an off-policy temporal-difference method that learns an action-value function Q(s, a) for an MDP without requiring a model of the reward for future steps. The core update is: Q(s, a) \u2190 Q(s, a) + \u03b1 [r + \u03b3 max_a' Q(s', a') \u2212 Q(s, a)], where r is the reward from taking action a in state s and transitioning to s'. For terminal next states, the bootstrap term max_a' Q(s', a') is set to zero, so the target reduces to r.\n\nWe use an epsilon-greedy strategy to balance exploration and exploitation: with probability \u03b5, select a random action; otherwise, choose an action with the highest Q-value (breaking ties randomly). Each episode starts from a random non-terminal state and proceeds by sampling next states from the provided transition probabilities P until a terminal state is reached. The Q-table is stored as a PyTorch tensor to leverage tensor operations, while inputs are converted from NumPy for compatibility. Finally, we return the learned Q-table as a NumPy array.\n\nThis approach iteratively refines Q-values across episodes using the Bellman optimality principle. With sufficient exploration and appropriate hyperparameters, the Q-table converges toward optimal action values for the MDP.",
  "solutionCode": "import torch\nimport numpy as np\nfrom typing import List, Sequence\n\ndef q_learning(num_states: int,\n               num_actions: int,\n               P: np.ndarray,\n               R: np.ndarray,\n               terminal_states: Sequence[int],\n               alpha: float,\n               gamma: float,\n               epsilon: float,\n               num_episodes: int) -> np.ndarray:\n    \"\"\"\n    Learn a Q-table using Q-Learning on a tabular MDP.\n\n    Args:\n        num_states: Number of states (S).\n        num_actions: Number of actions (A).\n        P: Transition probabilities, shape (S, A, S), rows sum to 1.\n        R: Reward matrix, shape (S, A).\n        terminal_states: List or array of terminal state indices.\n        alpha: Learning rate in [0, 1].\n        gamma: Discount factor in [0, 1].\n        epsilon: Exploration rate in [0, 1].\n        num_episodes: Number of episodes to run.\n\n    Returns:\n        A NumPy array of shape (S, A) containing the learned Q-table.\n    \"\"\"\n    # Basic validation\n    assert num_states >= 1 and num_actions >= 1, \"num_states and num_actions must be >= 1\"\n    assert 0.0 <= alpha <= 1.0, \"alpha must be in [0,1]\"\n    assert 0.0 <= gamma <= 1.0, \"gamma must be in [0,1]\"\n    assert 0.0 <= epsilon <= 1.0, \"epsilon must be in [0,1]\"\n    assert isinstance(P, np.ndarray) and P.shape == (num_states, num_actions, num_states), \"P must have shape (S, A, S)\"\n    assert isinstance(R, np.ndarray) and R.shape == (num_states, num_actions), \"R must have shape (S, A)\"\n\n    term_set = set(int(s) for s in terminal_states)\n    non_terminal_states = [s for s in range(num_states) if s not in term_set]\n\n    # If all states are terminal, return zeros\n    if len(non_terminal_states) == 0:\n        return np.zeros((num_states, num_actions), dtype=np.float64)\n\n    # Convert to torch tensors for computation\n    Q = torch.zeros((num_states, num_actions), dtype=torch.float64)\n    R_t = torch.as_tensor(R, dtype=torch.float64)\n\n    # We'll use NumPy's RNG (compatible with provided example's seeding)\n    P_np = P  # keep as numpy for sampling with np.random.choice\n\n    # Helper: epsilon-greedy with random tie-breaking\n    def select_action(state: int) -> int:\n        if np.random.rand() < epsilon:\n            return int(np.random.randint(num_actions))\n        q_row = Q[state].cpu().numpy()\n        max_val = q_row.max()\n        best_actions = np.flatnonzero(q_row == max_val)\n        return int(np.random.choice(best_actions))\n\n    # Cap steps to avoid pathological infinite episodes if no terminal is reachable\n    max_steps_per_episode = max(1000, num_states * 100)\n\n    for _ in range(int(num_episodes)):\n        s = int(np.random.choice(non_terminal_states))\n        steps = 0\n        while s not in term_set and steps < max_steps_per_episode:\n            steps += 1\n            a = select_action(s)\n            # Sample next state from categorical distribution P[s, a, :]\n            next_s = int(np.random.choice(num_states, p=P_np[s, a]))\n            # Immediate reward\n            r = float(R_t[s, a].item())\n\n            # Compute TD target\n            if next_s in term_set:\n                target = r\n            else:\n                max_next = float(torch.max(Q[next_s]).item())\n                target = r + gamma * max_next\n\n            # Q update\n            td_error = target - float(Q[s, a].item())\n            Q[s, a] = Q[s, a] + alpha * td_error\n\n            # Transition\n            s = next_s\n\n    return Q.cpu().numpy()\n\nif __name__ == \"__main__\":\n    # Example usage matching the prompt\n    import numpy as np\n    np.random.seed(42)\n    P = np.array([[[0, 1], [1, 0]],\n                  [[1, 0], [1, 0]]], dtype=float)\n    R = np.array([[1, 0],\n                  [0, 0]], dtype=float)\n    terminal_states = [1]\n    Q_learned = q_learning(2, 2, P, R, terminal_states, 0.1, 0.9, 0.1, 10)\n    print(Q_learned)",
  "timeComplexity": "O(num_episodes * average_episode_length)",
  "spaceComplexity": "O(num_states * num_actions)",
  "platform": "deepml"
};
