import { Problem } from '../../types';

export const DM_146_MOMENTUM_OPTIMIZER: Problem = {
  "id": "dm_146_momentum_optimizer",
  "title": "Momentum Optimizer",
  "difficulty": "Easy",
  "tags": [
    "Optimization",
    "Gradient Descent"
  ],
  "descriptionMarkdown": "Implement the momentum optimizer update step function. Your function should take the current parameter value, gradient, and velocity as inputs, and return the updated parameter value and new velocity. The function should also handle scalar and array (tensor) inputs.\n\nExample:\n- Input: parameter = 1.0, grad = 0.1, velocity = 0.1\n- Output: (0.909, 0.091)\n\nReasoning: Using the momentum update variant where the velocity accumulates the scaled gradient, the update is:\n- v_new = momentum * velocity + learning_rate * grad = 0.9 * 0.1 + 0.01 * 0.1 = 0.091\n- parameter_new = parameter - v_new = 1.0 - 0.091 = 0.909",
  "solutionExplanation": "Momentum accelerates gradient-based optimization by accumulating a velocity term that smooths updates along consistent descent directions. In the commonly used formulation that matches the example, the velocity includes the learning rate: v_t = \u03bc v_{t-1} + \u03b1 g_t, and the parameter update is \u03b8_t = \u03b8_{t-1} \u2212 v_t, where \u03bc is the momentum coefficient, \u03b1 is the learning rate, and g_t is the current gradient.\n\nThis formulation differs from an alternative variant (v_t = \u03bc v_{t-1} \u2212 \u03b1 g_t; \u03b8_t = \u03b8_{t-1} + v_t). To reproduce the provided example (parameter=1.0, grad=0.1, velocity=0.1 \u2192 v_new=0.091, param_new=0.909), we implement the former: v_new = \u03bc\u00b7v + \u03b1\u00b7grad and parameter_new = parameter \u2212 v_new. The implementation uses PyTorch tensors and supports scalars and tensors through broadcasting, preserving device and dtype when inputs are tensors.",
  "solutionCode": "import torch\nfrom typing import Tuple, Union\n\nTensorOrScalar = Union[torch.Tensor, float, int]\n\ndef _infer_device_dtype(*args):\n    \"\"\"Infer device and dtype from the first torch.Tensor among args, else default to CPU float32.\"\"\"\n    for x in args:\n        if isinstance(x, torch.Tensor):\n            return x.device, x.dtype\n    return torch.device(\"cpu\"), torch.float32\n\ndef momentum_optimizer(parameter: TensorOrScalar,\n                       grad: TensorOrScalar,\n                       velocity: TensorOrScalar,\n                       learning_rate: float = 0.01,\n                       momentum: float = 0.9) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Perform a single Momentum optimizer update step using the variant:\n      v_new = momentum * velocity + learning_rate * grad\n      param_new = parameter - v_new\n\n    Args:\n        parameter: Current parameter value (scalar or tensor).\n        grad: Current gradient (scalar or tensor; broadcastable to parameter).\n        velocity: Current velocity/momentum term (scalar or tensor; broadcastable to parameter).\n        learning_rate: Learning rate (alpha).\n        momentum: Momentum coefficient (mu).\n\n    Returns:\n        (param_new, v_new): Updated parameter and updated velocity as torch.Tensors.\n    \"\"\"\n    device, dtype = _infer_device_dtype(parameter, grad, velocity)\n\n    # Convert inputs to tensors on a common device and dtype\n    param_t = torch.as_tensor(parameter, device=device, dtype=dtype)\n    grad_t = torch.as_tensor(grad, device=device, dtype=dtype)\n    vel_t = torch.as_tensor(velocity, device=device, dtype=dtype)\n\n    # Compute new velocity and parameter\n    v_new = momentum * vel_t + learning_rate * grad_t\n    param_new = param_t - v_new\n\n    return param_new, v_new\n\n# Example usage\nif __name__ == \"__main__\":\n    # Scalar inputs\n    p, v = momentum_optimizer(1.0, 0.1, 0.1, learning_rate=0.01, momentum=0.9)\n    print(float(p), float(v))  # Expected approximately: 0.909 0.091\n\n    # Tensor inputs\n    parameter = torch.tensor([1.0, 2.0])\n    grad = torch.tensor([0.1, -0.2])\n    velocity = torch.tensor([0.1, 0.0])\n    p_new, v_new = momentum_optimizer(parameter, grad, velocity, learning_rate=0.01, momentum=0.9)\n    print(p_new, v_new)\n",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(1)",
  "platform": "deepml"
};
