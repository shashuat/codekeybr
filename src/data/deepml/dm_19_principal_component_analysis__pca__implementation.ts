import { Problem } from '../../types';

export const DM_19_PRINCIPAL_COMPONENT_ANALYSIS__PCA__IMPLEMENTATION: Problem = {
  "id": "dm_19_principal_component_analysis_pca_implementation",
  "title": "Principal Component Analysis (PCA) Implementation",
  "difficulty": "Medium",
  "tags": [
    "Linear Algebra",
    "Matrix Operations"
  ],
  "descriptionMarkdown": "Write a function that performs Principal Component Analysis (PCA) from scratch.\n\nRequirements:\n- Input: a 2D array where rows are samples and columns are features, and an integer k (number of principal components to return).\n- Standardize the dataset (per-feature z-score: subtract mean and divide by standard deviation). If a feature's standard deviation is zero, treat it as 1 to avoid division by zero.\n- Compute the covariance matrix.\n- Compute eigenvalues and eigenvectors of the covariance matrix.\n- Return the top k principal components (the eigenvectors corresponding to the largest eigenvalues), as a matrix of shape (n_features, k), rounded to 4 decimals.\n- Eigenvector sign convention: for each eigenvector, find its first non-zero entry; if it's negative, flip the sign of the entire vector to ensure consistent orientation.\n\nExample:\n- Input: data = [[1, 2], [3, 4], [5, 6]], k = 1\n- Output: [[0.7071], [0.7071]]\n\nAfter standardizing, computing the covariance matrix, and performing eigendecomposition, the eigenvector for the largest eigenvalue is returned as the principal component. If the first non-zero entry were negative, its sign would be flipped for consistency.",
  "solutionExplanation": "PCA projects data onto directions (principal components) that maximize variance. To compute these directions from scratch, we first standardize each feature to zero mean and unit variance so that features with larger scales do not dominate the covariance. Given standardized data X (n_samples \u00d7 n_features), the sample covariance matrix is C = (X^T X) / (n_samples \u2212 1).\n\nSince C is symmetric positive semi-definite, we use an eigen-decomposition specialized for symmetric matrices. The eigenvectors corresponding to the largest eigenvalues form the principal components. We sort eigenvalues in descending order and select the top k eigenvectors. Because eigenvectors are defined up to sign, we enforce a deterministic sign by checking the first non-zero entry of each eigenvector; if it is negative, we flip the vector. Finally, we round results to four decimals and return a matrix of shape (n_features, k).\n\nThis approach is numerically stable and efficient for moderate feature dimensions because it reduces eigendecomposition to an n_features \u00d7 n_features matrix. The implementation uses torch.linalg.eigh, which is optimized for symmetric matrices and returns eigenvalues in ascending order, so we reverse the order to obtain the top components.",
  "solutionCode": "import torch\nimport torch.nn as nn\nfrom typing import Union\n\ndef pca(data: Union[torch.Tensor, \"np.ndarray\"], k: int) -> torch.Tensor:\n    \"\"\"Perform PCA from scratch using PyTorch and return top-k principal components.\n\n    Args:\n        data: 2D tensor/array of shape (n_samples, n_features). If a NumPy array is passed,\n              it will be converted to a torch.Tensor.\n        k: Number of principal components to return.\n\n    Returns:\n        Tensor of shape (n_features, k) containing the top-k principal components,\n        rounded to 4 decimals. Each eigenvector's sign is flipped if its first non-zero\n        entry is negative to ensure consistent orientation.\n    \"\"\"\n    # Convert input to torch tensor (float64 for numerical stability)\n    if not isinstance(data, torch.Tensor):\n        data = torch.as_tensor(data)\n    if data.dim() != 2:\n        raise ValueError(\"Input data must be a 2D array/tensor of shape (n_samples, n_features)\")\n\n    X = data.to(dtype=torch.float64)\n    n_samples, n_features = X.shape\n    if k < 1 or k > n_features:\n        raise ValueError(\"k must be in the range [1, n_features]\")\n\n    # Standardize features: zero mean, unit variance (handle zero-std features)\n    mean = X.mean(dim=0, keepdim=True)\n    std = X.std(dim=0, unbiased=True, keepdim=True)\n    std_safe = torch.where(std == 0, torch.ones_like(std), std)\n    X_std = (X - mean) / std_safe\n\n    # Sample covariance matrix: (X^T X) / (n_samples - 1)\n    # If n_samples == 1, covariance is undefined; guard accordingly\n    if n_samples < 2:\n        raise ValueError(\"At least two samples are required to compute covariance\")\n    cov = (X_std.T @ X_std) / (n_samples - 1)\n\n    # Eigendecomposition for symmetric matrix; returns ascending eigenvalues\n    evals, evecs = torch.linalg.eigh(cov)\n\n    # Sort by descending eigenvalues\n    idx = torch.argsort(evals, descending=True)\n    evecs = evecs[:, idx]\n\n    # Select top-k eigenvectors (principal components)\n    comps = evecs[:, :k].contiguous()\n\n    # Enforce sign convention: flip if first non-zero entry is negative\n    for i in range(k):\n        vec = comps[:, i]\n        nz = torch.nonzero(vec, as_tuple=False)\n        if nz.numel() > 0:\n            first_nz_idx = nz[0, 0]\n            if vec[first_nz_idx] < 0:\n                comps[:, i] = -vec\n\n    # Round to 4 decimals\n    comps = torch.round(comps * 1e4) / 1e4\n    return comps\n\n\ndef solution():\n    # Example usage\n    data = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n    k = 1\n    pcs = pca(data, k)\n    print(pcs)  # Expected approx: tensor([[0.7071], [0.7071]])\n\nif __name__ == \"__main__\":\n    solution()\n",
  "timeComplexity": "O(n_samples * n_features^2 + n_features^3)",
  "spaceComplexity": "O(n_samples * n_features + n_features^2)",
  "platform": "deepml"
};
