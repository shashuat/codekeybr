import { Problem } from '../../types';

export const DM_79_BINOMIAL_DISTRIBUTION_PROBABILITY: Problem = {
  "id": "dm_79_binomial_distribution_probability",
  "title": "Binomial Distribution Probability",
  "difficulty": "Medium",
  "tags": [
    "Probability"
  ],
  "descriptionMarkdown": "Write a Python function to calculate the probability of achieving exactly k successes in n independent Bernoulli trials, each with probability p of success, using the Binomial distribution formula.\n\nExample:\n- Input: n = 6, k = 2, p = 0.5\n- Output: 0.23438\n\nReasoning: C(6,2) = 15 and the probability is 15 \u00d7 0.5^2 \u00d7 0.5^4 = 15/64 = 0.234375 \u2248 0.23438.",
  "solutionExplanation": "The number of successes X in n independent Bernoulli trials with success probability p follows a Binomial(n, p) distribution. The probability of exactly k successes is given by the binomial probability mass function (PMF): P(X = k) = C(n, k) p^k (1 \u2212 p)^(n \u2212 k), where C(n, k) is the binomial coefficient.\n\nTo compute this robustly and efficiently, we work in log-space to avoid numerical underflow/overflow: log C(n, k) = lgamma(n + 1) \u2212 lgamma(k + 1) \u2212 lgamma(n \u2212 k + 1). Then log P = log C + k log p + (n \u2212 k) log(1 \u2212 p) and P = exp(log P). Using torch.lgamma and torch tensor operations ensures numerical stability and allows easy vectorization. We also handle edge cases p = 0 and p = 1 exactly to return precise probabilities without relying on clamping.",
  "solutionCode": "import torch\nimport torch.nn as nn\n\n\ndef binomial_probability(n: int, k: int, p: float) -> torch.Tensor:\n    # Compute P(X = k) for X ~ Binomial(n, p) using log-space arithmetic.\n    # Supports scalar or broadcastable tensor inputs for n, k, p.\n\n    # Convert inputs to tensors with appropriate dtypes\n    n_t = torch.as_tensor(n, dtype=torch.long)\n    k_t = torch.as_tensor(k, dtype=torch.long)\n    p_t = torch.as_tensor(p, dtype=torch.float64)\n\n    # Basic validation\n    if torch.any(n_t < 0):\n        raise ValueError('n must be non-negative')\n    if torch.any((k_t < 0) | (k_t > n_t)):\n        raise ValueError('k must satisfy 0 <= k <= n')\n    if torch.any((p_t < 0) | (p_t > 1)):\n        raise ValueError('p must be in [0, 1]')\n\n    # Broadcast to a common shape and promote to float64 for numerical stability\n    n_f, k_f, p_f = [t.to(torch.float64) for t in torch.broadcast_tensors(n_t, k_t, p_t)]\n\n    # Clamp p away from {0,1} for log computations, but correct exact cases later\n    eps = torch.finfo(torch.float64).eps\n    p_safe = p_f.clamp(min=eps, max=1 - eps)\n\n    # log C(n, k) = lgamma(n+1) - lgamma(k+1) - lgamma(n-k+1)\n    logC = torch.lgamma(n_f + 1.0) - torch.lgamma(k_f + 1.0) - torch.lgamma(n_f - k_f + 1.0)\n\n    # log P = logC + k*log(p) + (n-k)*log(1-p)\n    log_prob = logC + k_f * torch.log(p_safe) + (n_f - k_f) * torch.log1p(-p_safe)\n    prob = torch.exp(log_prob)\n\n    # Exact fixes for p == 0 or p == 1\n    p0 = (p_f == 0.0)\n    p1 = (p_f == 1.0)\n    if bool(p0.any()):\n        prob = torch.where(p0 & (k_f == 0.0), torch.ones_like(prob), prob)\n        prob = torch.where(p0 & (k_f != 0.0), torch.zeros_like(prob), prob)\n    if bool(p1.any()):\n        prob = torch.where(p1 & (k_f == n_f), torch.ones_like(prob), prob)\n        prob = torch.where(p1 & (k_f != n_f), torch.zeros_like(prob), prob)\n\n    return prob\n\n\ndef solution():\n    # Example usage\n    n, k, p = 6, 2, 0.5\n    prob = binomial_probability(n, k, p)\n    print(f'Probability for n={n}, k={k}, p={p}: {prob.item():.5f}')  # Expected: 0.23438\n    return prob\n",
  "timeComplexity": "O(1)",
  "spaceComplexity": "O(1)",
  "platform": "deepml"
};
