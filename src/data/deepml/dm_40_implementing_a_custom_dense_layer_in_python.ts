import { Problem } from '../../types';

export const DM_40_IMPLEMENTING_A_CUSTOM_DENSE_LAYER_IN_PYTHON: Problem = {
  "id": "dm_40_implementing_a_custom_dense_layer_in_python",
  "title": "Implementing a Custom Dense Layer in Python",
  "difficulty": "Hard",
  "tags": [
    "Neural Networks",
    "Backpropagation",
    "Matrix Operations",
    "Optimization"
  ],
  "descriptionMarkdown": "You are provided with a base Layer class that defines the structure of a neural network layer. Your task is to implement a subclass called Dense, which represents a fully connected neural network layer.\n\nImplement the following in the Dense class:\n\n- Initialization (__init__):\n  - Define the layer with a specified number of neurons (n_units) and an optional input shape (input_shape).\n  - Set up placeholders for the layer's weights (W), biases (w0), and optimizers.\n- Weight Initialization (initialize):\n  - Initialize the weights W using a uniform distribution with a limit of 1 / sqrt(input_shape[0]).\n  - Initialize the bias w0 to zeros.\n  - Initialize optimizers for W and w0.\n- Parameter Count (parameters):\n  - Return the total number of trainable parameters in the layer, including both W and w0.\n- Forward Pass (forward_pass):\n  - Compute the output as X @ W + w0, where X is the input.\n- Backward Pass (backward_pass):\n  - Compute and return the gradient with respect to the input.\n  - If the layer is trainable, update W and w0 using the optimizer's update rule.\n- Output Shape (output_shape):\n  - Return the shape of the output as a tuple (self.n_units,).\n\nExample usage:\n\n- Initialize a Dense layer with 3 neurons and input shape (2,)\n- Use a simple mock optimizer with an update rule: weights - lr * grad\n- Perform a forward pass and a backward pass with sample tensors\n",
  "solutionExplanation": "A fully connected (dense) layer performs an affine transformation Y = XW + b, where X has shape (B, D), W has shape (D, U), and b has shape (1, U). The forward pass is a standard matrix multiplication followed by bias broadcasting. For stable training, weights are initialized from a symmetric uniform distribution with limit 1/sqrt(D), and biases are initialized to zero.\n\nThe backward pass computes gradients using matrix calculus: dL/dX = dL/dY \u00b7 W^T, dL/dW = X^T \u00b7 dL/dY, and dL/db = sum over batch of dL/dY. If the layer is marked trainable, these gradients are fed into an optimizer that applies an update rule (e.g., simple gradient descent) to W and b. We keep the tensors as plain torch.Tensors (without autograd) to match the requirement of a custom backward/update implementation.\n\nThis implementation cleanly separates initialization, forward, backward, and parameter updates, making it easy to integrate with different optimizers while leveraging efficient PyTorch tensor operations.",
  "solutionCode": "import math\nimport copy\nimport torch\n\ntorch.manual_seed(42)\n\nclass Layer(object):\n    def __init__(self):\n        self.input_shape = None\n        self.trainable = True\n\n    def set_input_shape(self, shape):\n        # shape is expected to be a tuple, e.g., (input_dim,)\n        self.input_shape = tuple(shape)\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        # Should be overridden by subclasses\n        return 0\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        super().__init__()\n        self.n_units = int(n_units)\n        if input_shape is not None:\n            self.set_input_shape(input_shape)\n        # Weights and bias will be initialized in initialize()\n        self.W = None   # shape: (in_features, n_units)\n        self.w0 = None  # shape: (1, n_units)\n        self.opt_W = None\n        self.opt_b = None\n        # Cache for backward pass\n        self._X = None\n\n    def initialize(self, optimizer):\n        if self.input_shape is None or len(self.input_shape) == 0:\n            raise ValueError(\"input_shape must be set before initialization.\")\n        in_features = int(self.input_shape[0])\n        limit = 1.0 / math.sqrt(in_features)\n        # Initialize weights: U(-limit, limit), bias: zeros\n        self.W = (torch.empty(in_features, self.n_units).uniform_(-limit, limit))\n        self.w0 = torch.zeros(1, self.n_units)\n        # Make separate optimizer instances for each parameter (to allow stateful optimizers)\n        self.opt_W = copy.deepcopy(optimizer)\n        self.opt_b = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        num_W = 0 if self.W is None else self.W.numel()\n        num_b = 0 if self.w0 is None else self.w0.numel()\n        return int(num_W + num_b)\n\n    def forward_pass(self, X):\n        # X: (B, D)\n        if not isinstance(X, torch.Tensor):\n            X = torch.as_tensor(X, dtype=torch.float32)\n        if self.W is None or self.w0 is None:\n            # Lazy initialization if input shape wasn't provided\n            self.set_input_shape((X.shape[1],))\n            raise RuntimeError(\"Layer not initialized. Call initialize(optimizer) after setting input_shape.\")\n        self._X = X\n        # (B, D) @ (D, U) -> (B, U); bias (1, U) broadcasts over batch\n        out = X.matmul(self.W) + self.w0\n        return out\n\n    def backward_pass(self, accum_grad):\n        # accum_grad: dL/dY with shape (B, U)\n        if not isinstance(accum_grad, torch.Tensor):\n            accum_grad = torch.as_tensor(accum_grad, dtype=torch.float32)\n        if self._X is None:\n            raise RuntimeError(\"No cached input. Call forward_pass before backward_pass.\")\n        X = self._X\n        # Gradients w.r.t parameters\n        # dL/dW = X^T @ dL/dY  -> (D, B) @ (B, U) = (D, U)\n        grad_W = X.t().matmul(accum_grad)\n        # dL/db = sum over batch of dL/dY -> (1, U)\n        grad_b = accum_grad.sum(dim=0, keepdim=True)\n        # Gradient w.r.t input: dL/dX = dL/dY @ W^T -> (B, U) @ (U, D) = (B, D)\n        grad_input = accum_grad.matmul(self.W.t())\n        # Parameter updates\n        if self.trainable:\n            if self.opt_W is None or self.opt_b is None:\n                raise RuntimeError(\"Optimizer not initialized. Call initialize(optimizer) first.\")\n            # Optimizer expected to implement: update(weights, grad) -> new_weights\n            self.W = self.opt_W.update(self.W, grad_W)\n            self.w0 = self.opt_b.update(self.w0, grad_b)\n        return grad_input\n\n    def output_shape(self):\n        return (self.n_units,)\n\n# Example usage\nif __name__ == \"__main__\":\n    class MockOptimizer:\n        def __init__(self, lr=0.01):\n            self.lr = lr\n        def update(self, weights, grad):\n            # Simple SGD step (no momentum/state)\n            return weights - self.lr * grad\n\n    dense_layer = Dense(n_units=3, input_shape=(2,))\n    optimizer = MockOptimizer(lr=0.01)\n    dense_layer.initialize(optimizer)\n\n    X = torch.tensor([[1.0, 2.0]], dtype=torch.float32)  # Shape: (1, 2)\n    output = dense_layer.forward_pass(X)\n    print(\"Forward pass output:\", output)\n\n    accum_grad = torch.tensor([[0.1, 0.2, 0.3]], dtype=torch.float32)  # Shape: (1, 3)\n    back_output = dense_layer.backward_pass(accum_grad)\n    print(\"Backward pass output:\", back_output)\n\n    print(\"Number of parameters:\", dense_layer.parameters())\n    print(\"Output shape:\", dense_layer.output_shape())\n",
  "timeComplexity": "Forward and backward: O(B * D * U)",
  "spaceComplexity": "O(D * U + U) for parameters, plus O(B * D) to cache the input during backprop",
  "platform": "deepml"
};
