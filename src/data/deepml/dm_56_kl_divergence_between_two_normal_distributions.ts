import { Problem } from '../../types';

export const DM_56_KL_DIVERGENCE_BETWEEN_TWO_NORMAL_DISTRIBUTIONS: Problem = {
  "id": "dm_56_kl_divergence_between_two_normal_distributions",
  "title": "KL Divergence Between Two Normal Distributions",
  "difficulty": "Easy",
  "tags": [
    "Probability",
    "Loss Functions"
  ],
  "descriptionMarkdown": "Task: Implement KL Divergence Between Two Normal Distributions\n\nYour task is to compute the Kullback\u2013Leibler (KL) divergence between two normal distributions. KL divergence measures how one probability distribution differs from a second, reference probability distribution.\n\nWrite a function `kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)` that calculates the KL divergence between two normal distributions. The function should return the KL divergence as a floating point number.\n\nExample:\n\nInput:\n- mu_p = 0.0\n- sigma_p = 1.0\n- mu_q = 1.0\n- sigma_q = 1.0\n\nExpected Output:\n- 0.5\n\nReasoning:\nThe KL divergence between the normal distributions P and Q with parameters (\u03bc_P = 0.0, \u03c3_P = 1.0) and (\u03bc_Q = 1.0, \u03c3_Q = 1.0) is 0.5.",
  "solutionExplanation": "For univariate normal distributions P = N(\u03bc_P, \u03c3_P^2) and Q = N(\u03bc_Q, \u03c3_Q^2), the KL divergence KL(P || Q) has the closed-form expression:\n\nKL(P || Q) = log(\u03c3_Q / \u03c3_P) + [\u03c3_P^2 + (\u03bc_P \u2212 \u03bc_Q)^2] / (2\u03c3_Q^2) \u2212 1/2\n\nThis formula follows from integrating the log-density ratio of two Gaussians and simplifies to the above expression. To implement this robustly, we ensure the standard deviations are strictly positive by clamping with a small epsilon to avoid division by zero and log of non-positive values.\n\nIn PyTorch, we convert inputs to tensors and use elementwise operations. The function supports both scalar inputs and tensor inputs (broadcasted); if all inputs are scalars, it returns a Python float to match the problem requirement. The implementation is numerically stable and concise.",
  "solutionCode": "import torch\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"Compute KL(P || Q) for univariate normal distributions.\n    \n    Args:\n        mu_p (float or Tensor): Mean of distribution P.\n        sigma_p (float or Tensor): Standard deviation of distribution P (must be > 0).\n        mu_q (float or Tensor): Mean of distribution Q.\n        sigma_q (float or Tensor): Standard deviation of distribution Q (must be > 0).\n    \n    Returns:\n        float or Tensor: KL divergence. Returns a Python float if inputs are scalars; otherwise a Tensor.\n    \"\"\"\n    # Convert inputs to tensors using the current default dtype for consistency\n    dtype = torch.get_default_dtype()\n    mu_p = torch.as_tensor(mu_p, dtype=dtype)\n    sigma_p = torch.as_tensor(sigma_p, dtype=dtype)\n    mu_q = torch.as_tensor(mu_q, dtype=dtype)\n    sigma_q = torch.as_tensor(sigma_q, dtype=dtype)\n\n    # Numerical stability: clamp standard deviations to be strictly positive\n    eps = torch.finfo(dtype).eps\n    sigma_p = torch.clamp(sigma_p, min=eps)\n    sigma_q = torch.clamp(sigma_q, min=eps)\n\n    # KL divergence formula for univariate Gaussians\n    term1 = torch.log(sigma_q / sigma_p)\n    term2 = (sigma_p.pow(2) + (mu_p - mu_q).pow(2)) / (2.0 * sigma_q.pow(2))\n    kl = term1 + term2 - 0.5\n\n    # If scalar, return Python float to match the problem specification\n    if kl.numel() == 1:\n        return float(kl.item())\n    return kl\n\n\ndef solution():\n    # Example usage\n    mu_p = 0.0\n    sigma_p = 1.0\n    mu_q = 1.0\n    sigma_q = 1.0\n    result = kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)\n    print(result)  # Expected: 0.5\n",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(1)",
  "platform": "deepml"
};
