import { Problem } from '../../types';

export const DM_136_CALCULATE_KL_DIVERGENCE_BETWEEN_TWO_MULTIVARIATE_GAUSSIAN_DISTRIBUTIONS: Problem = {
  "id": "dm_136_calculate_kl_divergence_between_two_multivariate_gaussian_distributions",
  "title": "Calculate KL Divergence Between Two Multivariate Gaussian Distributions",
  "difficulty": "Medium",
  "tags": [
    "Probability",
    "Loss Functions",
    "Linear Algebra",
    "Matrix Operations"
  ],
  "descriptionMarkdown": "KL divergence measures the dissimilarity between two probability distributions. In this problem, implement a function to compute the KL divergence between two multivariate Gaussian distributions given their mean vectors and covariance matrices.\n\nGiven two Gaussians p = N(\u03bc_p, \u03a3_p) and q = N(\u03bc_q, \u03a3_q) in d dimensions, the KL divergence KL(p || q) is computed as:\n\nKL(p || q) = 0.5 * [ log(det(\u03a3_q) / det(\u03a3_p)) - d + tr(\u03a3_q^{-1} \u03a3_p) + (\u03bc_q - \u03bc_p)^T \u03a3_q^{-1} (\u03bc_q - \u03bc_p) ]\n\nImplement this computation using numerically stable linear algebra (e.g., Cholesky factorization or slogdet) and avoid forming explicit matrix inverses.\n\nInput:\n- \u03bc_p (mean vector of the first distribution)\n- \u03a3_p (covariance matrix of the first distribution)\n- \u03bc_q (mean vector of the second distribution)\n- \u03a3_q (covariance matrix of the second distribution)\n\nOutput:\n- A single float (or scalar tensor) representing the KL divergence",
  "solutionExplanation": "For multivariate Gaussians, the KL divergence has a closed-form expression that depends on the mean difference, the trace term between covariances, and the ratio of determinants. Directly inverting covariance matrices or computing determinants can be numerically unstable, especially for nearly singular matrices. Therefore, we rely on Cholesky factorization for stable solves and log-determinant computation.\n\nWe compute log determinants via the Cholesky factor L of a positive-definite covariance \u03a3, where log det(\u03a3) = 2 * sum(log(diag(L))). The trace term tr(\u03a3_q^{-1} \u03a3_p) and the Mahalanobis term (\u03bc_q - \u03bc_p)^T \u03a3_q^{-1} (\u03bc_q - \u03bc_p) are both computed by solving linear systems using the Cholesky factor of \u03a3_q instead of explicitly forming the inverse. To further improve robustness, we add a small jitter (scaled identity) to the diagonals if the Cholesky fails, increasing it iteratively until a factorization succeeds.\n\nThis approach is efficient, differentiable in PyTorch, and avoids explicit inversion, ensuring both numerical stability and compatibility with gradient-based workflows.",
  "solutionCode": "import torch\n\ndef multivariate_kl_divergence(mu_p: torch.Tensor,\n                                Cov_p: torch.Tensor,\n                                mu_q: torch.Tensor,\n                                Cov_q: torch.Tensor,\n                                jitter: float = 1e-6,\n                                max_tries: int = 5) -> torch.Tensor:\n    \"\"\"\n    Compute KL(p || q) for two multivariate Gaussians p ~ N(mu_p, Cov_p), q ~ N(mu_q, Cov_q).\n\n    KL(p || q) = 0.5 * [ log(det(Cov_q)/det(Cov_p)) - d\n                          + tr(Cov_q^{-1} Cov_p)\n                          + (mu_q - mu_p)^T Cov_q^{-1} (mu_q - mu_p) ]\n\n    Parameters\n    ----------\n    mu_p : (d,) torch.Tensor\n        Mean of the first distribution p.\n    Cov_p : (d, d) torch.Tensor\n        Covariance of the first distribution p. Must be symmetric positive-definite.\n    mu_q : (d,) torch.Tensor\n        Mean of the second distribution q.\n    Cov_q : (d, d) torch.Tensor\n        Covariance of the second distribution q. Must be symmetric positive-definite.\n    jitter : float\n        Initial diagonal jitter added if Cholesky fails (stability for near-singular matrices).\n    max_tries : int\n        Maximum number of jitter escalations for a stable Cholesky factorization.\n\n    Returns\n    -------\n    torch.Tensor (scalar)\n        The KL divergence KL(p || q).\n    \"\"\"\n    # Basic shape checks\n    if mu_p.ndim != 1 or mu_q.ndim != 1:\n        raise ValueError(\"mu_p and mu_q must be 1D tensors (shape: (d,)).\")\n    if Cov_p.ndim != 2 or Cov_q.ndim != 2:\n        raise ValueError(\"Cov_p and Cov_q must be 2D tensors (shape: (d, d)).\")\n    d = mu_p.shape[0]\n    if mu_q.shape[0] != d or Cov_p.shape != (d, d) or Cov_q.shape != (d, d):\n        raise ValueError(\"Shapes must be: mu_p (d,), mu_q (d,), Cov_p (d,d), Cov_q (d,d).\")\n\n    dtype = Cov_q.dtype\n    device = Cov_q.device\n\n    I = torch.eye(d, dtype=dtype, device=device)\n\n    def stable_cholesky(A: torch.Tensor) -> torch.Tensor:\n        \"\"\"Compute a robust Cholesky factorization with escalating jitter if needed.\"\"\"\n        jit = jitter\n        last_err = None\n        for _ in range(max_tries):\n            try:\n                # Ensure symmetry numerically\n                A_sym = 0.5 * (A + A.transpose(-1, -2))\n                L = torch.linalg.cholesky(A_sym + jit * I)\n                return L\n            except RuntimeError as e:\n                last_err = e\n                jit *= 10.0\n        raise RuntimeError(f\"Cholesky factorization failed after {max_tries} attempts: {last_err}\")\n\n    # Cholesky factors for both covariances (for stable log-determinants)\n    L_q = stable_cholesky(Cov_q)\n    L_p = stable_cholesky(Cov_p)\n\n    # log det via Cholesky: logdet(\u03a3) = 2 * sum(log(diag(L)))\n    logdet_q = 2.0 * torch.log(torch.diagonal(L_q)).sum()\n    logdet_p = 2.0 * torch.log(torch.diagonal(L_p)).sum()\n\n    # Trace term: tr(\u03a3_q^{-1} \u03a3_p) computed via cholesky_solve\n    # cholesky_solve solves (L L^T) X = B => X = \u03a3_q^{-1} B\n    invq_Covp = torch.cholesky_solve(Cov_p, L_q)\n    trace_term = torch.trace(invq_Covp)\n\n    # Mahalanobis term: (mu_q - mu_p)^T \u03a3_q^{-1} (mu_q - mu_p)\n    diff = (mu_q - mu_p).reshape(d, 1)\n    invq_diff = torch.cholesky_solve(diff, L_q)\n    quad_term = (diff * invq_diff).sum()\n\n    kl = 0.5 * (logdet_q - logdet_p - d + trace_term + quad_term)\n    return kl\n\n\nif __name__ == \"__main__\":\n    # Example usage\n    torch.manual_seed(0)\n    d = 4\n    dtype = torch.double\n    device = \"cpu\"\n\n    # Construct SPD covariances\n    A = torch.randn(d, d, dtype=dtype, device=device)\n    Cov_p = A @ A.T + 1e-3 * torch.eye(d, dtype=dtype, device=device)\n\n    B = torch.randn(d, d, dtype=dtype, device=device)\n    Cov_q = B @ B.T + 1e-3 * torch.eye(d, dtype=dtype, device=device)\n\n    mu_p = torch.randn(d, dtype=dtype, device=device)\n    mu_q = torch.randn(d, dtype=dtype, device=device)\n\n    kl_val = multivariate_kl_divergence(mu_p, Cov_p, mu_q, Cov_q)\n    print(f\"KL(p || q) = {kl_val.item():.6f}\")\n",
  "timeComplexity": "O(d^3)",
  "spaceComplexity": "O(d^2)",
  "platform": "deepml"
};
