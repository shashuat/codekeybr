import { Problem } from '../../types';

export const DM_20_DECISION_TREE_LEARNING: Problem = {
  "id": "dm_20_decision_tree_learning",
  "title": "Decision Tree Learning",
  "difficulty": "Hard",
  "tags": [
    "Probability"
  ],
  "descriptionMarkdown": "Write a Python function that implements the decision tree learning algorithm for classification. The function should use recursive splitting based on entropy and information gain to build a decision tree. It should take a list of examples (each example is a dict of attribute-value pairs) and a list of attribute names as input, and return a nested dictionary representing the decision tree.\n\nExample:\n\nInput:\n\nexamples = [\n{'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'No'},\n{'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Strong', 'PlayTennis': 'No'},\n{'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n{'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'}\n]\n\nattributes = ['Outlook', 'Temperature', 'Humidity', 'Wind']\n\ntarget_attr = 'PlayTennis'\n\nOutput (one valid tree structure):\n\n{\n  'Outlook': {\n    'Sunny': {'Humidity': {'High': 'No', 'Normal': 'Yes'}},\n    'Overcast': 'Yes',\n    'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}\n  }\n}\n\nReasoning: Using the given examples, the decision tree algorithm determines that 'Outlook' is the best attribute to split the data initially. When 'Outlook' is 'Overcast', the outcome is always 'Yes', so it becomes a leaf node. In cases of 'Sunny' and 'Rain', it further splits based on 'Humidity' and 'Wind', respectively. The resulting tree structure can classify the training examples with the attributes 'Outlook', 'Temperature', 'Humidity', and 'Wind'.",
  "solutionExplanation": "This solution implements the classic ID3-style decision tree learner that uses entropy and information gain as its splitting criterion. For any set of labels Y, the entropy H(Y) = -\u2211 p(y) log2 p(y) measures impurity. When we split on a candidate attribute X, the expected impurity after splitting is the weighted sum of entropies across its values. The information gain is IG(Y, X) = H(Y) - \u2211v p(X=v) H(Y | X=v). We select the attribute with the highest information gain and recursively build subtrees on the corresponding partitions.\n\nThe recursion stops when all remaining examples in a node share the same target label, when there are no attributes left to split on, or when there are no examples (in which case we back off to the majority label from the parent). Although the dataset here is categorical and leads to multiway splits, the same entropy/information gain computation applies. PyTorch is used to compute entropies and majority classes efficiently via tensor operations (bincount and log2), while the dataset partitioning remains in native Python due to its dictionary-based format.\n\nThis yields a nested dictionary where each internal node is a mapping from an attribute to value-specific subtrees, and each leaf is a class label. The algorithm is deterministic given tie-breaking rules for equal gains (e.g., order of attributes).",
  "solutionCode": "import torch\nfrom typing import List, Dict, Any\n\n\ndef _entropy_from_labels(labels: List[Any]) -> float:\n    \"\"\"Compute entropy H(Y) using PyTorch.\n    labels: list of hashable class labels (e.g., strings).\n    Returns a Python float.\n    \"\"\"\n    if not labels:\n        return 0.0\n    # Map labels to integer indices\n    uniq = sorted(set(labels))\n    idx_map = {lab: i for i, lab in enumerate(uniq)}\n    idxs = torch.tensor([idx_map[l] for l in labels], dtype=torch.long)\n    counts = torch.bincount(idxs, minlength=len(uniq)).to(torch.float32)\n    total = counts.sum()\n    if total.item() == 0:\n        return 0.0\n    probs = counts / total\n    probs = probs[probs > 0]\n    return float((-(probs * torch.log2(probs)).sum()).item())\n\n\ndef _majority_label(labels: List[Any]) -> Any:\n    \"\"\"Return the majority label using PyTorch ops for counting.\"\"\"\n    if not labels:\n        return None\n    uniq = sorted(set(labels))\n    idx_map = {lab: i for i, lab in enumerate(uniq)}\n    idxs = torch.tensor([idx_map[l] for l in labels], dtype=torch.long)\n    counts = torch.bincount(idxs, minlength=len(uniq)).to(torch.float32)\n    max_idx = int(torch.argmax(counts).item())\n    return uniq[max_idx]\n\n\ndef _information_gain(examples: List[Dict[str, Any]], attr: str, target_attr: str) -> float:\n    \"\"\"Compute information gain IG(Y, attr).\"\"\"\n    labels = [ex[target_attr] for ex in examples]\n    base_entropy = _entropy_from_labels(labels)\n    if base_entropy == 0.0:\n        return 0.0\n    # Partition labels by attribute value\n    groups: Dict[Any, List[Any]] = {}\n    for ex in examples:\n        v = ex.get(attr)\n        groups.setdefault(v, []).append(ex[target_attr])\n    n = len(examples)\n    weighted_entropy = 0.0\n    for v, lbls in groups.items():\n        weight = len(lbls) / n\n        weighted_entropy += weight * _entropy_from_labels(lbls)\n    return base_entropy - weighted_entropy\n\n\ndef _all_same_label(examples: List[Dict[str, Any]], target_attr: str) -> Any:\n    if not examples:\n        return None\n    first = examples[0][target_attr]\n    for ex in examples[1:]:\n        if ex[target_attr] != first:\n            return None\n    return first\n\n\ndef learn_decision_tree(examples: List[Dict[str, Any]],\n                        attributes: List[str],\n                        target_attr: str,\n                        default_label: Any = None) -> Any:\n    \"\"\"Learn a decision tree using entropy/information gain (ID3-style).\n\n    Args:\n        examples: List of examples, each a dict of attribute -> value, including target_attr.\n        attributes: List of attribute names (categorical) available for splitting (exclude target).\n        target_attr: Name of the target attribute (class label key).\n        default_label: Fallback label used when examples is empty (typically parent's majority).\n\n    Returns:\n        A nested dictionary representing the decision tree; leaves are class labels.\n    \"\"\"\n    # Base case: no examples -> return default\n    if not examples:\n        return default_label\n\n    # Base case: all examples have the same label -> return that label\n    unanimous = _all_same_label(examples, target_attr)\n    if unanimous is not None:\n        return unanimous\n\n    # Base case: no attributes left -> return majority label\n    if not attributes:\n        majority = _majority_label([ex[target_attr] for ex in examples])\n        return majority\n\n    # Select attribute with highest information gain\n    gains = [(attr, _information_gain(examples, attr, target_attr)) for attr in attributes]\n    # Break ties by order in attributes (stable max)\n    best_attr, best_gain = max(gains, key=lambda x: x[1])\n\n    # If no information gain, return majority label\n    if best_gain <= 0.0:\n        majority = _majority_label([ex[target_attr] for ex in examples])\n        return majority\n\n    # Build the subtree for each value of best_attr\n    tree: Dict[Any, Any] = {}\n    majority_here = _majority_label([ex[target_attr] for ex in examples])\n    values = sorted(set(ex[best_attr] for ex in examples))\n\n    remaining_attrs = [a for a in attributes if a != best_attr]\n    for v in values:\n        subset = [ex for ex in examples if ex.get(best_attr) == v]\n        subtree = learn_decision_tree(\n            examples=subset,\n            attributes=remaining_attrs,\n            target_attr=target_attr,\n            default_label=majority_here,\n        )\n        tree[v] = subtree\n\n    return {best_attr: tree}\n\n\ndef solution():\n    # Example usage\n    examples = [\n        {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'No'},\n        {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Strong', 'PlayTennis': 'No'},\n        {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n        {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'}\n    ]\n    attributes = ['Outlook', 'Temperature', 'Humidity', 'Wind']\n    target_attr = 'PlayTennis'\n\n    tree = learn_decision_tree(examples, attributes, target_attr)\n    return tree\n",
  "timeComplexity": "O(N * A * V) where N is the number of examples, A the number of attributes, and V the average number of distinct values per attribute at a node",
  "spaceComplexity": "O(N + A) for storing data partitions during recursion and the resulting tree",
  "platform": "deepml"
};
