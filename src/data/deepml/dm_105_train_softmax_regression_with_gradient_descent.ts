import { Problem } from '../../types';

export const DM_105_TRAIN_SOFTMAX_REGRESSION_WITH_GRADIENT_DESCENT: Problem = {
  "id": "dm_105_train_softmax_regression_with_gradient_descent",
  "title": "Train Softmax Regression with Gradient Descent",
  "difficulty": "Hard",
  "tags": [
    "Optimization",
    "Loss Functions",
    "Activation Functions",
    "Gradient Descent",
    "Matrix Operations",
    "Linear Algebra"
  ],
  "descriptionMarkdown": "Implement a gradient descent-based training algorithm for multi-class Softmax (multinomial logistic) regression. Optimize the model parameters using the Cross Entropy loss and return both the learned coefficient matrix and the list of loss values collected over iterations. Round all returned numbers to 4 decimal places.\n\n- Input:\n  - X: 2D array of shape (n_samples, n_features)\n  - y: 1D array of integer class labels in [0, n_classes-1]\n  - learning_rate: float\n  - iterations: int\n\n- Output:\n  - A tuple: (coefficients, losses)\n    - coefficients: list of lists representing the weight matrix of shape ((n_features + 1) x n_classes). The last row corresponds to the bias parameters (via feature augmentation with ones).\n    - losses: list of scalar loss values per iteration.\n\nExample:\n\nInput:\ntrain_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10)\n\nOutput:\n([[-0.0011, 0.0145, -0.0921], [0.002, -0.0598, 0.1263], [-0.0009, 0.0453, -0.0342]], [3.2958, 3.2611, 3.2272, 3.1941, 3.1618, 3.1302, 3.0993, 3.0692, 3.0398, 3.011])\n",
  "solutionExplanation": "Softmax regression models the conditional class probabilities as softmax(XW), where W includes both feature weights and bias (handled by augmenting the input with a column of ones). Training is performed by minimizing the average Cross Entropy loss between the predicted class distribution and the true labels. Cross Entropy for logits z and labels y is computed stably via log-softmax; PyTorch's CrossEntropyLoss combines LogSoftmax and NLLLoss for numerical stability.\n\nWe perform batch gradient descent: on each iteration, we compute logits = X_aug @ W, evaluate the loss, backpropagate to obtain gradients with respect to W, and apply a parameter update W <- W - lr * grad. We collect the loss value after each iteration. Finally, we return the learned coefficient matrix (with the bias row included) and the loss history, both rounded to 4 decimal places.\n",
  "solutionCode": "import torch\nimport torch.nn as nn\nimport numpy as np\n\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int):\n    \"\"\"\n    Train a Softmax (multinomial logistic) regression model using batch gradient descent.\n\n    Parameters:\n        X (np.ndarray): Input features of shape (N, D).\n        y (np.ndarray): Integer class labels of shape (N,), values in [0, C-1].\n        learning_rate (float): Learning rate for gradient descent.\n        iterations (int): Number of gradient descent iterations.\n\n    Returns:\n        tuple: (coefficients, losses)\n            - coefficients: list[list[float]] of shape ((D+1) x C). The last row is bias.\n            - losses: list[float] of length `iterations`, Cross Entropy per iteration.\n        All values are rounded to 4 decimal places.\n    \"\"\"\n    # Convert inputs to torch tensors\n    X_t = torch.as_tensor(X, dtype=torch.float32)\n    y_t = torch.as_tensor(y, dtype=torch.long)\n\n    N, D = X_t.shape\n    num_classes = int(y_t.max().item() + 1) if y_t.numel() > 0 else 0\n\n    # Augment features with a bias column of ones\n    ones = torch.ones((N, 1), dtype=torch.float32)\n    X_aug = torch.cat([X_t, ones], dim=1)  # (N, D+1)\n\n    # Initialize weights (D+1, C)\n    W = torch.zeros((D + 1, num_classes), dtype=torch.float32, requires_grad=True)\n\n    loss_fn = nn.CrossEntropyLoss(reduction='mean')\n    losses = []\n\n    for _ in range(iterations):\n        # Forward pass: logits shape (N, C)\n        logits = X_aug @ W\n        loss = loss_fn(logits, y_t)\n\n        # Record loss (unrounded; we'll round at the end)\n        losses.append(loss.item())\n\n        # Backprop and manual gradient descent step\n        loss.backward()\n        with torch.no_grad():\n            W -= learning_rate * W.grad\n            W.grad.zero_()\n\n    # Prepare outputs: round to 4 decimals\n    W_np = W.detach().cpu().numpy().tolist()\n    W_rounded = [[round(float(v), 4) for v in row] for row in W_np]\n    losses_rounded = [round(float(l), 4) for l in losses]\n\n    return W_rounded, losses_rounded\n\n\ndef solution():\n    # Example usage\n    X = np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]], dtype=np.float32)\n    y = np.array([0, 1, 2], dtype=np.int64)\n    coeffs, loss_hist = train_softmaxreg(X, y, learning_rate=0.01, iterations=10)\n    print(coeffs)\n    print(loss_hist)\n",
  "timeComplexity": "O(T * N * D * C) where T is iterations, N is samples, D is features, C is classes",
  "spaceComplexity": "O(D * C) for parameters plus O(N * (D + 1)) for the augmented batch",
  "platform": "deepml"
};
