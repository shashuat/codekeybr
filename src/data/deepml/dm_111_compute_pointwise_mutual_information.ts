import { Problem } from '../../types';

export const DM_111_COMPUTE_POINTWISE_MUTUAL_INFORMATION: Problem = {
  "id": "dm_111_compute_pointwise_mutual_information",
  "title": "Compute Pointwise Mutual Information",
  "difficulty": "Medium",
  "tags": [
    "Probability"
  ],
  "descriptionMarkdown": "Implement a function to compute the Pointwise Mutual Information (PMI) given the joint occurrence count of two events, their individual counts, and the total number of samples. PMI measures how much the actual joint occurrence of events differs from what we would expect by chance.\n\nExample:\n- Input: `compute_pmi(50, 200, 300, 1000)`\n- Output: `-0.263`\n\nReasoning: The PMI compares the actual joint probability (50/1000 = 0.05) to the product of the individual probabilities (200/1000 \u00d7 300/1000 = 0.06). Thus, PMI = log\u2082(0.05 / (0.2 \u00d7 0.3)) \u2248 -0.263, indicating the events co-occur slightly less than expected by chance.",
  "solutionExplanation": "Pointwise Mutual Information (PMI) between two events X and Y is defined as PMI(X, Y) = log2(p(X, Y) / (p(X) p(Y))). Given counts, we estimate probabilities with p(X, Y) = joint_counts / total_samples, p(X) = total_counts_x / total_samples, and p(Y) = total_counts_y / total_samples. We then compute the base-2 logarithm of the ratio between the observed joint probability and the expected joint probability under independence.\n\nEdge cases arise when probabilities are zero. If p(X, Y) = 0 and the expected probability is positive, PMI is -inf. If the expected probability is zero but p(X, Y) > 0, PMI tends to +inf. If both are zero, PMI is undefined; we return -inf by convention. The implementation uses PyTorch tensors for numerical stability, broadcasting, and vectorization, and returns either a scalar float or a tensor depending on inputs.",
  "solutionCode": "import torch\nimport torch.nn as nn\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute Pointwise Mutual Information (PMI) in bits (log base 2).\n\n    Args:\n        joint_counts: Joint count of events X and Y (scalar or tensor).\n        total_counts_x: Count of event X (scalar or tensor).\n        total_counts_y: Count of event Y (scalar or tensor).\n        total_samples: Total number of samples (scalar or tensor).\n\n    Returns:\n        PMI as a torch.Tensor (or Python float if all inputs are scalars).\n    \"\"\"\n    # Convert inputs to tensors with a common high-precision dtype\n    jc = torch.as_tensor(joint_counts, dtype=torch.float64)\n    cx = torch.as_tensor(total_counts_x, dtype=torch.float64)\n    cy = torch.as_tensor(total_counts_y, dtype=torch.float64)\n    n = torch.as_tensor(total_samples, dtype=torch.float64)\n\n    if torch.any(n <= 0):\n        raise ValueError(\"total_samples must be > 0.\")\n\n    # Empirical probabilities\n    p_xy = jc / n\n    p_x = cx / n\n    p_y = cy / n\n    expected = p_x * p_y\n\n    # Prepare output tensor\n    pmi = torch.empty_like(p_xy, dtype=torch.float64)\n\n    zero_joint = p_xy == 0\n    zero_expected = expected == 0\n\n    both_zero = zero_joint & zero_expected\n    pos_joint_pos_exp = (~zero_joint) & (~zero_expected)\n    pos_joint_zero_exp = (~zero_joint) & zero_expected\n    zero_joint_pos_exp = zero_joint & (~zero_expected)\n\n    # Case 1: valid positive probabilities\n    pmi[pos_joint_pos_exp] = torch.log2(p_xy[pos_joint_pos_exp] / expected[pos_joint_pos_exp])\n    # Case 2: p(x,y) > 0, expected == 0 -> +inf\n    pmi[pos_joint_zero_exp] = float(\"inf\")\n    # Case 3: p(x,y) == 0, expected > 0 -> -inf\n    pmi[zero_joint_pos_exp] = float(\"-inf\")\n    # Case 4: both zero -> define as -inf (undefined mathematically)\n    pmi[both_zero] = float(\"-inf\")\n\n    # If scalar, return a Python float for convenience\n    if pmi.ndim == 0:\n        return pmi.item()\n    return pmi\n\n\ndef solution():\n    # Example usage\n    val = compute_pmi(50, 200, 300, 1000)\n    # Expected approximately -0.263\n    print(f\"PMI: {val:.3f}\")\n    return val\n\n\nif __name__ == \"__main__\":\n    solution()\n",
  "timeComplexity": "O(1)",
  "spaceComplexity": "O(1)",
  "platform": "deepml"
};
