import { Problem } from '../../types';

export const DM_109_IMPLEMENT_LAYER_NORMALIZATION_FOR_SEQUENCE_DATA: Problem = {
  "id": "dm_109_implement_layer_normalization_for_sequence_data",
  "title": "Implement Layer Normalization for Sequence Data",
  "difficulty": "Medium",
  "tags": [
    "Neural Networks",
    "Transformers",
    "Matrix Operations",
    "Linear Algebra"
  ],
  "descriptionMarkdown": "Implement a function to perform Layer Normalization on an input tensor. Given a 3D tensor representing (batch_size, sequence_length, feature_dim), normalize the data across the feature dimension for each sequence element, then apply scaling (gamma) and shifting (beta) parameters.\n\nExample:\n\nInput:\n\n```\nnp.random.seed(42)\nX = np.random.randn(2, 2, 3)\ngamma = np.ones(3).reshape(1, 1, -1)\nbeta = np.zeros(3).reshape(1, 1, -1)\nlayer_normalization(X, gamma, beta)\n```\n\nOutput:\n\n```\n[[[ 0.47373971 -1.39079736  0.91705765]\n  [ 1.41420326 -0.70711154 -0.70709172]]\n [[ 1.13192477  0.16823009 -1.30015486]\n  [ 1.4141794  -0.70465482 -0.70952458]]]\n```\n\nThe function computes the mean and variance across the feature dimension (d_model = 3) for each sequence element, normalizes the input, then applies gamma = 1 and beta = 0.",
  "solutionExplanation": "Layer Normalization normalizes each sample independently across its feature dimension. For a 3D input X with shape (B, T, D), we compute the mean and variance along the last dimension D for each (b, t) element. This produces mean and var of shape (B, T, 1). The normalized tensor is then (X \u2212 mean) / sqrt(var + eps).\n\nFinally, we apply learnable (or provided) affine parameters gamma and beta, which are broadcastable to shape (B, T, D): output = normalized * gamma + beta. Using unbiased=False when computing the variance matches NumPy's default ddof=0, ensuring consistency with the reference output. An epsilon is added for numerical stability to avoid division by zero.\n\nThis operation is differentiable and is commonly used in Transformers and other sequence models. It stabilizes training by keeping hidden activations well-conditioned, independent of batch size, since normalization is performed per sample and per time step across features.",
  "solutionCode": "import torch\nimport torch.nn as nn\nfrom typing import Tuple\n\n\ndef layer_normalization(x: torch.Tensor,\n                        gamma: torch.Tensor,\n                        beta: torch.Tensor,\n                        epsilon: float = 1e-5) -> torch.Tensor:\n    \"\"\"\n    Perform Layer Normalization over the last dimension (features) of a 3D tensor.\n\n    Args:\n        x: Input tensor of shape (batch_size, seq_len, feature_dim).\n        gamma: Scale tensor broadcastable to x's shape, typically (1, 1, feature_dim).\n        beta: Shift tensor broadcastable to x's shape, typically (1, 1, feature_dim).\n        epsilon: Small constant for numerical stability.\n\n    Returns:\n        A tensor of the same shape as x with layer normalization applied.\n    \"\"\"\n    # Compute mean and variance across the feature dimension\n    mean = x.mean(dim=-1, keepdim=True)\n    var = x.var(dim=-1, unbiased=False, keepdim=True)  # population variance to match NumPy's default\n\n    # Normalize\n    x_hat = (x - mean) / torch.sqrt(var + epsilon)\n\n    # Affine transform\n    return x_hat * gamma + beta\n\n\ndef solution() -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Example usage of layer_normalization with a random input.\n\n    Returns:\n        A tuple of (output, stats), where output is the normalized tensor and\n        stats contains per-token mean and variance to verify normalization.\n    \"\"\"\n    torch.manual_seed(42)\n\n    # Example shapes: (batch=2, seq=2, features=3)\n    x = torch.randn(2, 2, 3, dtype=torch.float32)\n    gamma = torch.ones(1, 1, 3, dtype=torch.float32)\n    beta = torch.zeros(1, 1, 3, dtype=torch.float32)\n\n    out = layer_normalization(x, gamma, beta, epsilon=1e-5)\n\n    # Verify that for gamma=1 and beta=0, each (b, t) slice has ~zero mean and unit variance\n    per_token_mean = out.mean(dim=-1)\n    per_token_var = out.var(dim=-1, unbiased=False)\n\n    # Print for demonstration (can be removed in production)\n    print(\"Input:\\n\", x)\n    print(\"\\nLayerNorm Output:\\n\", out)\n    print(\"\\nPer-token mean (should be ~0):\\n\", per_token_mean)\n    print(\"\\nPer-token variance (should be ~1):\\n\", per_token_var)\n\n    return out, torch.stack([per_token_mean, per_token_var])\n",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(1)",
  "platform": "deepml"
};
