import { Problem } from '../../types';

export const DM_123_CALCULATE_COMPUTATIONAL_EFFICIENCY_OF_MOE: Problem = {
  "id": "dm_123_calculate_computational_efficiency_of_moe",
  "title": "Calculate Computational Efficiency of MoE",
  "difficulty": "Easy",
  "tags": [
    "Neural Networks",
    "Matrix Operations",
    "Linear Algebra"
  ],
  "descriptionMarkdown": "Calculate the computational cost savings of a Mixture-of-Experts (MoE) layer compared to a dense layer, as discussed in the paper \"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.\" Given the number of experts, sparsity (number of active experts), and input/output dimensions, compute the floating-point operations (FLOPs) for both and determine the savings percentage.\n\nExample:\n- Input: `compute_efficiency(1000, 2, 512, 512)`\n- Output: `99.8`\n\nReasoning:\n- Dense layer FLOPs: `1000 * 512 * 512 = 262,144,000`\n- MoE FLOPs: `2 * 512 * 512 = 524,288`\n- Savings: `((262,144,000 - 524,288) / 262,144,000) * 100 \u2248 99.8%`",
  "solutionExplanation": "In an MoE layer with E experts, only k experts are activated per input (top-k gating). Each expert is typically implemented as a feed-forward (dense) layer. The key computational advantage comes from sparsity: rather than computing all experts, only k are evaluated per input.\n\nIf a dense baseline were to compute all E experts, its cost (ignoring biases and non-linearities) is proportional to E \u00d7 d_in \u00d7 d_out FLOPs for a single input vector. The MoE layer, activating only k experts, reduces the cost to k \u00d7 d_in \u00d7 d_out. Therefore, the percentage savings is computed as ((E \u00d7 d_in \u00d7 d_out \u2212 k \u00d7 d_in \u00d7 d_out) / (E \u00d7 d_in \u00d7 d_out)) \u00d7 100, which simplifies to (1 \u2212 k/E) \u00d7 100.\n\nThis formulation follows the sparsely-gated MoE idea where the router selects a small subset of experts. Router overhead and auxiliary losses are typically negligible compared to matrix multiplications and are ignored here, matching the example provided.",
  "solutionCode": "import torch\nimport torch.nn as nn\n\ndef compute_efficiency(n_experts: int, k_active: int, d_in: int, d_out: int) -> float:\n    \"\"\"Compute percentage FLOP savings of an MoE layer vs. a dense-all-experts baseline.\n\n    Args:\n        n_experts: Total number of experts (E > 0).\n        k_active: Number of active experts per input (0 < k <= E).\n        d_in: Input feature dimension (d_in > 0).\n        d_out: Output feature dimension (d_out > 0).\n\n    Returns:\n        Percentage savings in FLOPs as a float.\n    \"\"\"\n    # Basic validation\n    for name, val in [(\"n_experts\", n_experts), (\"k_active\", k_active), (\"d_in\", d_in), (\"d_out\", d_out)]:\n        if not isinstance(val, int) or val <= 0:\n            raise ValueError(f\"{name} must be a positive integer, got {val}.\")\n    if k_active > n_experts:\n        raise ValueError(\"k_active cannot exceed n_experts.\")\n\n    # Use torch tensors for robust numeric handling\n    E = torch.tensor(float(n_experts), dtype=torch.float64)\n    k = torch.tensor(float(k_active), dtype=torch.float64)\n    d_in_t = torch.tensor(float(d_in), dtype=torch.float64)\n    d_out_t = torch.tensor(float(d_out), dtype=torch.float64)\n\n    dense_flops = E * d_in_t * d_out_t\n    moe_flops = k * d_in_t * d_out_t\n\n    # Avoid any potential division by zero (though inputs prevent it)\n    savings_pct = (dense_flops - moe_flops) / dense_flops * 100.0\n    return float(savings_pct.item())\n\n\ndef solution():\n    # Example usage matching the prompt\n    return compute_efficiency(1000, 2, 512, 512)\n\n\nif __name__ == \"__main__\":\n    val = solution()\n    # Expect approximately 99.8\n    print(val)\n",
  "timeComplexity": "O(1)",
  "spaceComplexity": "O(1)",
  "platform": "deepml"
};
