import { Problem } from '../../types';

export const DM_163_GRADIENT_BANDIT_ACTION_SELECTION: Problem = {
  "id": "dm_163_gradient_bandit_action_selection",
  "title": "Gradient Bandit Action Selection",
  "difficulty": "Medium",
  "tags": [
    "Probability",
    "Optimization",
    "Gradient Descent",
    "Activation Functions"
  ],
  "descriptionMarkdown": "Implement the gradient bandit algorithm for action selection in a multi-armed bandit setting. Write a class GradientBandit that maintains a vector of action preferences and updates them after each reward. The policy over actions is given by the softmax of the preferences. Provide:\n\n- softmax(): returns the current softmax probabilities over actions\n- select_action(): samples an action according to the softmax probabilities\n- update(action, reward): applies the gradient ascent update using an average-reward baseline\n\nUse the average reward as a baseline to reduce variance: after each step, update the running average of rewards and adjust the preferences accordingly. The selected action\u2019s preference should increase when the received reward is above the baseline and decrease otherwise, with the opposite effect for non-selected actions.\n\nExample:\n\n- Initialize with num_actions=3 and alpha=0.1\n- Sample an action via select_action()\n- Call update(selected_action, reward)\n- Call softmax() to retrieve updated action probabilities",
  "solutionExplanation": "The gradient bandit algorithm maintains a set of real-valued preferences H(a) for each action a. The policy is the softmax over these preferences: \u03c0(a) = exp(H(a)) / \u03a3_b exp(H(b)). After receiving a reward, we perform gradient ascent on the expected reward objective. To reduce variance, we use a baseline equal to the running average reward R\u0304.\n\nLet A be the selected action and R the observed reward. With baseline R\u0304, the update for preferences is: H \u2190 H + \u03b1 (R \u2212 R\u0304) (one_hot(A) \u2212 \u03c0), where \u03c0 is the current softmax probability vector. Intuitively, if the reward is higher than the baseline, we increase the preference for the chosen action and decrease others proportionally to their probabilities; if lower, we do the opposite. The baseline R\u0304 is updated incrementally as R\u0304 \u2190 R\u0304 + (R \u2212 R\u0304)/t, where t is the number of steps so far.\n\nEach step computes a softmax over K actions and performs a vector update, both O(K). This implementation uses stable softmax (by subtracting max preference) and samples actions using a Categorical distribution.",
  "solutionCode": "import torch\nfrom torch.distributions import Categorical\n\nclass GradientBandit:\n    \"\"\"\n    Gradient bandit for multi-armed bandits using a softmax policy over preferences.\n\n    Args:\n        num_actions (int): Number of actions (arms).\n        alpha (float): Step size for preference updates.\n        device (torch.device or str, optional): Device to host tensors. Defaults to CPU.\n        dtype (torch.dtype, optional): Floating dtype. Defaults to torch.float32.\n    \"\"\"\n    def __init__(self, num_actions: int, alpha: float = 0.1, device=None, dtype=torch.float32):\n        assert num_actions > 0, \"num_actions must be positive\"\n        self.num_actions = int(num_actions)\n        self.alpha = float(alpha)\n        self.device = torch.device(device) if device is not None else torch.device(\"cpu\")\n        self.dtype = dtype\n\n        self.preferences = torch.zeros(self.num_actions, dtype=self.dtype, device=self.device)\n        self.avg_reward = torch.tensor(0.0, dtype=self.dtype, device=self.device)\n        self.time = 0  # number of updates performed\n\n    def softmax(self) -> torch.Tensor:\n        \"\"\"Return the current policy probabilities via a numerically stable softmax.\"\"\"\n        # Subtract max for numerical stability\n        centered = self.preferences - self.preferences.max()\n        exp_vals = torch.exp(centered)\n        probs = exp_vals / exp_vals.sum()\n        return probs\n\n    def select_action(self) -> int:\n        \"\"\"Sample an action according to the current softmax policy.\"\"\"\n        probs = self.softmax()\n        action = Categorical(probs=probs).sample().item()\n        return action\n\n    def update(self, action: int, reward: float) -> None:\n        \"\"\"\n        Update preferences using the gradient ascent rule with average-reward baseline.\n\n        H <- H + alpha * (R - R_bar) * (one_hot(action) - pi)\n        R_bar <- R_bar + (R - R_bar) / t\n        \"\"\"\n        # Convert reward to tensor\n        r = torch.as_tensor(reward, dtype=self.dtype, device=self.device)\n\n        # Current policy probabilities\n        probs = self.softmax()\n\n        # Advantage using baseline BEFORE updating the baseline\n        advantage = r - self.avg_reward\n\n        # Gradient vector: one_hot(action) - probs\n        grad = -probs.clone()\n        grad[action] += 1.0\n\n        # Preference update (gradient ascent)\n        self.preferences += self.alpha * advantage * grad\n\n        # Update running average reward (incremental mean)\n        self.time += 1\n        self.avg_reward += (r - self.avg_reward) / float(self.time)\n\n\ndef solution():\n    # Example usage\n    torch.manual_seed(0)\n    gb = GradientBandit(num_actions=3, alpha=0.1)\n    a = gb.select_action()\n    gb.update(a, reward=1.0)\n    probs = gb.softmax()\n    # Return rounded probabilities for demonstration\n    return [round(float(p), 2) for p in probs]\n",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(N)",
  "platform": "deepml"
};
