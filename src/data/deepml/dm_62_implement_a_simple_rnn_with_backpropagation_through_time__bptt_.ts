import { Problem } from '../../types';

export const DM_62_IMPLEMENT_A_SIMPLE_RNN_WITH_BACKPROPAGATION_THROUGH_TIME__BPTT_: Problem = {
  "id": "dm_62_implement_a_simple_rnn_with_backpropagation_through_time_bptt",
  "title": "Implement a Simple RNN with Backpropagation Through Time (BPTT)",
  "difficulty": "Hard",
  "tags": [
    "Neural Networks",
    "RNNs",
    "Backpropagation",
    "Gradient Descent",
    "Loss Functions",
    "Activation Functions",
    "Matrix Operations"
  ],
  "descriptionMarkdown": "Task: Implement a Simple Recurrent Neural Network (RNN) and Backpropagation Through Time (BPTT)\n\nYour task is to implement a simple RNN to learn from sequential data. The network should process input sequences, update hidden states, and perform backpropagation through time to adjust weights based on the error gradient.\n\nImplement a class SimpleRNN with the following methods:\n- __init__(self, input_size, hidden_size, output_size): Initialize weights randomly (small values) and biases to zero.\n- forward(self, x): Process a sequence of inputs and return the per-timestep outputs.\n- backward(self, x, y, learning_rate): Perform BPTT to update parameters based on the loss.\n\nTraining objective:\n- Sequence prediction: at each step, predict the next item in the sequence.\n- Use 1/2 * Mean Squared Error (MSE) as the per-timestep loss and aggregate by summing across timesteps.\n\nExample:\n\n```\nimport numpy as np\ninput_sequence = np.array([[1.0], [2.0], [3.0], [4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\n\n# Initialize RNN\nrnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1)\n\n# Forward pass\noutput = rnn.forward(input_sequence)\n\n# Backward pass (one update step)\nrnn.backward(input_sequence, expected_output, learning_rate=0.01)\n\nprint(output)\n# Output should show predictions for each step of the input sequence\n```\n",
  "solutionExplanation": "A simple RNN maintains a hidden state that is updated at each timestep using the current input and the previous hidden state. With tanh as the activation, the recurrence is h_t = tanh(W_xh x_t + W_hh h_{t-1} + b_h), and the output is y_t = W_hy h_t + b_y. We accumulate the outputs across timesteps to compute the sequence predictions.\n\nFor training, we use the summed 1/2 MSE across timesteps: L = sum_t 1/2 ||y_t - y_t^*||^2. The gradient at the output is dL/dy_t = (y_t - y_t^*). Backpropagation through time proceeds in reverse order of timesteps. At each timestep t, the gradient flowing into the hidden state is dh_t = W_hy^T (dL/dy_t) + dh_{t+1}, which is then passed through the tanh derivative: da_t = dh_t \u2299 (1 - h_t^2). We accumulate parameter gradients as dW_hy += (dL/dy_t) h_t^T, dW_xh += da_t x_t^T, dW_hh += da_t h_{t-1}^T, and corresponding biases. Finally, we update parameters via gradient descent.\n\nThis implementation uses pure PyTorch tensor operations while performing manual BPTT (without autograd) to make the learning dynamics explicit. It supports numpy or torch inputs, maintains necessary intermediate states for BPTT, and applies a single-step gradient descent update per backward call.",
  "solutionCode": "import torch\nimport torch.nn as nn\nfrom typing import Union\n\nclass SimpleRNN:\n    \"\"\"\n    A simple RNN with manual Backpropagation Through Time (BPTT) implemented in PyTorch.\n    - Activation: tanh\n    - Loss: sum over timesteps of (1/2 * squared error)\n    - Parameter update: vanilla gradient descent\n    \"\"\"\n    def __init__(self, input_size: int, hidden_size: int, output_size: int, seed: int = None):\n        if seed is not None:\n            torch.manual_seed(seed)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n\n        # Initialize weights (small random) and biases (zeros)\n        self.W_xh = torch.randn(hidden_size, input_size) * 0.01\n        self.W_hh = torch.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = torch.randn(output_size, hidden_size) * 0.01\n        self.b_h = torch.zeros(hidden_size, 1)\n        self.b_y = torch.zeros(output_size, 1)\n\n        # Buffers for BPTT\n        self.last_inputs = []    # list of (input_size, 1)\n        self.last_hs = []        # list of (hidden_size, 1), includes h_0 at index 0\n        self.last_outputs = []   # list of (output_size, 1)\n\n    @staticmethod\n    def _to_tensor_2d(x: Union[torch.Tensor, 'np.ndarray'], cols: int) -> torch.Tensor:\n        # Convert to torch tensor of shape (T, cols), dtype float32\n        if isinstance(x, torch.Tensor):\n            t = x\n        else:\n            import numpy as np\n            assert isinstance(x, np.ndarray), \"Input must be torch.Tensor or np.ndarray\"\n            t = torch.from_numpy(x)\n        t = t.to(dtype=torch.float32)\n        if t.dim() == 1:\n            t = t.view(-1, cols)\n        elif t.dim() == 2:\n            if t.shape[1] != cols and t.shape[0] == cols:\n                t = t.t()\n        else:\n            raise ValueError(\"Input must be 1D or 2D with shape (T, input_size)\")\n        return t\n\n    def forward(self, x: Union[torch.Tensor, 'np.ndarray']) -> torch.Tensor:\n        \"\"\"\n        Process a sequence x with shape (T, input_size) and return outputs of shape (T, output_size).\n        Stores intermediate states for BPTT.\n        \"\"\"\n        x = self._to_tensor_2d(x, self.input_size)\n        T = x.shape[0]\n\n        self.last_inputs = []\n        self.last_hs = []\n        self.last_outputs = []\n\n        # Initial hidden state h_0\n        h_prev = torch.zeros(self.hidden_size, 1, dtype=torch.float32)\n        self.last_hs.append(h_prev)\n\n        for t in range(T):\n            x_t = x[t].reshape(self.input_size, 1)\n            self.last_inputs.append(x_t)\n            a_t = self.W_xh @ x_t + self.W_hh @ h_prev + self.b_h\n            h_t = torch.tanh(a_t)\n            y_t = self.W_hy @ h_t + self.b_y\n\n            self.last_hs.append(h_t)\n            self.last_outputs.append(y_t)\n            h_prev = h_t\n\n        # Stack outputs to shape (T, output_size)\n        outputs = torch.stack([y.squeeze(1) for y in self.last_outputs], dim=0)\n        return outputs\n\n    def backward(self, x: Union[torch.Tensor, 'np.ndarray'], y: Union[torch.Tensor, 'np.ndarray'], learning_rate: float = 0.01) -> None:\n        \"\"\"\n        Perform Backpropagation Through Time (BPTT) and update weights.\n        Loss: sum_t 1/2 * ||y_hat_t - y_t||^2 (sum over timesteps and output dims).\n        \"\"\"\n        # Ensure inputs/targets are tensors and run a forward pass to cache states\n        x = self._to_tensor_2d(x, self.input_size)\n        y = self._to_tensor_2d(y, self.output_size)\n        outputs = self.forward(x)\n        T = x.shape[0]\n\n        # Initialize gradients\n        dW_xh = torch.zeros_like(self.W_xh)\n        dW_hh = torch.zeros_like(self.W_hh)\n        dW_hy = torch.zeros_like(self.W_hy)\n        db_h = torch.zeros_like(self.b_h)\n        db_y = torch.zeros_like(self.b_y)\n\n        dh_next = torch.zeros(self.hidden_size, 1, dtype=torch.float32)\n        # Optional: compute loss (not required to return)\n        # Using explicit accumulation for clarity\n        # loss = 0.0\n\n        for t in reversed(range(T)):\n            y_hat_t = self.last_outputs[t]                 # (output_size, 1)\n            y_t = y[t].reshape(self.output_size, 1)        # (output_size, 1)\n            h_t = self.last_hs[t + 1]                      # (hidden_size, 1)\n            h_prev = self.last_hs[t]                       # (hidden_size, 1)\n            x_t = self.last_inputs[t]                      # (input_size, 1)\n\n            # dL/dy_t for 1/2 * ||y_hat_t - y_t||^2 is (y_hat_t - y_t)\n            dy = (y_hat_t - y_t)\n            # loss += 0.5 * torch.sum((y_hat_t - y_t) ** 2).item()\n\n            # Gradients for output layer\n            dW_hy += dy @ h_t.T\n            db_y += dy\n\n            # Backprop into h_t\n            dh = self.W_hy.T @ dy + dh_next\n            # tanh' = 1 - h_t^2 (elementwise)\n            da = dh * (1.0 - h_t * h_t)\n\n            # Gradients for recurrent core\n            dW_xh += da @ x_t.T\n            dW_hh += da @ h_prev.T\n            db_h += da\n\n            # Propagate to previous timestep\n            dh_next = self.W_hh.T @ da\n\n        # Gradient descent update (no autograd)\n        with torch.no_grad():\n            self.W_xh -= learning_rate * dW_xh\n            self.W_hh -= learning_rate * dW_hh\n            self.W_hy -= learning_rate * dW_hy\n            self.b_h  -= learning_rate * db_h\n            self.b_y  -= learning_rate * db_y\n\n\ndef solution():\n    # Example usage demonstrating forward and one BPTT update step\n    import numpy as np\n\n    input_sequence = np.array([[1.0], [2.0], [3.0], [4.0]], dtype=np.float32)\n    expected_output = np.array([[2.0], [3.0], [4.0], [5.0]], dtype=np.float32)\n\n    rnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1, seed=42)\n\n    # Forward pass before training step\n    output_before = rnn.forward(input_sequence)\n\n    # One BPTT step\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n\n    # Forward pass after training step\n    output_after = rnn.forward(input_sequence)\n\n    print(\"Predictions before update:\\n\", output_before)\n    print(\"Predictions after one update:\\n\", output_after)\n\n    return output_after\n",
  "timeComplexity": "O(T * (H*I + H*H + O*H)) per sequence, where T is sequence length, I=input_size, H=hidden_size, O=output_size",
  "spaceComplexity": "O(T*H) to store hidden states for BPTT, plus O(H*I + H*H + O*H) for parameters",
  "platform": "deepml"
};
