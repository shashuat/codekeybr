import { Problem } from '../../types';

export const DM_85_POSITIONAL_ENCODING_CALCULATOR: Problem = {
  "id": "dm_85_positional_encoding_calculator",
  "title": "Positional Encoding Calculator",
  "difficulty": "Hard",
  "tags": [
    "Transformers",
    "Attention",
    "Embeddings",
    "Neural Networks"
  ],
  "descriptionMarkdown": "Implement a function that computes sinusoidal positional encodings used in Transformer models. Given a sequence length (position) and model dimensionality (d_model), compute a (1, position, d_model) array where even indices use sine and odd indices use cosine with angular frequencies defined by the Transformer paper.\n\nRequirements:\n- Return -1 if position == 0 or d_model <= 0.\n- The output must be a NumPy array of dtype float16.\n- Use the standard formulation: for dimension 2i use sin(pos / 10000^(2i/d_model)) and for dimension 2i+1 use cos(pos / 10000^(2i/d_model)).\n\nExample:\n- Input: position = 2, d_model = 8\n- Output shape: (1, 2, 8)\n- Row 0 corresponds to position 0, row 1 to position 1.",
  "solutionExplanation": "Sinusoidal positional encoding provides a continuous, fixed representation of token positions that can be added to token embeddings in Transformers. For a given position p and model dimension index k, the encoding alternates between sine (even k) and cosine (odd k), with wavelengths forming a geometric progression from 2\u03c0 to 10000\u00b72\u03c0. Concretely, for even indices 2i we use sin(p / 10000^(2i/d_model)) and for odd indices 2i+1 we use cos(p / 10000^(2i/d_model)). This design allows the model to attend by relative positions and extrapolate to longer sequences.\n\nWe implement the layer in a fully vectorized manner using PyTorch tensors: create a column vector of positions and a row vector of inverse frequencies derived from 10000^(2i/d_model). Broadcasting their product yields the angle matrix for all positions and dimensions. We then fill even channels with sin and odd channels with cos. To satisfy the problem\u2019s output constraints, we convert the result to float16 and return a NumPy array with shape (1, position, d_model). Edge cases where position == 0 or d_model <= 0 return -1.",
  "solutionCode": "import math\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Compute sinusoidal positional encodings as in \"Attention Is All You Need\".\n\n    Args:\n        position (int): Sequence length (number of positions). Must be > 0.\n        d_model (int): Model dimensionality. Must be > 0.\n\n    Returns:\n        -1 if invalid input; otherwise a numpy.ndarray with shape (1, position, d_model)\n        and dtype float16 containing the positional encodings.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n\n    # Compute on CPU; use float32 for numerical stability, cast to float16 at the end\n    pe = torch.zeros((position, d_model), dtype=torch.float32)\n\n    # We fill pairs of dimensions: even -> sin, odd -> cos\n    half_dim = d_model // 2\n    if half_dim > 0:\n        i = torch.arange(half_dim, dtype=torch.float32)\n        # exponent = (2i) / d_model, so base^(exponent) with base=10000\n        exponent = (2.0 * i) / float(d_model)\n        inv_freq = torch.exp(-math.log(10000.0) * exponent)  # shape: (half_dim,)\n\n        pos = torch.arange(position, dtype=torch.float32).unsqueeze(1)  # (position, 1)\n        angles = pos * inv_freq.unsqueeze(0)  # (position, half_dim)\n\n        pe[:, 0::2] = torch.sin(angles)\n        pe[:, 1::2] = torch.cos(angles)\n\n    # If d_model is odd, the last channel remains zero (consistent with using floor(d_model/2) pairs)\n    pe = pe.unsqueeze(0)  # (1, position, d_model)\n\n    # Return numpy array with dtype float16\n    return pe.to(dtype=torch.float16).cpu().numpy()\n\n\ndef solution():\n    # Example usage\n    arr = pos_encoding(position=2, d_model=8)\n    return arr\n",
  "timeComplexity": "O(position * d_model)",
  "spaceComplexity": "O(position * d_model)",
  "platform": "deepml"
};
