import { Problem } from '../../types';

export const DM_169_IMPLEMENT_ADAMW_OPTIMIZER_STEP: Problem = {
  "id": "dm_169_implement_adamw_optimizer_step",
  "title": "Implement AdamW Optimizer Step",
  "difficulty": "Medium",
  "tags": [
    "Optimization",
    "Regularization",
    "Gradient Descent"
  ],
  "descriptionMarkdown": "Implement a single update step of the AdamW optimizer for a parameter vector w and its gradient g. AdamW is a variant of Adam that decouples weight decay from the gradient-based update, often improving generalization.\n\nYour task: Write a function adamw_update(w, g, m, v, t, lr, beta1, beta2, epsilon, weight_decay) that performs one AdamW step. The function should:\n- Update the first moment m and second moment v (moving averages of gradients and squared gradients)\n- Apply bias correction to both m and v\n- Apply the AdamW update rule with decoupled weight decay\n- Return the updated parameter vector and the new values of m and v\n\nArguments:\n- w: parameter vector (same shape as g)\n- g: gradient vector (same shape as w)\n- m: first moment vector (same shape as w)\n- v: second moment vector (same shape as w)\n- t: integer time step (starting from 1)\n- lr: learning rate (float)\n- beta1: decay rate for first moment (float)\n- beta2: decay rate for second moment (float)\n- epsilon: small constant for numerical stability (float)\n- weight_decay: weight decay coefficient (float)\n\nExample:\n\nInput:\n- w = [1.0, 2.0]\n- g = [0.1, -0.2]\n- m = [0.0, 0.0]\n- v = [0.0, 0.0]\n- t = 1, lr = 0.01, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, weight_decay = 0.1\n\nOutput (rounded):\n- w_new \u2248 [0.989, 2.008]",
  "solutionExplanation": "AdamW maintains exponential moving averages of the gradients (first moment m) and squared gradients (second moment v). At each step t, the raw moments are updated as m = beta1 * m + (1 - beta1) * g and v = beta2 * v + (1 - beta2) * (g \u2299 g). Because m and v are initialized at zero, we apply bias correction to obtain unbiased estimates: m_hat = m / (1 - beta1^t) and v_hat = v / (1 - beta2^t).\n\nThe Adam update uses m_hat normalized by the square root of v_hat (plus epsilon for stability): update = m_hat / (sqrt(v_hat) + epsilon). AdamW decouples weight decay from the gradient update by subtracting an additional lr * weight_decay * w directly from the parameters, rather than mixing it into the gradient. Thus, the final parameter update is: w_new = w - lr * (update + weight_decay * w).\n\nThis decoupling prevents weight decay from being distorted by the adaptive moments and typically yields better generalization than the L2-regularized Adam formulation.",
  "solutionCode": "import torch\n\ndef adamw_update(w, g, m, v, t, lr, beta1, beta2, epsilon, weight_decay):\n    \"\"\"\n    Perform one AdamW optimizer step.\n\n    Args:\n        w (torch.Tensor or array-like): Current parameters.\n        g (torch.Tensor or array-like): Current gradients (same shape as w).\n        m (torch.Tensor or array-like): First moment vector (same shape as w).\n        v (torch.Tensor or array-like): Second moment vector (same shape as w).\n        t (int): Time step (starting from 1).\n        lr (float): Learning rate.\n        beta1 (float): Exponential decay rate for first moment estimates.\n        beta2 (float): Exponential decay rate for second moment estimates.\n        epsilon (float): Small term to avoid division by zero.\n        weight_decay (float): Decoupled weight decay coefficient.\n\n    Returns:\n        (w_new, m_new, v_new): Updated parameter tensor and updated moments.\n    \"\"\"\n    # Convert to tensors and ensure consistent dtype/device\n    w = torch.as_tensor(w).clone()\n    g = torch.as_tensor(g, dtype=w.dtype, device=w.device)\n    m = torch.as_tensor(m, dtype=w.dtype, device=w.device).clone()\n    v = torch.as_tensor(v, dtype=w.dtype, device=w.device).clone()\n\n    # Update biased first and second moment estimates\n    m = beta1 * m + (1.0 - beta1) * g\n    v = beta2 * v + (1.0 - beta2) * (g * g)\n\n    # Bias correction\n    beta1_t = beta1 ** t\n    beta2_t = beta2 ** t\n    m_hat = m / (1.0 - beta1_t)\n    v_hat = v / (1.0 - beta2_t)\n\n    # Compute AdamW update (decoupled weight decay)\n    denom = v_hat.sqrt().add(epsilon)\n    adam_step = m_hat / denom\n\n    # w <- w - lr * (adam_step + weight_decay * w)\n    w = w - lr * (adam_step + weight_decay * w)\n\n    return w, m, v\n\n\ndef solution():\n    # Example usage matching the problem statement\n    w = torch.tensor([1.0, 2.0])\n    g = torch.tensor([0.1, -0.2])\n    m = torch.zeros_like(w)\n    v = torch.zeros_like(w)\n    t = 1\n    lr = 0.01\n    beta1 = 0.9\n    beta2 = 0.999\n    epsilon = 1e-8\n    weight_decay = 0.1\n\n    w_new, m_new, v_new = adamw_update(w, g, m, v, t, lr, beta1, beta2, epsilon, weight_decay)\n\n    # Print updated weights (rounded for readability); expected approx: [0.989, 2.008]\n    print((torch.round(w_new * 1000) / 1000).tolist())\n",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(1)",
  "platform": "deepml"
};
