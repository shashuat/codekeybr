import { Problem } from '../../types';

export const DM_143_INSTANCE_NORMALIZATION__IN__IMPLEMENTATION: Problem = {
  "id": "dm_143_instance_normalization_in_implementation",
  "title": "Instance Normalization (IN) Implementation",
  "difficulty": "Medium",
  "tags": [
    "Neural Networks",
    "CNNs",
    "Matrix Operations",
    "Backpropagation"
  ],
  "descriptionMarkdown": "Implement the Instance Normalization operation for 4D tensors of shape (B, C, H, W). For each instance in the batch and each channel, normalize the spatial dimensions (height and width) by subtracting the mean and dividing by the standard deviation, then apply a learned scale (gamma) and shift (beta).\n\n- Input: X of shape (B, C, H, W)\n- Parameters: gamma (C,), beta (C,)\n- Operation per instance and channel: normalize across (H, W)\n\nExample (NumPy-style for reference):\n\n```\nimport numpy as np\nB, C, H, W = 2, 2, 2, 2\nnp.random.seed(42)\nX = np.random.randn(B, C, H, W)\ngamma = np.ones(C)\nbeta = np.zeros(C)\nout = instance_normalization(X, gamma, beta)\nprint(np.round(out, 8))\n```\n\nExpected behavior: The function normalizes each instance and channel across (H, W), then applies gamma and beta. This matches standard InstanceNorm behavior.",
  "solutionExplanation": "Instance Normalization normalizes each channel of each sample independently across spatial dimensions. For a 4D tensor X with shape (B, C, H, W), we compute the mean and variance over the last two dimensions (H and W) for each (B, C) pair. Specifically, for each b in [0, B) and c in [0, C), compute mu_{b,c} and var_{b,c} over all spatial elements X[b, c, :, :]. To ensure numerical stability, an epsilon term is added before taking the square root of the variance.\n\nAfter obtaining normalized activations \\hat{X} = (X - mu) / sqrt(var + eps), we apply an affine transformation with learnable per-channel parameters gamma and beta: Y = gamma * \\hat{X} + beta. Broadcasting is used by reshaping gamma and beta to (1, C, 1, 1) so they apply uniformly across spatial positions and batch items.\n\nThis operation is differentiable and commonly used in style transfer and tasks where per-instance, per-channel normalization is beneficial. In PyTorch, we implement it with tensor operations over dims (2, 3), using unbiased=False for variance to match NumPy's default ddof=0 behavior.",
  "solutionCode": "import torch\nimport torch.nn as nn\nfrom typing import Tuple\n\ndef instance_normalization(x: torch.Tensor, gamma: torch.Tensor, beta: torch.Tensor, epsilon: float = 1e-5) -> torch.Tensor:\n    \"\"\"Apply Instance Normalization to a 4D tensor (B, C, H, W).\n\n    For each instance (per batch element) and per channel, compute mean/variance\n    across spatial dimensions (H, W), normalize, then apply per-channel gamma/beta.\n\n    Args:\n        x: Input tensor of shape (B, C, H, W).\n        gamma: Scale parameters of shape (C,).\n        beta: Shift parameters of shape (C,).\n        epsilon: Small constant added to variance for numerical stability.\n\n    Returns:\n        Tensor of shape (B, C, H, W) after instance normalization.\n    \"\"\"\n    if x.dim() != 4:\n        raise ValueError(f\"Expected 4D input (B, C, H, W), got shape {tuple(x.shape)}\")\n    B, C, H, W = x.shape\n\n    if gamma.dim() != 1 or beta.dim() != 1:\n        raise ValueError(\"gamma and beta must be 1D tensors of shape (C,)\")\n    if gamma.numel() != C or beta.numel() != C:\n        raise ValueError(f\"gamma and beta must have length C={C}, got {gamma.numel()} and {beta.numel()}\")\n\n    # Ensure gamma/beta are on the same device and dtype as x\n    gamma = gamma.to(device=x.device, dtype=x.dtype)\n    beta = beta.to(device=x.device, dtype=x.dtype)\n\n    # Compute per-instance, per-channel mean and variance over spatial dims\n    mean = x.mean(dim=(2, 3), keepdim=True)  # (B, C, 1, 1)\n    # Use unbiased=False to match NumPy's ddof=0\n    var = x.var(dim=(2, 3), keepdim=True, unbiased=False)  # (B, C, 1, 1)\n\n    # Normalize\n    x_hat = (x - mean) / torch.sqrt(var + epsilon)\n\n    # Apply affine transform (broadcast gamma/beta to (1, C, 1, 1))\n    y = x_hat * gamma.view(1, C, 1, 1) + beta.view(1, C, 1, 1)\n    return y\n\nclass InstanceNorm2dCustom(nn.Module):\n    \"\"\"A minimal InstanceNorm2d-like module using the functional implementation above.\n\n    This module learns per-channel affine parameters (gamma/beta) and applies\n    instance normalization without running statistics (as in standard IN).\n    \"\"\"\n    def __init__(self, num_channels: int, eps: float = 1e-5, affine: bool = True):\n        super().__init__()\n        self.eps = eps\n        self.affine = affine\n        if affine:\n            self.gamma = nn.Parameter(torch.ones(num_channels))\n            self.beta = nn.Parameter(torch.zeros(num_channels))\n        else:\n            self.register_parameter('gamma', None)\n            self.register_parameter('beta', None)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        C = x.size(1)\n        if self.affine:\n            return instance_normalization(x, self.gamma, self.beta, self.eps)\n        else:\n            # If no affine parameters, use gamma=1, beta=0\n            device, dtype = x.device, x.dtype\n            gamma = torch.ones(C, device=device, dtype=dtype)\n            beta = torch.zeros(C, device=device, dtype=dtype)\n            return instance_normalization(x, gamma, beta, self.eps)\n\n\ndef solution() -> Tuple[torch.Tensor, torch.Tensor]:\n    # Example usage\n    B, C, H, W = 2, 2, 2, 2\n    torch.manual_seed(42)\n    X = torch.randn(B, C, H, W)\n\n    gamma = torch.ones(C)\n    beta = torch.zeros(C)\n\n    out = instance_normalization(X, gamma, beta)\n\n    # Print rounded output (for readability)\n    print(torch.round(out * 1e8) / 1e8)\n\n    # Validate zero-mean and unit-variance per (B, C) when gamma=1, beta=0\n    mean = out.mean(dim=(2, 3))\n    var = out.var(dim=(2, 3), unbiased=False)\n    print(\"Per (B,C) means:\\n\", torch.round(mean * 1e6) / 1e6)\n    print(\"Per (B,C) variances:\\n\", torch.round(var * 1e6) / 1e6)\n\n    # Also demonstrate the Module API\n    layer = InstanceNorm2dCustom(num_channels=C, eps=1e-5, affine=True)\n    # Initialize module gamma/beta to match the functional example\n    with torch.no_grad():\n        layer.gamma.copy_(gamma)\n        layer.beta.copy_(beta)\n    out_module = layer(X)\n    # Check close\n    assert torch.allclose(out, out_module, atol=1e-6), \"Module and functional outputs should match\"\n\n    return out, out_module\n\nif __name__ == \"__main__\":\n    solution()\n",
  "timeComplexity": "O(B*C*H*W)",
  "spaceComplexity": "O(B*C) additional (for per-instance, per-channel statistics)",
  "platform": "deepml"
};
