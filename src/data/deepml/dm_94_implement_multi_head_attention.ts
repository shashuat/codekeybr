import { Problem } from '../../types';

export const DM_94_IMPLEMENT_MULTI_HEAD_ATTENTION: Problem = {
  "id": "dm_94_implement_multi_head_attention",
  "title": "Implement Multi-Head Attention",
  "difficulty": "Hard",
  "tags": [
    "Transformers",
    "Attention",
    "Neural Networks",
    "Matrix Operations",
    "Linear Algebra"
  ],
  "descriptionMarkdown": "Implement the multi-head attention mechanism, a core component of Transformer models. You must implement three functions:\n\n- compute_qkv(X, W_q, W_k, W_v): Compute Query, Key, and Value matrices by multiplying input X with weight matrices. Returns a tuple (Q, K, V), each of shape (seq_len, d_model).\n- self_attention(Q, K, V): Compute scaled dot-product attention for a single head. Returns the attention output with the same shape as V.\n- multi_head_attention(Q, K, V, n_heads): Split Q, K, V into multiple heads along the feature dimension, compute self-attention for each head independently, and concatenate the results. Returns output with the same shape as Q.\n\nInputs:\n- X: Input matrix of shape (seq_len, d_model) representing a sequence of token embeddings\n- W_q, W_k, W_v: Weight matrices of shape (d_model, d_model)\n- n_heads: Number of attention heads (must evenly divide d_model)\n\nWorkflow: First call compute_qkv to get Q, K, V matrices, then pass them to multi_head_attention.\n\nImportant: Use numerically stable softmax (subtract max before exponentiating) to prevent overflow.\n\nExample:\n- X = [[1, 2, 3, 4], [5, 6, 7, 8]] (shape (2, 4))\n- W_q = W_k = W_v = identity(4)\n- Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n- result = multi_head_attention(Q, K, V, n_heads=2)\n- result.shape = (2, 4)\n",
  "solutionExplanation": "Multi-head attention projects the input sequence embeddings X into three learned representations: queries (Q), keys (K), and values (V). The scaled dot-product attention for a single head is computed by forming similarity scores between queries and keys (QK^T), scaling by the square root of the head dimension to stabilize gradients, applying a softmax to obtain attention weights, and finally weighting values V by these attention weights. To prevent numerical overflow, we use a numerically stable softmax by subtracting the per-row maximum before exponentiation.\n\nIn multi-head attention, we split the feature dimension d_model into n_heads heads, each with head_dim = d_model / n_heads. We compute self-attention independently for each head using the single-head routine, then concatenate the head outputs along the feature dimension to recover the original model dimension. This allows the model to jointly attend to information from different representation subspaces at different positions.\n\nThe implementation provided uses pure PyTorch tensor operations, includes input validation, and follows the required function signatures. It operates on tensors of shape (seq_len, d_model) without a batch dimension for simplicity, matching the prompt's specification.",
  "solutionCode": "import math\nfrom typing import Tuple\nimport torch\n\n\ndef _stable_softmax(x: torch.Tensor, dim: int = -1) -> torch.Tensor:\n    \"\"\"Numerically stable softmax: subtract max before exp.\"\"\"\n    max_vals, _ = torch.max(x, dim=dim, keepdim=True)\n    x_shifted = x - max_vals\n    exp_x = torch.exp(x_shifted)\n    sum_exp = torch.sum(exp_x, dim=dim, keepdim=True)\n    # Add small epsilon to avoid division by zero (though sum_exp should be > 0)\n    eps = torch.finfo(x.dtype).eps if x.is_floating_point() else 1e-12\n    return exp_x / (sum_exp + eps)\n\n\ndef compute_qkv(\n    X: torch.Tensor,\n    W_q: torch.Tensor,\n    W_k: torch.Tensor,\n    W_v: torch.Tensor,\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n\n    Args:\n        X: Tensor of shape (seq_len, d_model)\n        W_q, W_k, W_v: Tensors of shape (d_model, d_model)\n\n    Returns:\n        Tuple (Q, K, V), each of shape (seq_len, d_model)\n    \"\"\"\n    if X.dim() != 2:\n        raise ValueError(\"X must be 2D of shape (seq_len, d_model)\")\n    d_model = X.shape[1]\n    for name, W in [(\"W_q\", W_q), (\"W_k\", W_k), (\"W_v\", W_v)]:\n        if W.dim() != 2 or W.shape != (d_model, d_model):\n            raise ValueError(f\"{name} must be of shape (d_model, d_model)=({d_model}, {d_model})\")\n\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return Q, K, V\n\n\ndef self_attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Compute scaled dot-product attention for a single head.\n\n    Args:\n        Q, K, V: Tensors of shape (seq_len, d_head)\n\n    Returns:\n        Tensor of shape (seq_len, d_head)\n    \"\"\"\n    if not (Q.dim() == K.dim() == V.dim() == 2):\n        raise ValueError(\"Q, K, V must be 2D tensors of shape (seq_len, d_head)\")\n    if not (Q.shape == K.shape == V.shape):\n        raise ValueError(\"Q, K, V must have the same shape\")\n\n    seq_len, d_head = Q.shape\n    # Scaled dot-product attention\n    scores = (Q @ K.transpose(0, 1)) / math.sqrt(d_head)  # (seq_len, seq_len)\n    attn = _stable_softmax(scores, dim=-1)                # (seq_len, seq_len)\n    out = attn @ V                                       # (seq_len, d_head)\n    return out\n\n\ndef multi_head_attention(\n    Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, n_heads: int\n) -> torch.Tensor:\n    \"\"\"\n    Multi-head attention over Q, K, V (no batch dimension).\n\n    Args:\n        Q, K, V: Tensors of shape (seq_len, d_model)\n        n_heads: Number of attention heads (must divide d_model)\n\n    Returns:\n        Tensor of shape (seq_len, d_model)\n    \"\"\"\n    if not (Q.shape == K.shape == V.shape):\n        raise ValueError(\"Q, K, V must have the same shape (seq_len, d_model)\")\n    if Q.dim() != 2:\n        raise ValueError(\"Q, K, V must be 2D tensors of shape (seq_len, d_model)\")\n\n    seq_len, d_model = Q.shape\n    if d_model % n_heads != 0:\n        raise ValueError(\"n_heads must evenly divide d_model\")\n    head_dim = d_model // n_heads\n\n    # Split into heads: (seq_len, d_model) -> (seq_len, n_heads, head_dim)\n    Qh = Q.contiguous().view(seq_len, n_heads, head_dim)\n    Kh = K.contiguous().view(seq_len, n_heads, head_dim)\n    Vh = V.contiguous().view(seq_len, n_heads, head_dim)\n\n    # Compute attention per head independently using the single-head routine\n    outputs = []\n    for h in range(n_heads):\n        Q_single = Qh[:, h, :]  # (seq_len, head_dim)\n        K_single = Kh[:, h, :]\n        V_single = Vh[:, h, :]\n        out_h = self_attention(Q_single, K_single, V_single)  # (seq_len, head_dim)\n        outputs.append(out_h)\n\n    # Concatenate heads along the feature dimension: (seq_len, d_model)\n    out = torch.cat(outputs, dim=-1)\n    return out\n\n\ndef solution():\n    # Example usage matching the prompt\n    X = torch.tensor([[1., 2., 3., 4.], [5., 6., 7., 8.]], dtype=torch.float32)  # (2, 4)\n    W_q = torch.eye(4, dtype=torch.float32)\n    W_k = torch.eye(4, dtype=torch.float32)\n    W_v = torch.eye(4, dtype=torch.float32)\n\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    result = multi_head_attention(Q, K, V, n_heads=2)\n\n    # Example check\n    # print(result.shape)  # should be: torch.Size([2, 4])\n    return result\n",
  "timeComplexity": "O(L^2 * d_model)",
  "spaceComplexity": "O(L^2 + L * d_model)",
  "platform": "deepml"
};
