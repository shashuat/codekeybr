import { Problem } from '../../types';

export const DM_156_IMPLEMENT_SWIGLU_ACTIVATION_FUNCTION: Problem = {
  "id": "dm_156_implement_swiglu_activation_function",
  "title": "Implement SwiGLU activation function",
  "difficulty": "Easy",
  "tags": [
    "Neural Networks",
    "Activation Functions",
    "Transformers"
  ],
  "descriptionMarkdown": "Implement a function that applies the SwiGLU activation to an input of shape (batch_size, 2d). The input is assumed to be the output of a linear projection. Split the input along the last dimension into two equal parts: x1 and x2. Compute Swish on x2 as swish(x2) = x2 * sigmoid(x2), then compute SwiGLU as x1 * swish(x2). Finally, round each output value to four decimal places and return a tensor of shape (batch_size, d).\n\nExample:\n\nInput:\n\nnp.array([[1, -1, 1000, -1000]])\n\nOutput:\n\n[[1000., 0.]]\n\nReasoning:\nThe input is of shape (1, 4), so it is split into x1 = [1, -1] and x2 = [1000, -1000]. The sigmoid of 1000 is approximately 1, and the sigmoid of -1000 is approximately 0. Thus, Swish(1000) \u2248 1000 and Swish(-1000) \u2248 0. Then, SwiGLU = x1 * Swish(x2) = [1000, 0].",
  "solutionExplanation": "SwiGLU is a gated activation commonly used in modern Transformer feed-forward networks. Given an input of shape (batch_size, 2d), we split it into two halves along the last dimension: x1 and x2. We then apply the Swish activation to x2, defined as swish(z) = z * sigmoid(z). The final output is the element-wise product x1 * swish(x2), resulting in shape (batch_size, d).\n\nFor numerical stability and simplicity, we use PyTorch's built-in sigmoid. After computing the SwiGLU outputs, we round each element to four decimal places using a multiply-round-divide approach to ensure consistent behavior across PyTorch versions. The implementation is fully vectorized for efficiency and includes both a functional form and an nn.Module wrapper. We also validate that the last dimension is even to allow an equal split.",
  "solutionCode": "import torch\nimport torch.nn as nn\nfrom typing import Tuple\n\n\ndef _round4(x: torch.Tensor) -> torch.Tensor:\n    \"\"\"Round tensor elements to 4 decimal places in a vectorized manner.\"\"\"\n    return torch.round(x * 10_000.0) / 10_000.0\n\n\ndef swiglu(x: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Apply the SwiGLU activation to input tensor x.\n\n    SwiGLU(x) = x1 * swish(x2), where swish(z) = z * sigmoid(z), and\n    x is split evenly along the last dimension into x1 and x2.\n\n    Args:\n        x: Tensor of shape (batch_size, 2d) or any shape with even last dimension.\n\n    Returns:\n        Tensor of shape (batch_size, d) (or matching the input shape with last dim halved),\n        rounded to four decimal places.\n    \"\"\"\n    if x.dim() < 1:\n        raise ValueError(\"Input must have at least 1 dimension.\")\n    last_dim = x.size(-1)\n    if last_dim % 2 != 0:\n        raise ValueError(\"Last dimension must be even to split into (x1, x2).\")\n\n    x1, x2 = torch.chunk(x, 2, dim=-1)\n    swish_x2 = x2 * torch.sigmoid(x2)\n    out = x1 * swish_x2\n    return _round4(out)\n\n\nclass SwiGLU(nn.Module):\n    \"\"\"nn.Module wrapper for the SwiGLU activation with 4-decimal rounding.\"\"\"\n    def __init__(self) -> None:\n        super().__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return swiglu(x)\n\n\ndef solution():\n    # Example usage matching the problem statement\n    # Input: [[1, -1, 1000, -1000]] -> shape (1, 4) so d=2\n    x = torch.tensor([[1.0, -1.0, 1000.0, -1000.0]])\n\n    # Functional API\n    y_func = swiglu(x)\n    print(\"Functional SwiGLU output (torch):\", y_func)\n    print(\"Functional SwiGLU output (numpy):\", y_func.detach().cpu().numpy())\n\n    # Module API\n    act = SwiGLU()\n    y_mod = act(x)\n    print(\"Module SwiGLU output (torch):\", y_mod)\n\n    return y_func\n",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(N)",
  "platform": "deepml"
};
