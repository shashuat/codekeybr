import { Problem } from '../../types';

export const DM_155_COSINEANNEALINGLR_LEARNING_RATE_SCHEDULER: Problem = {
  "id": "dm_155_cosineannealinglr_learning_rate_scheduler",
  "title": "CosineAnnealingLR Learning Rate Scheduler",
  "difficulty": "Medium",
  "tags": [
    "Optimization",
    "Gradient Descent",
    "Neural Networks"
  ],
  "descriptionMarkdown": "Implement a cosine annealing learning rate scheduler.\n\nWrite a Python class `CosineAnnealingLRScheduler` with:\n- `__init__(self, initial_lr: float, T_max: int, min_lr: float)` to set the initial learning rate, the maximum number of iterations/epochs per cycle, and the minimum learning rate.\n- `get_lr(self, epoch: int) -> float` that returns the learning rate at a given epoch following a cosine annealing schedule. The learning rate must be rounded to 4 decimal places.\n\nThe schedule should start at `initial_lr` at epoch 0, decay following a cosine curve to `min_lr` at epoch `T_max`, and then cycle back (i.e., repeat every `T_max` epochs).\n\nExample:\n\nInput:\n```\nimport math\nscheduler = CosineAnnealingLRScheduler(initial_lr=0.1, T_max=10, min_lr=0.001)\nprint(f\"{scheduler.get_lr(epoch=0):.4f}\")\nprint(f\"{scheduler.get_lr(epoch=2):.4f}\")\nprint(f\"{scheduler.get_lr(epoch=5):.4f}\")\nprint(f\"{scheduler.get_lr(epoch=7):.4f}\")\nprint(f\"{scheduler.get_lr(epoch=10):.4f}\")\n```\n\nOutput:\n```\n0.1000\n0.0905\n0.0505\n0.0214\n0.0010\n```\n\nReasoning: The learning rate starts at `initial_lr`, follows a cosine curve, reaches `min_lr` at `T_max`, and then repeats. Each value is rounded to 4 decimal places.",
  "solutionExplanation": "Cosine annealing gradually decreases the learning rate from an initial value to a minimum value following a half-cosine curve. For a given effective epoch t within a cycle of length T_max, the learning rate is computed as:\n\nlr(t) = min_lr + 0.5 * (initial_lr - min_lr) * (1 + cos(pi * t / T_max))\n\nThis ensures lr(0) = initial_lr and lr(T_max) = min_lr. To support cycling, we map any epoch to its position within the current cycle. We take t = epoch % T_max, except when the remainder is 0 and epoch > 0, where we set t = T_max so that exact multiples of T_max hit the minimum learning rate rather than restarting at the maximum.\n\nWe implement this with PyTorch's cosine function for numerical stability and consistency, then round the final scalar result to four decimal places to match the problem's requirements. The method runs in constant time and uses constant space.",
  "solutionCode": "import torch\nfrom typing import List\n\nclass CosineAnnealingLRScheduler:\n    \"\"\"Cosine Annealing Learning Rate Scheduler with cycling every T_max epochs.\n\n    lr(t) = min_lr + 0.5 * (initial_lr - min_lr) * (1 + cos(pi * t / T_max))\n\n    where t is the effective epoch within the current cycle.\n    \"\"\"\n    def __init__(self, initial_lr: float, T_max: int, min_lr: float):\n        if T_max <= 0:\n            raise ValueError(\"T_max must be a positive integer.\")\n        if initial_lr < 0 or min_lr < 0:\n            raise ValueError(\"Learning rates must be non-negative.\")\n        if min_lr > initial_lr:\n            raise ValueError(\"min_lr cannot be greater than initial_lr.\")\n        self.initial_lr = float(initial_lr)\n        self.T_max = int(T_max)\n        self.min_lr = float(min_lr)\n\n    def get_lr(self, epoch: int) -> float:\n        \"\"\"Return the cosine-annealed learning rate at a given epoch.\n\n        The schedule cycles every T_max epochs and returns values rounded to 4 decimals.\n        \"\"\"\n        if epoch < 0:\n            raise ValueError(\"epoch must be non-negative.\")\n        if epoch == 0:\n            lr_val = self.initial_lr\n        else:\n            # Map to effective epoch within cycle; ensure multiples of T_max hit min_lr\n            r = epoch % self.T_max\n            t_cur = self.T_max if r == 0 else r\n            # Compute cosine with PyTorch\n            t_cur_t = torch.tensor(float(t_cur), dtype=torch.float64)\n            cos_arg = t_cur_t * (torch.pi / float(self.T_max))\n            cos_val = torch.cos(cos_arg).item()\n            lr_val = self.min_lr + 0.5 * (self.initial_lr - self.min_lr) * (1.0 + cos_val)\n        return round(float(lr_val), 4)\n\n\ndef solution():\n    # Example usage demonstrating required behavior\n    scheduler = CosineAnnealingLRScheduler(initial_lr=0.1, T_max=10, min_lr=0.001)\n    epochs: List[int] = [0, 2, 5, 7, 10]\n    for e in epochs:\n        print(f\"{scheduler.get_lr(epoch=e):.4f}\")\n",
  "timeComplexity": "O(1)",
  "spaceComplexity": "O(1)",
  "platform": "deepml"
};
