import { Problem } from '../../types';

export const DM_21_PEGASOS_KERNEL_SVM_IMPLEMENTATION: Problem = {
  "id": "dm_21_pegasos_kernel_svm_implementation",
  "title": "Pegasos Kernel SVM Implementation",
  "difficulty": "Hard",
  "tags": [
    "Optimization",
    "Regularization",
    "Gradient Descent",
    "Loss Functions",
    "Matrix Operations",
    "Linear Algebra"
  ],
  "descriptionMarkdown": "Implement a deterministic (full-batch) version of the Pegasos algorithm to train a kernel SVM classifier from scratch. The function should:\n\n- Accept a dataset as a 2D NumPy array (rows are samples, columns are features) and a label vector as a 1D NumPy array with labels in {\u22121, +1}.\n- Support a choice of kernel: 'linear' or 'rbf' (Gaussian), with RBF width parameter sigma when applicable.\n- Take the regularization parameter lambda (lambda_val) and the number of training iterations.\n- Use all samples at every iteration (no random sampling) and perform binary classification.\n- Return the learned alpha coefficients and bias term b.\n\nExample:\n- Input: data = [[1,2],[2,3],[3,1],[4,1]], labels = [1,1,-1,-1], kernel = 'rbf', lambda_val = 0.01, iterations = 100, sigma = 1.0\n- Output: alpha = [..], b = ..\n\nNotes:\n- Use the kernelized Pegasos update with step size \u03b7_t = 1 / (\u03bb t).\n- For the full-batch variant, violations are identified as samples with y_i f(x_i) < 1 at each iteration.\n- The decision function is f(x) = \\sum_j alpha_j y_j K(x_j, x) + b.",
  "solutionExplanation": "Pegasos minimizes the primal SVM objective with L2 regularization and hinge loss using (stochastic) subgradient descent. In the kernelized setting, the weight vector is represented implicitly as w = \\sum_j alpha_j y_j \u03c6(x_j), so the decision function becomes f(x) = \\sum_j alpha_j y_j K(x_j, x) + b. The subgradient step with a full batch of size n uses the set of margin-violating indices M = {i | y_i f(x_i) < 1}. For the primal update, w is shrunk by a factor (1 \u2212 \u03b7_t \u03bb) and then updated by the average contribution of violating samples. In the dual-like representation, this translates to shrinking all alpha by the same factor and adding \u03b7_t / n to alpha_i for each violating i. The bias b is updated using the subgradient of the hinge loss with respect to b, which adds \u03b7_t (1/n) \\sum_{i\u2208M} y_i.\n\nTo keep the algorithm deterministic, each iteration computes the full margin vector on the entire dataset using the precomputed kernel matrix K (either linear or RBF). The step size follows \u03b7_t = 1/(\u03bb t), and no random sampling is involved. This implementation avoids the explicit projection step used in the original Pegasos for w, which works well in practice here because alpha remains nonnegative (initialized at zero, shrunken, and incremented positively) and the kernel matrix mediates the effective norm of w. The method returns the alpha coefficients and bias b, which together define the classifier.",
  "solutionCode": "import torch\nfrom typing import List, Tuple\n\ndef _linear_kernel(X: torch.Tensor) -> torch.Tensor:\n    # X: (n, d) -> K: (n, n)\n    return X @ X.T\n\n\ndef _rbf_kernel(X: torch.Tensor, sigma: float) -> torch.Tensor:\n    # Efficient pairwise squared distances for RBF kernel\n    # K_ij = exp(-||x_i - x_j||^2 / (2 * sigma^2))\n    X_norm = (X ** 2).sum(dim=1, keepdim=True)  # (n, 1)\n    dists = X_norm + X_norm.T - 2.0 * (X @ X.T)\n    gamma = 1.0 / (2.0 * (sigma ** 2))\n    return torch.exp(-gamma * dists)\n\n\ndef pegasos_kernel_svm(\n    data, \n    labels, \n    kernel: str = 'linear', \n    lambda_val: float = 0.01, \n    iterations: int = 100, \n    sigma: float = 1.0,\n    dtype: torch.dtype = torch.float64,\n    device: torch.device = torch.device('cpu')\n) -> Tuple[List[float], float]:\n    \"\"\"\n    Deterministic (full-batch) Pegasos for Kernel SVM using PyTorch.\n\n    Args:\n        data: 2D NumPy array or array-like of shape (n_samples, n_features)\n        labels: 1D NumPy array or array-like of shape (n_samples,) with labels in {-1, +1}\n        kernel: 'linear' or 'rbf'\n        lambda_val: regularization parameter (lambda > 0)\n        iterations: number of full-batch iterations (T)\n        sigma: RBF kernel width (only used if kernel == 'rbf')\n        dtype: torch dtype to use (default: float64 for numerical stability)\n        device: torch device (default: CPU)\n\n    Returns:\n        (alphas, b): list of alpha coefficients (length n_samples) and bias (float)\n    \"\"\"\n    if lambda_val <= 0:\n        raise ValueError(\"lambda_val must be > 0\")\n    if kernel not in ('linear', 'rbf'):\n        raise ValueError(\"kernel must be 'linear' or 'rbf'\")\n    if kernel == 'rbf' and sigma <= 0:\n        raise ValueError(\"sigma must be > 0 for RBF kernel\")\n\n    # Convert inputs to tensors\n    X = torch.as_tensor(data, dtype=dtype, device=device)\n    y = torch.as_tensor(labels, dtype=dtype, device=device)\n\n    # Ensure labels are in {-1, +1}\n    y_unique = torch.unique(y)\n    if not torch.all((y_unique == -1) | (y_unique == 1)):\n        # Attempt to map {0,1} to {-1,1}\n        y = torch.where(y > 0, torch.tensor(1.0, dtype=dtype, device=device), torch.tensor(-1.0, dtype=dtype, device=device))\n\n    n, d = X.shape\n\n    # Precompute kernel matrix K (n x n)\n    if kernel == 'linear':\n        K = _linear_kernel(X)\n    else:\n        K = _rbf_kernel(X, sigma)\n\n    # Initialize parameters\n    alpha = torch.zeros(n, dtype=dtype, device=device)\n    b = torch.tensor(0.0, dtype=dtype, device=device)\n\n    # Full-batch Pegasos iterations\n    for t in range(1, iterations + 1):\n        eta = 1.0 / (lambda_val * t)\n\n        # f = K @ (alpha * y) + b\n        y_alpha = alpha * y  # (n,)\n        f = K.matmul(y_alpha) + b  # (n,)\n\n        # Violations: y_i * f_i < 1\n        margin = y * f\n        M = margin < 1.0\n\n        # Shrink alphas\n        alpha = (1.0 - eta * lambda_val) * alpha\n\n        # Add contributions for violating samples\n        if torch.any(M):\n            alpha = alpha.clone()\n            alpha[M] += eta / n\n            # Bias update: b <- b + eta * (1/n) * sum_{i in M} y_i\n            b = b + eta * (y[M].sum() / n)\n\n        # Numerical safety: keep alphas non-negative\n        alpha = torch.clamp(alpha, min=0.0)\n\n    return alpha.detach().cpu().tolist(), float(b.detach().cpu().item())\n\n\ndef predict_kernel_svm(\n    train_data, \n    train_labels, \n    alphas: List[float], \n    b: float, \n    test_data,\n    kernel: str = 'linear',\n    sigma: float = 1.0,\n    dtype: torch.dtype = torch.float64,\n    device: torch.device = torch.device('cpu')\n):\n    \"\"\"\n    Predict labels for test_data using the trained kernel SVM parameters (alphas, b).\n    Returns decision values and predicted labels in {-1, +1}.\n    \"\"\"\n    X_tr = torch.as_tensor(train_data, dtype=dtype, device=device)\n    y_tr = torch.as_tensor(train_labels, dtype=dtype, device=device)\n    alpha = torch.as_tensor(alphas, dtype=dtype, device=device)\n    X_te = torch.as_tensor(test_data, dtype=dtype, device=device)\n\n    if kernel == 'linear':\n        # f(x) = x^T w + b, with w = X_tr^T (alpha * y)\n        w = X_tr.T @ (alpha * y_tr)\n        f = X_te @ w + b\n    else:\n        # Compute K(X_te, X_tr)\n        X_tr_norm = (X_tr ** 2).sum(dim=1, keepdim=True)\n        X_te_norm = (X_te ** 2).sum(dim=1, keepdim=True)\n        dists = X_te_norm + X_tr_norm.T - 2.0 * (X_te @ X_tr.T)\n        gamma = 1.0 / (2.0 * (sigma ** 2))\n        K_te_tr = torch.exp(-gamma * dists)\n        f = K_te_tr @ (alpha * y_tr) + b\n\n    preds = torch.where(f >= 0, torch.tensor(1.0, dtype=dtype, device=device), torch.tensor(-1.0, dtype=dtype, device=device))\n    return f.detach().cpu().numpy(), preds.detach().cpu().numpy()\n\n\nif __name__ == \"__main__\":\n    import numpy as np\n\n    data = np.array([[1, 2], [2, 3], [3, 1], [4, 1]], dtype=float)\n    labels = np.array([1, 1, -1, -1], dtype=float)\n\n    alphas, b = pegasos_kernel_svm(\n        data=data,\n        labels=labels,\n        kernel='rbf',\n        lambda_val=0.01,\n        iterations=100,\n        sigma=1.0,\n    )\n    print(\"alphas:\", alphas)\n    print(\"b:\", b)\n\n    # Predict on training data (for demonstration)\n    scores, preds = predict_kernel_svm(\n        train_data=data,\n        train_labels=labels,\n        alphas=alphas,\n        b=b,\n        test_data=data,\n        kernel='rbf',\n        sigma=1.0,\n    )\n    print(\"scores:\", scores)\n    print(\"preds:\", preds)\n",
  "timeComplexity": "O(T * n^2 + n^2 * d) for T iterations (kernel matrix build is O(n^2 * d), each iteration is O(n^2))",
  "spaceComplexity": "O(n^2) due to the kernel matrix",
  "platform": "deepml"
};
