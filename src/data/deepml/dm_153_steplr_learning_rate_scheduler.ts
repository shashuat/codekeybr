import { Problem } from '../../types';

export const DM_153_STEPLR_LEARNING_RATE_SCHEDULER: Problem = {
  "id": "dm_153_step_lr_learning_rate_scheduler",
  "title": "StepLR Learning Rate Scheduler",
  "difficulty": "Easy",
  "tags": [
    "Optimization",
    "Gradient Descent"
  ],
  "descriptionMarkdown": "Implement a learning rate scheduler using the StepLR strategy.\n\nRequirements:\n- Create a class `StepLRScheduler` with:\n  - `__init__(self, initial_lr: float, step_size: int, gamma: float)`\n  - `get_lr(self, epoch: int) -> float`\n- The learning rate should decay by a factor of `gamma` every `step_size` epochs.\n- Return the learning rate rounded to 4 decimal places.\n\nExample:\n```\nscheduler = StepLRScheduler(initial_lr=0.1, step_size=5, gamma=0.5)\nprint(scheduler.get_lr(epoch=0))   # 0.1\nprint(scheduler.get_lr(epoch=4))   # 0.1\nprint(scheduler.get_lr(epoch=5))   # 0.05\nprint(scheduler.get_lr(epoch=9))   # 0.05\nprint(scheduler.get_lr(epoch=10))  # 0.025\n```\n\nReasoning:\n- The initial LR is 0.1 for epochs 0\u20134.\n- At epoch 5 (one step), LR becomes 0.1 \u00d7 0.5 = 0.05 and stays for epochs 5\u20139.\n- At epoch 10 (two steps), LR becomes 0.1 \u00d7 0.5\u00b2 = 0.025.",
  "solutionExplanation": "The StepLR policy keeps the learning rate constant within fixed-length intervals and decays it multiplicatively by a factor gamma at each boundary. For a given epoch t, the number of decays applied is floor(t / step_size). Thus, the learning rate is computed as lr_t = initial_lr * gamma^(floor(t / step_size)).\n\nThis schedule is piecewise constant and simple to reason about: it reduces the learning rate at regular intervals, which often stabilizes training after initial rapid progress. We implement this with a small class where get_lr(epoch) performs the computation and returns the value rounded to four decimal places, as specified.",
  "solutionCode": "import torch\nfrom typing import Union\n\nclass StepLRScheduler:\n    \"\"\"\n    A simple StepLR learning rate scheduler.\n\n    Every `step_size` epochs, the learning rate is multiplied by `gamma`.\n\n    lr(epoch) = initial_lr * gamma ** floor(epoch / step_size)\n    \"\"\"\n    def __init__(self, initial_lr: float, step_size: int, gamma: float) -> None:\n        if not isinstance(initial_lr, (int, float)):\n            raise TypeError(\"initial_lr must be a number\")\n        if not isinstance(step_size, int):\n            raise TypeError(\"step_size must be an integer\")\n        if not isinstance(gamma, (int, float)):\n            raise TypeError(\"gamma must be a number\")\n        if initial_lr <= 0:\n            raise ValueError(\"initial_lr must be positive\")\n        if step_size <= 0:\n            raise ValueError(\"step_size must be a positive integer\")\n        if gamma <= 0:\n            raise ValueError(\"gamma must be positive\")\n\n        self.initial_lr = float(initial_lr)\n        self.step_size = int(step_size)\n        self.gamma = float(gamma)\n\n    def get_lr(self, epoch: int) -> float:\n        \"\"\"Return the learning rate for the given epoch, rounded to 4 decimals.\"\"\"\n        if not isinstance(epoch, int):\n            raise TypeError(\"epoch must be an integer\")\n        if epoch < 0:\n            raise ValueError(\"epoch must be non-negative\")\n\n        # Number of completed steps: floor division\n        steps = epoch // self.step_size\n\n        # Use torch operations for numerical consistency and to adhere to PyTorch usage\n        lr0 = torch.tensor(self.initial_lr, dtype=torch.float64)\n        g = torch.tensor(self.gamma, dtype=torch.float64)\n        lr_t = lr0 * torch.pow(g, int(steps))\n\n        # Return as a Python float rounded to 4 decimals\n        return round(float(lr_t.item()), 4)\n\n\ndef solution():\n    # Example usage and simple verification\n    scheduler = StepLRScheduler(initial_lr=0.1, step_size=5, gamma=0.5)\n    examples = [0, 4, 5, 9, 10]\n    lrs = [scheduler.get_lr(e) for e in examples]\n\n    # Print example outputs\n    for e, lr in zip(examples, lrs):\n        print(f\"epoch={e}: lr={lr}\")\n\n    # Return values for potential automated checks\n    return lrs\n\nif __name__ == \"__main__\":\n    solution()\n",
  "timeComplexity": "O(1)",
  "spaceComplexity": "O(1)",
  "platform": "deepml"
};
