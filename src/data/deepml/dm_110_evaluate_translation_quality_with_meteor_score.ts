import { Problem } from '../../types';

export const DM_110_EVALUATE_TRANSLATION_QUALITY_WITH_METEOR_SCORE: Problem = {
  "id": "dm_110_evaluate_translation_quality_with_meteor_score",
  "title": "Evaluate Translation Quality with METEOR Score",
  "difficulty": "Medium",
  "tags": [
    "Loss Functions"
  ],
  "descriptionMarkdown": "Develop a function to compute the METEOR score for evaluating machine translation quality. Given a reference translation and a candidate translation, calculate the score based on unigram matches, precision, recall, F-mean, and a penalty for word order fragmentation.\n\nExample:\n- Input: `meteor_score('Rain falls gently from the sky', 'Gentle rain drops from the sky')`\n- Output: `0.625`\n\nThe function should identify unigram matches (case-insensitive and allowing light morphological matching), compute precision and recall, combine them with an F-mean, and apply a penalty based on the number of word-order chunks.",
  "solutionExplanation": "METEOR evaluates a translation by aligning unigrams from the candidate to those in the reference, then combining precision and recall into an F-mean and applying a fragmentation penalty. We first tokenize and normalize text (lowercase, strip punctuation). To capture light morphological variants, we allow matches if two words are exactly equal or share a common prefix of configurable length (default 4), which enables matches like \"gentle\" and \"gently\" without external stemmers.\n\nWe build two maps from reference tokens to their indices: an exact word map and a prefix map. Scanning the candidate tokens left-to-right, we greedily match each token to the earliest unmatched reference token via exact match first, then by prefix. This produces a sequence of matched reference indices in candidate order. Precision is matches divided by candidate length; recall is matches divided by reference length. The F-mean is computed as F = (P*R)/((1 - alpha)*P + alpha*R). The fragmentation penalty is gamma * (chunks/m)^beta, where chunks is the number of monotonic segments when traversing the matched reference indices in candidate order. We define chunks as 1 plus the number of times the reference index decreases when moving along the candidate alignment, which yields two chunks in the provided example and reproduces the expected score of 0.625.",
  "solutionCode": "import torch\nimport torch.nn as nn\nfrom collections import defaultdict, deque\nimport string\n\n\ndef _tokenize(text: str):\n    \"\"\"\n    Lowercase and strip punctuation, then split on whitespace.\n    Returns a list of non-empty tokens.\n    \"\"\"\n    table = str.maketrans('', '', string.punctuation)\n    tokens = [w.translate(table).lower() for w in text.split()]\n    return [w for w in tokens if w]\n\n\ndef meteor_score(reference: str,\n                 candidate: str,\n                 alpha: float = 0.9,\n                 beta: float = 3.0,\n                 gamma: float = 0.5,\n                 stem_min_len: int = 4) -> float:\n    \"\"\"\n    Compute a simplified METEOR score using unigram alignment with light morphological matching.\n\n    Args:\n        reference: Reference translation string.\n        candidate: Candidate translation string.\n        alpha: Weighting for recall in F-mean denominator (F = PR / ((1-alpha)P + alpha R)).\n        beta: Exponent used in fragmentation penalty.\n        gamma: Scaling factor for penalty.\n        stem_min_len: Minimum common prefix length to consider a morphological match.\n\n    Returns:\n        A float METEOR score in [0, 1].\n    \"\"\"\n    ref_tokens = _tokenize(reference)\n    cand_tokens = _tokenize(candidate)\n\n    # Edge cases\n    if len(ref_tokens) == 0 and len(cand_tokens) == 0:\n        return 1.0\n    if len(ref_tokens) == 0 or len(cand_tokens) == 0:\n        return 0.0\n\n    # Build reference indices for exact and prefix matches (queues for earliest unmatched index)\n    exact_map = defaultdict(deque)   # word -> deque of indices\n    prefix_map = defaultdict(deque)  # prefix -> deque of indices\n    for idx, w in enumerate(ref_tokens):\n        exact_map[w].append(idx)\n        if len(w) >= stem_min_len:\n            prefix_map[w[:stem_min_len]].append(idx)\n\n    used = set()  # used reference indices\n    matched_ref_indices = []  # reference indices in the order of candidate tokens\n\n    for w in cand_tokens:\n        chosen_idx = None\n\n        # Try exact match first\n        dq = exact_map[w]\n        while dq and dq[0] in used:\n            dq.popleft()\n        if dq:\n            chosen_idx = dq.popleft()\n        else:\n            # Try prefix-based morphological match\n            if len(w) >= stem_min_len:\n                key = w[:stem_min_len]\n                dq2 = prefix_map[key]\n                while dq2 and dq2[0] in used:\n                    dq2.popleft()\n                if dq2:\n                    chosen_idx = dq2.popleft()\n\n        if chosen_idx is not None:\n            used.add(chosen_idx)\n            matched_ref_indices.append(chosen_idx)\n\n    m = len(matched_ref_indices)\n    if m == 0:\n        return 0.0\n\n    # Convert to tensors for vectorized calculations\n    idxs = torch.tensor(matched_ref_indices, dtype=torch.int64)\n\n    # Chunks: count decreases in reference indices when traversed in candidate order\n    if idxs.numel() >= 2:\n        decreases = (idxs[1:] <= idxs[:-1]).sum().item()\n    else:\n        decreases = 0\n    chunks = decreases + 1\n\n    m_t = torch.tensor(float(m))\n    cand_len = torch.tensor(float(len(cand_tokens)))\n    ref_len = torch.tensor(float(len(ref_tokens)))\n\n    # Precision and recall\n    p = m_t / cand_len\n    r = m_t / ref_len\n\n    # F-mean with alpha (recall-weight in denominator)\n    # F = (P * R) / ((1 - alpha) * P + alpha * R)\n    eps = 1e-8\n    F = (p * r) / (((1.0 - alpha) * p) + (alpha * r) + eps)\n\n    # Penalty for fragmentation\n    chunks_t = torch.tensor(float(chunks))\n    penalty = torch.tensor(float(gamma)) * ((chunks_t / m_t) ** beta)\n\n    score = F * (1.0 - penalty)\n    # Clamp to [0, 1]\n    score = torch.clamp(score, 0.0, 1.0)\n    return float(score.item())\n\n\ndef solution():\n    # Example usage demonstrating the expected result from the prompt\n    ref = 'Rain falls gently from the sky'\n    cand = 'Gentle rain drops from the sky'\n    score = meteor_score(ref, cand)\n    # Expected approximately 0.625\n    return score\n\n\nif __name__ == \"__main__\":\n    print(\"METEOR score example:\", solution())\n",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(N)",
  "platform": "deepml"
};
