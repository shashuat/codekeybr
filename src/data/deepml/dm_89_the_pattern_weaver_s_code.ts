import { Problem } from '../../types';

export const DM_89_THE_PATTERN_WEAVER_S_CODE: Problem = {
  "id": "dm_89_the_pattern_weaver_s_code",
  "title": "The Pattern Weaver's Code",
  "difficulty": "Medium",
  "tags": [
    "Neural Networks",
    "Transformers",
    "Attention",
    "Matrix Operations",
    "Probability"
  ],
  "descriptionMarkdown": "Deep in the Crystal Cave, the enigmatic Pattern Weaver creates stunning sequences by uncovering the intricate relationships between crystals. Each crystal is marked by a unique numeric value, and its true power depends on how it interacts with all others.\n\nYou are given N crystals with numeric values and asked to implement a simplified self-attention mechanism. For each crystal i:\n- Compute its relationship score with every crystal j using dot-product attention: s_ij = (q_i \u00b7 k_j) / sqrt(d), where q_i = k_i = v_i = the crystal's value (with d = dimension).\n- Convert the scores over j to probabilities using the softmax function.\n- Produce the final weighted pattern as the sum over j of the attention weights times the values v_j.\n\nExample\n- Input: number of crystals: 5, values: [4, 2, 7, 1, 9], dimension: 1\n- Output: [8.9993, 8.9638, 9.0, 8.7259, 9.0]\n\nThe attention scores are computed for every pair, normalized via softmax per query, and then used to form a weighted sum of the values.",
  "solutionExplanation": "This task is a direct implementation of dot-product self-attention specialized to a single sequence. We use the input values both as queries (Q), keys (K), and values (V). For each pair (i, j), the compatibility score is computed as the scaled dot product s_ij = (Q_i \u00b7 K_j) / sqrt(d). The scaling by sqrt(d) stabilizes the softmax when the dimensionality grows. With dimension = 1, this is simply the product of the two scalars.\n\nNext, we normalize these scores across j for each i using the softmax function to obtain attention weights a_ij. These weights sum to 1 for each i and indicate how much crystal i attends to crystal j. Finally, the output for each crystal i is the weighted sum over all values V_j using these weights: out_i = sum_j a_ij V_j. Because we set Q = K = V = X (the inputs), this is a minimal self-attention mechanism that captures pairwise relationships.\n\nFor the provided example, the largest input value (9) dominates most attention distributions due to large dot-products with queries, so each output becomes a weighted average heavily biased toward 9, producing values near 9 after softmax normalization.",
  "solutionCode": "import math\nimport torch\nimport torch.nn.functional as F\n\ndef softmax_stable(x: torch.Tensor, dim: int = -1) -> torch.Tensor:\n    \"\"\"Numerically stable softmax using PyTorch.\"\"\"\n    return F.softmax(x, dim=dim)\n\n\ndef pattern_weaver(n: int, crystal_values, dimension: int = 1) -> torch.Tensor:\n    \"\"\"\n    Compute simplified self-attention over a sequence of crystals.\n\n    Args:\n        n: Number of crystals (sequence length).\n        crystal_values: Iterable of numbers. If dimension == 1, length should be n.\n                        If dimension > 1, length should be n * dimension (row-major),\n                        which will be reshaped to (n, dimension).\n        dimension: Feature dimension d used for scaled dot-product attention.\n\n    Returns:\n        A tensor of shape (n,) if dimension == 1, else (n, dimension), containing\n        the attention-weighted outputs for each crystal.\n    \"\"\"\n    x = torch.tensor(crystal_values, dtype=torch.float32)\n\n    # Reshape to (n, d)\n    expected = n * dimension\n    if x.numel() != expected:\n        raise ValueError(f\"Input size mismatch: got {x.numel()} values, expected {expected} for n={n}, d={dimension}.\")\n    x = x.view(n, dimension)\n\n    # Self-attention with Q = K = V = x\n    Q, K, V = x, x, x\n\n    # Scaled dot-product attention: scores shape (n, n)\n    scale = 1.0 / math.sqrt(dimension)\n    scores = (Q @ K.T) * scale\n\n    # Attention weights over keys for each query\n    attn = softmax_stable(scores, dim=-1)\n\n    # Weighted sum of values: output shape (n, d)\n    out = attn @ V\n\n    # If d == 1, return as 1D for convenience\n    if dimension == 1:\n        return out.squeeze(1)\n    return out\n\n\ndef solution():\n    # Example usage matching the prompt\n    n = 5\n    values = [4, 2, 7, 1, 9]\n    d = 1\n\n    out = pattern_weaver(n, values, d)\n\n    # Round to 4 decimals to mirror the example formatting\n    return [round(float(v), 4) for v in out]\n\n\nif __name__ == \"__main__\":\n    print(solution())\n",
  "timeComplexity": "O(N^2)",
  "spaceComplexity": "O(N^2)",
  "platform": "deepml"
};
