import { Problem } from '../../types';

export const DM_9_MATRIX_TIMES_MATRIX: Problem = {
  "id": "dm_9_matrix_times_matrix",
  "title": "Matrix times Matrix",
  "difficulty": "Medium",
  "tags": [
    "Linear Algebra",
    "Matrix Operations"
  ],
  "descriptionMarkdown": "Multiply two matrices A and B and return their product C = A \u00b7 B. If the shapes do not align for matrix multiplication (i.e., the number of columns of A does not equal the number of rows of B), return -1.\n\nExample 1:\n- Input: A = [[1,2],[2,4]], B = [[2,1],[3,4]]\n- Output: [[8, 9],[16, 18]]\n- Reasoning: 1*2 + 2*3 = 8; 1*1 + 2*4 = 9; 2*2 + 4*3 = 16; 2*1 + 4*4 = 18\n\nExample 2:\n- Input: A = [[1,2], [2,4]], B = [[2,1], [3,4], [4,5]]\n- Output: -1\n- Reasoning: The number of columns in A (2) does not equal the number of rows in B (3), so multiplication is not defined.",
  "solutionExplanation": "To multiply two matrices A (m\u00d7k) and B (k\u00d7n), the inner dimensions must match (columns of A equal rows of B). The resulting matrix C has shape (m\u00d7n), where each element C[i][j] is the dot product of the i-th row of A and the j-th column of B.\n\nWe first validate inputs: ensure both A and B are rectangular 2D lists and check the compatibility condition a_cols == b_rows. If not satisfied, we return -1. For the multiplication itself, we leverage PyTorch's efficient tensor operation torch.matmul, which performs optimized matrix multiplication on CPU (and GPU if tensors are moved there). We convert the input lists to tensors, perform matmul, and then convert the result back to a standard Python list of lists for output. If both inputs are integer matrices, we preserve integer dtype in the output; otherwise, we compute in float64 for numeric stability.",
  "solutionCode": "import torch\nimport torch.nn as nn\nfrom typing import List, Union\n\nNumber = Union[int, float]\n\n\ndef _is_rectangular(mat: List[List[Number]]) -> bool:\n    if not isinstance(mat, (list, tuple)) or len(mat) == 0:\n        return False\n    if not all(isinstance(row, (list, tuple)) for row in mat):\n        return False\n    row_lens = [len(row) for row in mat]\n    if len(row_lens) == 0 or row_lens[0] == 0:\n        return False\n    return all(l == row_lens[0] for l in row_lens)\n\n\ndef _all_ints(mat: List[List[Number]]) -> bool:\n    try:\n        return all(isinstance(x, (int, bool)) for row in mat for x in row)\n    except TypeError:\n        return False\n\n\ndef matrixmul(a: List[List[Number]], b: List[List[Number]]):\n    \"\"\"\n    Multiply two 2D matrices a and b using PyTorch. If shapes are incompatible, return -1.\n\n    Args:\n        a: List of lists representing matrix A (shape m x k)\n        b: List of lists representing matrix B (shape k x n)\n\n    Returns:\n        - List of lists representing the product C = A @ B (shape m x n), or\n        - -1 if the matrices are not compatible for multiplication or inputs are invalid.\n    \"\"\"\n    # Validate rectangular structure\n    if not _is_rectangular(a) or not _is_rectangular(b):\n        return -1\n\n    a_rows, a_cols = len(a), len(a[0])\n    b_rows, b_cols = len(b), len(b[0])\n\n    # Shape compatibility check: (m x k) @ (k x n)\n    if a_cols != b_rows:\n        return -1\n\n    # Choose dtype: preserve integer outputs if both inputs are integer matrices\n    if _all_ints(a) and _all_ints(b):\n        A = torch.tensor(a, dtype=torch.int64)\n        B = torch.tensor(b, dtype=torch.int64)\n        C = torch.matmul(A, B)\n        return C.tolist()\n    else:\n        # Use float64 for numerical stability\n        A = torch.tensor(a, dtype=torch.float64)\n        B = torch.tensor(b, dtype=torch.float64)\n        C = torch.matmul(A, B)\n        return C.tolist()\n\n\ndef solution():\n    # Example 1\n    A1 = [[1, 2], [2, 4]]\n    B1 = [[2, 1], [3, 4]]\n    result1 = matrixmul(A1, B1)\n    print(result1)  # Expected: [[8, 9], [16, 18]]\n\n    # Example 2 (incompatible shapes)\n    A2 = [[1, 2], [2, 4]]\n    B2 = [[2, 1], [3, 4], [4, 5]]\n    result2 = matrixmul(A2, B2)\n    print(result2)  # Expected: -1\n",
  "timeComplexity": "O(m * k * n) for multiplying an (m\u00d7k) matrix by a (k\u00d7n) matrix",
  "spaceComplexity": "O(m * n) for the output matrix (additional transient tensor allocations are proportional to input and output sizes)",
  "platform": "deepml"
};
