import { Problem } from '../../types';

export const DM_139_ELASTIC_NET_REGRESSION_VIA_GRADIENT_DESCENT: Problem = {
  "id": "dm_139_elastic_net_regression_via_gradient_descent",
  "title": "Elastic Net Regression via Gradient Descent",
  "difficulty": "Medium",
  "tags": [
    "Optimization",
    "Regularization",
    "Gradient Descent",
    "Loss Functions",
    "Matrix Operations"
  ],
  "descriptionMarkdown": "Implement Elastic Net Regression using gradient descent. Elastic Net combines L1 (Lasso) and L2 (Ridge) penalties to handle multicollinearity while encouraging sparsity in feature weights. Learn weights and bias by minimizing the mean squared error (MSE) plus L1 and L2 regularization terms using gradient-based optimization.\n\nExample:\n- Input:\n  - X = [[0, 0], [1, 1], [2, 2]]\n  - y = [0, 1, 2]\n- Output (approx.):\n  - weights \u2248 [0.37, 0.37], bias \u2248 0.25\n\nThe model learns a nearly perfect linear relationship with regularization controlling weight magnitude. The weights converge around 0.37 with a bias around 0.25.",
  "solutionExplanation": "We optimize the Elastic Net objective:\n\nmin_{w,b} (1/n) \u03a3_i (y_i \u2212 x_i^T w \u2212 b)^2 + \u03b11 ||w||_1 + (\u03b12/2) ||w||_2^2\n\nThis couples the mean squared error with L1 (sparsity) and L2 (shrinkage) penalties on the weights. The bias term is typically not regularized. While the MSE and L2 terms are smooth, the L1 term is non-differentiable at zero; PyTorch handles this via subgradients during autograd, which is suitable for simple gradient descent.\n\nWe implement gradient descent with PyTorch tensors and autograd. At each iteration, we compute the loss (MSE + \u03b11 L1 + (\u03b12/2) L2), backpropagate to obtain gradients for the weights and bias, and update parameters with a fixed learning rate. We stop early when the change in loss falls below a tolerance. This approach is simple, uses efficient tensor operations, and avoids manually deriving subgradients for the L1 term.",
  "solutionCode": "import torch\nimport torch.nn as nn\nfrom typing import Tuple\n\n\ndef elastic_net_gradient_descent(\n    X,\n    y,\n    alpha1: float = 0.1,\n    alpha2: float = 0.1,\n    learning_rate: float = 0.1,\n    max_iter: int = 10000,\n    tol: float = 1e-8,\n) -> Tuple[torch.Tensor, float]:\n    \"\"\"\n    Elastic Net linear regression via gradient descent using PyTorch autograd.\n\n    Args:\n        X: Array-like (n_samples, n_features). Will be converted to torch.float32 tensor.\n        y: Array-like (n_samples,). Will be converted to torch.float32 tensor.\n        alpha1: L1 regularization strength (Lasso).\n        alpha2: L2 regularization strength (Ridge); used as 0.5 * alpha2 * ||w||^2.\n        learning_rate: Gradient descent step size.\n        max_iter: Maximum number of iterations.\n        tol: Absolute tolerance on loss improvement for early stopping.\n\n    Returns:\n        (w, b): where w is a 1D torch tensor of shape (n_features,), and b is a float bias.\n    \"\"\"\n    X = torch.as_tensor(X, dtype=torch.float32)\n    y = torch.as_tensor(y, dtype=torch.float32).view(-1)\n\n    n, d = X.shape\n\n    # Initialize parameters\n    w = torch.zeros(d, dtype=torch.float32, requires_grad=True)\n    b = torch.zeros(1, dtype=torch.float32, requires_grad=True)\n\n    prev_loss = float(\"inf\")\n\n    for _ in range(max_iter):\n        # Forward pass\n        y_hat = X.mv(w) + b  # (n,)\n        mse = torch.mean((y_hat - y) ** 2)\n        l1 = torch.abs(w).sum()\n        l2 = 0.5 * torch.sum(w ** 2)\n        loss = mse + alpha1 * l1 + alpha2 * l2\n\n        # Backward pass\n        loss.backward()\n\n        # Parameter update (manual GD)\n        with torch.no_grad():\n            w -= learning_rate * w.grad\n            b -= learning_rate * b.grad\n\n        # Zero gradients\n        w.grad.zero_()\n        b.grad.zero_()\n\n        # Early stopping\n        if abs(prev_loss - loss.item()) < tol:\n            break\n        prev_loss = loss.item()\n\n    return w.detach(), float(b.detach().item())\n\n\ndef solution():\n    # Example usage reproducing the prompt scenario\n    import numpy as np\n\n    X = np.array([[0.0, 0.0], [1.0, 1.0], [2.0, 2.0]], dtype=np.float32)\n    y = np.array([0.0, 1.0, 2.0], dtype=np.float32)\n\n    w, b = elastic_net_gradient_descent(\n        X=X,\n        y=y,\n        alpha1=0.1,\n        alpha2=0.1,\n        learning_rate=0.1,\n        max_iter=10000,\n        tol=1e-8,\n    )\n\n    # Return numpy-like presentation for easy comparison\n    return w.numpy(), b\n",
  "timeComplexity": "O(n * d * max_iter)",
  "spaceComplexity": "O(d)",
  "platform": "deepml"
};
