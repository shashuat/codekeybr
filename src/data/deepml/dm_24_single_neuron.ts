import { Problem } from '../../types';

export const DM_24_SINGLE_NEURON: Problem = {
  "id": "dm_24_single_neuron",
  "title": "Single Neuron",
  "difficulty": "Easy",
  "tags": [
    "Neural Networks",
    "Activation Functions",
    "Loss Functions",
    "Linear Algebra"
  ],
  "descriptionMarkdown": "Write a Python function that simulates a single neuron with a sigmoid activation function for binary classification, handling multidimensional input features. The function should take a list of feature vectors (each vector representing multiple features for an example), associated true binary labels, and the neuron's weights (one for each feature) and bias as input. It should return the predicted probabilities after sigmoid activation and the mean squared error (MSE) between the predicted probabilities and the true labels, both rounded to four decimal places.\n\nExample:\n- Input:\n  - features = [[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]]\n  - labels = [0, 1, 0]\n  - weights = [0.7, -0.4]\n  - bias = -0.1\n- Output: ([0.4626, 0.4134, 0.6682], 0.3349)\n\nFor each input vector, the weighted sum is calculated by multiplying each feature by its corresponding weight, adding these up along with the bias, then applying the sigmoid function to produce a probability. The MSE is calculated as the average squared difference between each predicted probability and the corresponding true label.",
  "solutionExplanation": "A single neuron with a sigmoid activation computes, for each example, a linear combination of the features and weights plus a bias (z = x \u00b7 w + b). The sigmoid activation \u03c3(z) = 1 / (1 + exp(\u2212z)) maps the real-valued logit to a probability in (0, 1). To evaluate how well these probabilities match the binary labels, we compute the mean squared error: MSE = (1/N) \u03a3 (p_i \u2212 y_i)^2.\n\nWe implement this efficiently using PyTorch tensors. Inputs are converted to tensors with shape (N, D) for features, (D,) for weights, and (N,) for labels. We then compute logits via a matrix-vector product, apply torch.sigmoid, and compute the MSE using tensor operations. Finally, we round the probabilities element-wise and the MSE to four decimal places and return them as standard Python types. Vectorization ensures the implementation is concise and efficient.",
  "solutionCode": "import torch\nimport torch.nn as nn\nfrom typing import List, Tuple\n\ndef single_neuron_model(features: List[List[float]],\n                        labels: List[int],\n                        weights: List[float],\n                        bias: float) -> Tuple[List[float], float]:\n    \"\"\"Simulate a single sigmoid neuron for binary classification.\n\n    Args:\n        features: List of N examples, each a list of D feature values.\n        labels: List of N binary labels (0 or 1).\n        weights: List of D weights corresponding to the D features.\n        bias: Scalar bias term.\n\n    Returns:\n        A tuple (probabilities, mse):\n        - probabilities: list of N predicted probabilities rounded to 4 decimals.\n        - mse: mean squared error (float) rounded to 4 decimals.\n    \"\"\"\n    # Convert inputs to torch tensors\n    X = torch.tensor(features, dtype=torch.float32)\n    y = torch.tensor(labels, dtype=torch.float32)\n    w = torch.tensor(weights, dtype=torch.float32)\n    b = torch.tensor(bias, dtype=torch.float32)\n\n    # Basic shape validation\n    if X.dim() != 2:\n        raise ValueError(\"features must be a 2D list with shape [N][D]\")\n    N, D = X.shape\n    if w.numel() != D:\n        raise ValueError(f\"weights length ({w.numel()}) must match feature dimension ({D})\")\n    if y.numel() != N:\n        raise ValueError(f\"labels length ({y.numel()}) must match number of examples ({N})\")\n\n    # Linear combination + bias: logits of shape (N,)\n    logits = X.matmul(w) + b\n\n    # Sigmoid activation to get probabilities\n    probs = torch.sigmoid(logits)\n\n    # Mean squared error between probabilities and labels\n    mse = torch.mean((probs - y) ** 2)\n\n    # Round to 4 decimals\n    probs_rounded = torch.round(probs * 10000.0) / 10000.0\n    mse_rounded = torch.round(mse * 10000.0) / 10000.0\n\n    return probs_rounded.tolist(), float(mse_rounded.item())\n\n\ndef solution():\n    # Example usage based on the problem statement\n    features = [[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]]\n    labels = [0, 1, 0]\n    weights = [0.7, -0.4]\n    bias = -0.1\n    return single_neuron_model(features, labels, weights, bias)\n\nif __name__ == \"__main__\":\n    print(solution())\n",
  "timeComplexity": "O(N*D)",
  "spaceComplexity": "O(N*D)",
  "platform": "deepml"
};
