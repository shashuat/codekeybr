import { Problem } from '../../types';

export const DM_17_K_MEANS_CLUSTERING: Problem = {
  "id": "dm_17_k_means_clustering",
  "title": "K-Means Clustering",
  "difficulty": "Medium",
  "tags": [
    "Optimization",
    "Matrix Operations",
    "Linear Algebra"
  ],
  "descriptionMarkdown": "Implement the k-Means clustering algorithm.\n\nInputs:\n- points: A list of points, where each point is a tuple of coordinates (e.g., (x, y) for 2D points)\n- k: An integer representing the number of clusters\n- initial_centroids: A list of initial centroid points, each a tuple of coordinates\n- max_iterations: An integer for the maximum number of iterations\n\nOutput:\n- A list of final centroids, each rounded to the nearest 4th decimal place.\n\nExample:\n- Input:\n  - points = [(1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)]\n  - k = 2\n  - initial_centroids = [(1, 1), (10, 1)]\n  - max_iterations = 10\n- Output: [(1, 2), (10, 2)]\n\nk-Means partitions n points into k clusters by iteratively assigning each point to the nearest centroid and then updating centroids to the mean of their assigned points. The process repeats until convergence or until the maximum number of iterations is reached.",
  "solutionExplanation": "k-Means alternates between two steps: assignment and update. In the assignment step, each point is assigned to the nearest centroid according to Euclidean distance. In the update step, each centroid is moved to the mean of all points assigned to it. These two steps are repeated until the centroids stop changing (convergence) or until a fixed number of iterations is reached.\n\nA fully vectorized implementation in PyTorch uses broadcasting to compute the pairwise distances between all points and centroids in O(N\u00b7K\u00b7D) per iteration, where N is the number of points, K is the number of clusters, and D is the dimensionality. We accumulate point sums and counts per cluster with index_add to update centroids efficiently. If a cluster receives no points in an iteration, we keep its previous centroid to avoid invalid divisions. Finally, we round the resulting centroids to 4 decimal places as required by the problem.",
  "solutionCode": "import torch\nfrom typing import List, Tuple\n\ndef k_means_clustering(points: List[Tuple[float, ...]],\n                        k: int,\n                        initial_centroids: List[Tuple[float, ...]],\n                        max_iterations: int) -> List[Tuple[float, ...]]:\n    \"\"\"\n    PyTorch implementation of k-Means clustering.\n\n    Args:\n        points: List of N points, each a tuple of D coordinates.\n        k: Number of clusters.\n        initial_centroids: List of K centroid tuples (same dimension as points).\n        max_iterations: Maximum number of iterations to run.\n\n    Returns:\n        List of K centroid tuples rounded to 4 decimal places.\n    \"\"\"\n    if len(points) == 0:\n        return []\n    # Convert inputs to tensors (use float64 for numerical stability)\n    X = torch.tensor(points, dtype=torch.float64)\n    C = torch.tensor(initial_centroids, dtype=torch.float64)\n\n    N, D = X.shape\n    if C.shape != (k, D):\n        raise ValueError(\"initial_centroids must have shape (k, D) matching points' dimensionality\")\n\n    # Iterate assignment and update steps\n    for _ in range(max_iterations):\n        # Compute squared Euclidean distances between each point and centroid: (N, K)\n        # dist[i, j] = ||X[i] - C[j]||^2\n        # Using broadcasting for vectorized computation\n        diff = X[:, None, :] - C[None, :, :]  # (N, K, D)\n        dists = (diff * diff).sum(dim=2)      # (N, K)\n\n        # Assignment step: pick nearest centroid for each point\n        labels = torch.argmin(dists, dim=1)  # (N,)\n\n        # Update step: compute new centroids as mean of assigned points\n        sums = torch.zeros((k, D), dtype=X.dtype)\n        counts = torch.zeros(k, dtype=X.dtype)\n        # Accumulate sums and counts per cluster\n        sums.index_add_(0, labels, X)\n        counts.index_add_(0, labels, torch.ones(N, dtype=X.dtype))\n\n        # Avoid division by zero for empty clusters: keep previous centroid\n        new_C = C.clone()\n        non_empty = counts > 0\n        new_C[non_empty] = sums[non_empty] / counts[non_empty].unsqueeze(1)\n\n        # Check for convergence\n        if torch.allclose(new_C, C, rtol=1e-6, atol=1e-8):\n            C = new_C\n            break\n        C = new_C\n\n    # Round to 4 decimal places\n    C = torch.round(C * 10000.0) / 10000.0\n\n    # Convert to list of tuples\n    final_centroids: List[Tuple[float, ...]] = [tuple(row.tolist()) for row in C]\n    return final_centroids\n\nif __name__ == \"__main__\":\n    points = [(1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)]\n    k = 2\n    initial_centroids = [(1, 1), (10, 1)]\n    max_iterations = 10\n    result = k_means_clustering(points, k, initial_centroids, max_iterations)\n    print(result)  # Expected: [(1.0, 2.0), (10.0, 2.0)]",
  "timeComplexity": "O(T * N * K * D) per run, where T is iterations, N points, K clusters, D dimensions",
  "spaceComplexity": "O(N * D + K * D)",
  "platform": "deepml"
};
