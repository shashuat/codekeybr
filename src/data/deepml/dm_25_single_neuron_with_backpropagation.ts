import { Problem } from '../../types';

export const DM_25_SINGLE_NEURON_WITH_BACKPROPAGATION: Problem = {
  "id": "dm_25_single_neuron_with_backpropagation",
  "title": "Single Neuron with Backpropagation",
  "difficulty": "Medium",
  "tags": [
    "Neural Networks",
    "Activation Functions",
    "Loss Functions",
    "Backpropagation",
    "Gradient Descent",
    "Optimization"
  ],
  "descriptionMarkdown": "Implement a single neuron (logistic unit) with a sigmoid activation and train it using backpropagation with mean squared error (MSE) loss. Write a function that takes:\n\n- features: list of feature vectors (list[list[float]])\n- labels: list of true binary labels (list[int] or list[float])\n- initial_weights: initial weight vector (list[float])\n- initial_bias: initial bias (float)\n- learning_rate: learning rate for gradient descent (float)\n- epochs: number of passes over the data (int)\n\nThe function should:\n- Perform forward passes through a single neuron: y_pred = sigmoid(Xw + b)\n- Compute MSE loss over the dataset\n- Backpropagate gradients and update weights and bias using gradient descent\n- Return the updated weights, updated bias, and a list of MSE values per epoch\n- Round all returned numeric values to four decimal places\n\nExample:\n- Input: features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]], labels = [1, 0, 0], initial_weights = [0.1, -0.2], initial_bias = 0.0, learning_rate = 0.1, epochs = 2\n- Output: updated_weights = [0.1036, -0.1425], updated_bias = -0.0167, mse_values = [0.3033, 0.2942]",
  "solutionExplanation": "We model a single neuron that computes z = Xw + b and outputs y_hat = sigmoid(z). Given N samples with D features, X is an N\u00d7D matrix, w is a D-dimensional vector, and b is a scalar. We use the mean squared error (MSE) loss: L = (1/N) * sum_i (y_hat_i \u2212 y_i)^2. The sigmoid activation is \u03c3(t) = 1 / (1 + e^(\u2212t)).\n\nTo train, we perform gradient descent. The gradients of the loss with respect to the parameters are obtained via backpropagation (autograd in PyTorch): \u2202L/\u2202w and \u2202L/\u2202b are computed by chaining the derivative of MSE with the derivative of the sigmoid. We then update parameters as w \u2190 w \u2212 \u03b7 \u2202L/\u2202w and b \u2190 b \u2212 \u03b7 \u2202L/\u2202b, where \u03b7 is the learning rate. We repeat this for a specified number of epochs, logging the MSE each epoch.\n\nUsing PyTorch ensures numerically stable sigmoid and automatic differentiation. We keep the implementation fully vectorized (X @ w + b) for efficiency and round results to four decimals as required.",
  "solutionCode": "import torch\nfrom typing import List, Tuple\n\n\ndef train_neuron(\n    features: List[List[float]],\n    labels: List[float],\n    initial_weights: List[float],\n    initial_bias: float,\n    learning_rate: float,\n    epochs: int,\n) -> Tuple[List[float], float, List[float]]:\n    \"\"\"\n    Train a single neuron with sigmoid activation using MSE loss and gradient descent.\n\n    Args:\n        features: List of feature vectors (N x D).\n        labels: List of binary labels (length N), values in {0, 1}.\n        initial_weights: Initial weights (length D).\n        initial_bias: Initial bias scalar.\n        learning_rate: Learning rate for gradient descent.\n        epochs: Number of training epochs.\n\n    Returns:\n        updated_weights: List of weights rounded to 4 decimals.\n        updated_bias: Bias rounded to 4 decimals.\n        mse_values: List of epoch-wise MSE values rounded to 4 decimals.\n    \"\"\"\n    # Convert inputs to tensors\n    X = torch.tensor(features, dtype=torch.float32)  # (N, D)\n    y = torch.tensor(labels, dtype=torch.float32)    # (N,)\n\n    # Initialize parameters with gradients\n    w = torch.tensor(initial_weights, dtype=torch.float32, requires_grad=True)  # (D,)\n    b = torch.tensor(initial_bias, dtype=torch.float32, requires_grad=True)      # ()\n\n    mse_values: List[float] = []\n\n    for _ in range(epochs):\n        # Forward pass: logits and sigmoid activation\n        logits = X.matmul(w) + b  # (N,)\n        y_pred = torch.sigmoid(logits)  # (N,)\n\n        # MSE loss over the batch\n        loss = torch.mean((y_pred - y) ** 2)\n\n        # Backpropagate\n        loss.backward()\n\n        # Gradient descent update\n        with torch.no_grad():\n            w -= learning_rate * w.grad\n            b -= learning_rate * b.grad\n\n        # Zero gradients for next iteration\n        w.grad.zero_()\n        b.grad.zero_()\n\n        # Log MSE (rounded to 4 decimals)\n        mse_values.append(round(float(loss.item()), 4))\n\n    updated_weights = [round(float(val), 4) for val in w.detach().tolist()]\n    updated_bias = round(float(b.detach().item()), 4)\n\n    return updated_weights, updated_bias, mse_values\n\n\n# Example usage\nif __name__ == \"__main__\":\n    features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]\n    labels = [1, 0, 0]\n    initial_weights = [0.1, -0.2]\n    initial_bias = 0.0\n    learning_rate = 0.1\n    epochs = 2\n\n    w_out, b_out, mse_out = train_neuron(\n        features, labels, initial_weights, initial_bias, learning_rate, epochs\n    )\n    print(\"updated_weights =\", w_out)\n    print(\"updated_bias =\", b_out)\n    print(\"mse_values =\", mse_out)\n",
  "timeComplexity": "O(E * N * D)",
  "spaceComplexity": "O(D)",
  "platform": "deepml"
};
