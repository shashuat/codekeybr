import { Problem } from '../../types';

export const DM_166_EVALUATE_EXPECTED_VALUE_IN_A_MARKOV_DECISION_PROCESS: Problem = {
  "id": "dm_166_evaluate_expected_value_in_a_markov_decision_process",
  "title": "Evaluate Expected Value in a Markov Decision Process",
  "difficulty": "Medium",
  "tags": [
    "Probability"
  ],
  "descriptionMarkdown": "Given an MDP (Markov Decision Process) specified by a set of states, actions, transition probabilities P, and rewards R, write a function to compute the expected value of taking a particular action in a particular state, assuming a discount factor gamma. The expected action value is the one-step lookahead using the current value function V.\n\nFormally, for state s and action a:\nQ(s, a) = sum_{s'} P(s' | s, a) [ R(s, a, s') + gamma * V(s') ]\n\nExample:\n\n- Input:\n  - states = [0, 1]\n  - actions = ['a', 'b']\n  - P = {0: {'a': {0: 0.5, 1: 0.5}, 'b': {0: 1.0}}, 1: {'a': {1: 1.0}, 'b': {0: 0.7, 1: 0.3}}}\n  - R = {0: {'a': {0: 5, 1: 10}, 'b': {0: 2}}, 1: {'a': {1: 0}, 'b': {0: -1, 1: 3}}}\n  - gamma = 0.9\n  - V = [1.0, 2.0]\n\nFor state 0 and action 'a':\n- Next state 0: 0.5 * (5 + 0.9*1.0) = 2.95\n- Next state 1: 0.5 * (10 + 0.9*2.0) = 5.9\nTotal: 2.95 + 5.9 = 8.85",
  "solutionExplanation": "The expected action value Q(s, a) for a given state-action pair is the expectation over all possible next states of the immediate reward plus the discounted value of the next state. Mathematically, Q(s, a) = sum_{s'} P(s'|s, a) [R(s, a, s') + gamma * V(s')]. The transition probabilities P(s'|s,a) and rewards R(s,a,s') are provided as nested dictionaries, while V is the current value function over states.\n\nTo compute Q(s, a), we gather the set of reachable next states s' from the provided transition dictionary P[s][a], extract the corresponding probabilities and rewards, and look up the next-state values V(s'). We then compute the dot product between the probability vector and the vector of (reward + gamma * V(s')) terms. This yields the expected action value efficiently using tensor operations. If some reward entries are missing for a reachable next state, they are treated as zero by default.",
  "solutionCode": "import torch\nimport torch.nn as nn\nfrom typing import Any, Dict\n\n\ndef expected_action_value(state: Any, action: Any, P: Dict, R: Dict, V: torch.Tensor, gamma: float) -> float:\n    \"\"\"\n    Compute the expected value Q(s, a) for a single state-action pair using:\n        Q(s, a) = sum_{s'} P(s'|s,a) * [ R(s,a,s') + gamma * V[s'] ]\n\n    Args:\n        state: Hashable state identifier (int/str/etc.)\n        action: Hashable action identifier (str/int/etc.)\n        P: dict of dicts; P[s][a][s'] = probability of transitioning to s'\n        R: dict of dicts; R[s][a][s'] = reward for (s, a, s')\n        V: torch.Tensor of shape [num_states], V[s] is the value of state s (state indices must be integers)\n        gamma: float, discount factor\n\n    Returns:\n        float: The expected action value Q(s, a).\n    \"\"\"\n    if state not in P or action not in P[state]:\n        raise ValueError(f\"No transitions defined for state={state}, action={action}.\")\n\n    trans = P[state][action]  # dict: next_state -> prob\n    if len(trans) == 0:\n        return 0.0\n\n    # Rewards may be missing for some (s, a, s') entries; default to 0.0\n    reward_map = R.get(state, {}).get(action, {})\n\n    next_states = list(trans.keys())\n    probs = torch.tensor([float(trans[s_next]) for s_next in next_states], dtype=torch.float32)\n    rewards = torch.tensor([float(reward_map.get(s_next, 0.0)) for s_next in next_states], dtype=torch.float32)\n\n    # Assume state indices are integers for indexing V\n    next_indices = torch.tensor(next_states, dtype=torch.long)\n    v_next = V[next_indices].to(dtype=torch.float32)\n\n    q = torch.dot(probs, rewards + gamma * v_next)\n    return float(q.item())\n\n\ndef solution():\n    # Example usage from the prompt\n    states = [0, 1]\n    actions = ['a', 'b']\n    P = {\n        0: {'a': {0: 0.5, 1: 0.5}, 'b': {0: 1.0}},\n        1: {'a': {1: 1.0}, 'b': {0: 0.7, 1: 0.3}}\n    }\n    R = {\n        0: {'a': {0: 5, 1: 10}, 'b': {0: 2}},\n        1: {'a': {1: 0}, 'b': {0: -1, 1: 3}}\n    }\n    gamma = 0.9\n    V = torch.tensor([1.0, 2.0], dtype=torch.float32)\n\n    val = expected_action_value(0, 'a', P, R, V, gamma)\n    print(val)  # Expected: 8.85\n\n\nif __name__ == \"__main__\":\n    solution()\n",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(N)",
  "platform": "deepml"
};
