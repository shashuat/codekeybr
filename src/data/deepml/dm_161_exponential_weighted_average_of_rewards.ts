import { Problem } from '../../types';

export const DM_161_EXPONENTIAL_WEIGHTED_AVERAGE_OF_REWARDS: Problem = {
  "id": "dm_161_exponential_weighted_average_of_rewards",
  "title": "Exponential Weighted Average of Rewards",
  "difficulty": "Medium",
  "tags": [
    "Probability",
    "Optimization"
  ],
  "descriptionMarkdown": "Given an initial estimate Q1, a list of k observed rewards R1, R2, ..., Rk, and a step size \u03b1 (0 < \u03b1 \u2264 1), compute the exponential recency-weighted average directly from the closed-form formula (no incremental updates):\n\nQ_k = (1 \u2212 \u03b1)^k \u00b7 Q1 + \u03a3_{i=1..k} [ \u03b1 \u00b7 (1 \u2212 \u03b1)^{k\u2212i} \u00b7 R_i ]\n\nThis formulation emphasizes recent rewards more heavily while the influence of the initial estimate Q1 decays exponentially over time.\n\nExample:\n- Q1 = 2.0, rewards = [5.0, 9.0], \u03b1 = 0.3\n- Q2 = (1\u22120.3)^2\u00b72.0 + 0.3\u00b7(1\u22120.3)^1\u00b75.0 + 0.3\u00b7(1\u22120.3)^0\u00b79.0 = 4.73",
  "solutionExplanation": "The exponential recency-weighted average can be derived from the recursive update Q \u2190 Q + \u03b1(R \u2212 Q). Unrolling this recursion yields a closed-form expression that is a weighted sum of the initial estimate and all observed rewards, where weights decay geometrically by powers of (1 \u2212 \u03b1). Specifically, the initial estimate Q1 receives weight (1 \u2212 \u03b1)^k after k observations, and the i-th reward receives weight \u03b1(1 \u2212 \u03b1)^{k\u2212i}.\n\nTo compute this efficiently and without incremental updates, we vectorize the calculation: construct a vector of exponents [k\u22121, k\u22122, ..., 0], compute the geometric decay (1\u2212\u03b1) raised to these exponents, scale by \u03b1 to get reward weights, and then take the dot product with the rewards. Finally, add the initial term (1\u2212\u03b1)^k \u00b7 Q1. Using PyTorch tensors allows concise, fast, and numerically stable computation.",
  "solutionCode": "import torch\\nimport torch.nn as nn\\n\\n# Exponential Recency-Weighted Average computed from the closed-form expression\\ndef exp_weighted_average(Q1, rewards, alpha):\\n    \\\"\\\"\\\"\\n    Compute Q_k = (1 - alpha)^k * Q1 + sum_{i=1..k} alpha * (1 - alpha)^{k-i} * R_i\\n\\n    Args:\\n        Q1 (float or Tensor): Initial estimate.\\n        rewards (Sequence[float] or Tensor): Rewards R_1 ... R_k.\\n        alpha (float): Step size in (0, 1].\\n\\n    Returns:\\n        float: Exponentially weighted average after k rewards.\\n    \\\"\\\"\\\"\\n    if not (0.0 < float(alpha) <= 1.0):\\n        raise ValueError(\\\"alpha must be in (0, 1]\\\")\\n\\n    # Ensure tensors with a consistent floating dtype\\n    r = torch.as_tensor(rewards, dtype=torch.get_default_dtype())\\n    q1 = torch.as_tensor(Q1, dtype=r.dtype)\\n\\n    k = r.numel()\\n    if k == 0:\\n        return float(q1.item())\\n\\n    decay = 1.0 - float(alpha)\\n\\n    # Exponents: k-1, k-2, ..., 0\\n    exponents = torch.arange(k - 1, -1, -1, dtype=r.dtype, device=r.device)\\n    decay_pows = decay ** exponents  # tensor of (1-alpha)^{k-i}\\n\\n    reward_weights = float(alpha) * decay_pows\\n    reward_term = torch.dot(r, reward_weights)\\n\\n    initial_term = (decay ** k) * q1\\n    result = initial_term + reward_term\\n\\n    return float(result.item())\\n\\n\\ndef solution():\\n    # Example usage\\n    Q1 = 2.0\\n    rewards = [5.0, 9.0]\\n    alpha = 0.3\\n    result = exp_weighted_average(Q1, rewards, alpha)\\n    print(round(result, 4))  # Expected: 4.73\\n    return result\\n\\nif __name__ == \\\"__main__\\\":\\n    solution()\\n",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(N)",
  "platform": "deepml"
};
