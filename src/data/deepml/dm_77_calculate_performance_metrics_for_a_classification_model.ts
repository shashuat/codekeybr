import { Problem } from '../../types';

export const DM_77_CALCULATE_PERFORMANCE_METRICS_FOR_A_CLASSIFICATION_MODEL: Problem = {
  "id": "dm_77_calculate_performance_metrics_for_a_classification_model",
  "title": "Calculate Performance Metrics for a Classification Model",
  "difficulty": "Medium",
  "tags": [
    "Probability"
  ],
  "descriptionMarkdown": "Implement a function performance_metrics(actual, predicted) that computes performance metrics for a binary classification problem. The labels use 1 for positive and 0 for negative.\n\nThe function must return:\n- Confusion Matrix: a 2x2 matrix formatted as [[TP, FN], [FP, TN]]\n- Accuracy\n- F1 Score\n- Specificity\n- Negative Predictive Value (NPV)\n\nConstraints:\n- actual and predicted are lists of equal length\n- All elements are either 0 or 1\n\nExample:\n\nInput:\n- actual = [1, 0, 1, 0, 1]\n- predicted = [1, 0, 0, 1, 1]\n\nOutput:\n- ([[2, 1], [1, 1]], 0.6, 0.667, 0.5, 0.5)\n\nAll metric values should be rounded to three decimal places.",
  "solutionExplanation": "We compute the confusion matrix by counting the occurrences of true positives (TP), false negatives (FN), false positives (FP), and true negatives (TN) using vectorized boolean operations. With actual and predicted labels encoded as 0/1, these counts are obtained by masking and summing. The confusion matrix is returned in the order [[TP, FN], [FP, TN]] to match the example.\n\nFrom the confusion matrix we derive the metrics:\n- Accuracy = (TP + TN) / (TP + TN + FP + FN)\n- Precision = TP / (TP + FP) (guards for zero denominator)\n- Recall = TP / (TP + FN)\n- F1 = 2 * precision * recall / (precision + recall) (0 when precision + recall is 0)\n- Specificity = TN / (TN + FP)\n- Negative Predictive Value (NPV) = TN / (TN + FN)\n\nWe handle edge cases by returning 0.0 for any metric whose denominator is zero, and we round the final metric values to three decimal places as required. The implementation uses PyTorch tensors and vectorized operations for efficiency.",
  "solutionCode": "import torch\n\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"Compute confusion matrix, accuracy, F1 score, specificity, and NPV for binary classification.\n\n    Returns a tuple: (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\n    where confusion_matrix = [[TP, FN],[FP, TN]]. All metrics are rounded to 3 decimals.\n    \"\"\"\n    if len(actual) != len(predicted):\n        raise ValueError(\"actual and predicted must have the same length\")\n\n    # Convert to torch tensors\n    a = torch.as_tensor(actual, dtype=torch.int64)\n    p = torch.as_tensor(predicted, dtype=torch.int64)\n\n    # Validate values are binary (0 or 1)\n    if not torch.all((a == 0) | (a == 1)):\n        raise ValueError(\"All elements in 'actual' must be 0 or 1\")\n    if not torch.all((p == 0) | (p == 1)):\n        raise ValueError(\"All elements in 'predicted' must be 0 or 1\")\n\n    # Compute confusion matrix components\n    tp = int(torch.sum((a == 1) & (p == 1)).item())\n    fn = int(torch.sum((a == 1) & (p == 0)).item())\n    fp = int(torch.sum((a == 0) & (p == 1)).item())\n    tn = int(torch.sum((a == 0) & (p == 0)).item())\n\n    confusion_matrix = [[tp, fn], [fp, tn]]\n\n    total = tp + fn + fp + tn\n    accuracy = (tp + tn) / total if total > 0 else 0.0\n\n    # Precision, Recall and F1\n    prec_den = tp + fp\n    rec_den = tp + fn\n    precision = (tp / prec_den) if prec_den > 0 else 0.0\n    recall = (tp / rec_den) if rec_den > 0 else 0.0\n\n    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n\n    # Specificity and NPV\n    spec_den = tn + fp\n    npv_den = tn + fn\n    specificity = (tn / spec_den) if spec_den > 0 else 0.0\n    negative_predictive_value = (tn / npv_den) if npv_den > 0 else 0.0\n\n    return (\n        confusion_matrix,\n        round(accuracy, 3),\n        round(f1, 3),\n        round(specificity, 3),\n        round(negative_predictive_value, 3),\n    )\n\n# Example usage\nif __name__ == \"__main__\":\n    actual = [1, 0, 1, 0, 1]\n    predicted = [1, 0, 0, 1, 1]\n    print(performance_metrics(actual, predicted))\n",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(1)",
  "platform": "deepml"
};
