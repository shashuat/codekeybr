import { Problem } from '../../types';

export const DM_142_GRIDWORLD_POLICY_EVALUATION: Problem = {
  "id": "dm_142_gridworld_policy_evaluation",
  "title": "Gridworld Policy Evaluation",
  "difficulty": "Medium",
  "tags": [
    "Probability",
    "Matrix Operations"
  ],
  "descriptionMarkdown": "Implement policy evaluation for a 5x5 gridworld. Given a policy (mapping each state to action probabilities), compute the state-value function V(s) for each cell using the Bellman expectation equation. The agent can move up, down, left, or right, receiving a constant reward of -1 for each move. Terminal states (the four corners) are fixed at 0. Iterate until the largest change in V is less than a given threshold. Only use Python built-ins and no external RL libraries.\n\nExample:\n\nInput:\n\npolicy = {(i, j): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25} for i in range(5) for j in range(5)}\n\ngamma = 0.9\n\nthreshold = 0.001\n\nV = gridworld_policy_evaluation(policy, gamma, threshold)\n\nprint(round(V[2][2], 4))\n\nOutput:\n\n-7.0902\n\nReasoning:\n\nThe policy is uniform (equal chance of each move). The agent receives -1 per step. After iterative updates, the center state value converges to about -7.09, and corners remain at 0.",
  "solutionExplanation": "We evaluate the state-value function under a fixed policy using the Bellman expectation equation: V(s) = sum_a pi(a|s)[r + gamma * V(s')]. In this 5x5 gridworld, the reward is a constant -1 for each move, the discount factor is gamma, and the terminal states are the four corners, which are fixed at value 0. We update each non-terminal state's value by taking the expected return over actions according to the policy and the next state's value. If an action would move the agent off the grid, the agent stays in the same cell.\n\nWe perform iterative policy evaluation: starting with V initialized to zeros, we repeatedly perform a full sweep over the grid to compute a new value function using the current V, and stop when the maximum absolute change across all states is below the given threshold. This procedure converges because it is a contraction mapping under 0 <= gamma < 1. The resulting values match the expected solution (e.g., approximately -7.0902 at the center for the uniform random policy).",
  "solutionCode": "import torch\n\ndef gridworld_policy_evaluation(policy: dict, gamma: float, threshold: float) -> list:\n    \"\"\"\n    Evaluate the state-value function V(s) for a given policy on a 5x5 gridworld using\n    iterative policy evaluation (Bellman expectation updates).\n\n    Args:\n        policy: dict mapping (row, col) -> {action: probability}, actions in {'up','down','left','right'}\n        gamma: discount factor (0 <= gamma < 1)\n        threshold: convergence threshold on max value change per iteration\n\n    Returns:\n        A 5x5 nested Python list of floats representing V(s) for each cell.\n    \"\"\"\n    n = 5\n    dtype = torch.float64\n    V = torch.zeros((n, n), dtype=dtype)\n\n    # Fixed terminal states (corners)\n    terminals = {(0, 0), (0, n - 1), (n - 1, 0), (n - 1, n - 1)}\n\n    # Action to delta mapping\n    action_delta = {\n        'up': (-1, 0),\n        'down': (1, 0),\n        'left': (0, -1),\n        'right': (0, 1),\n    }\n\n    reward = -1.0\n\n    def next_state(i: int, j: int, action: str) -> tuple:\n        di, dj = action_delta[action]\n        ni, nj = i + di, j + dj\n        # If moving out of bounds, stay in place\n        if 0 <= ni < n and 0 <= nj < n:\n            return ni, nj\n        return i, j\n\n    while True:\n        new_V = V.clone()\n        # Update all non-terminal states\n        for i in range(n):\n            for j in range(n):\n                if (i, j) in terminals:\n                    new_V[i, j] = 0.0\n                    continue\n                val = 0.0\n                act_probs = policy.get((i, j), None)\n                if act_probs is None:\n                    raise ValueError(f\"Missing policy for state {(i, j)}\")\n                for a, p in act_probs.items():\n                    if p == 0.0:\n                        continue\n                    ni, nj = next_state(i, j, a)\n                    val += p * (reward + gamma * V[ni, nj].item())\n                new_V[i, j] = val\n        delta = torch.max(torch.abs(new_V - V)).item()\n        V = new_V\n        if delta < threshold:\n            break\n\n    return V.tolist()\n\nif __name__ == \"__main__\":\n    # Example usage\n    policy = {(i, j): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25} for i in range(5) for j in range(5)}\n    gamma = 0.9\n    threshold = 0.001\n    V = gridworld_policy_evaluation(policy, gamma, threshold)\n    # Center state value (should be approximately -7.0902)\n    print(round(V[2][2], 4))\n",
  "timeComplexity": "O(S * A * I), where S=25 states, A=4 actions, and I is the number of iterations until convergence",
  "spaceComplexity": "O(S) for storing the value function over all states",
  "platform": "deepml"
};
