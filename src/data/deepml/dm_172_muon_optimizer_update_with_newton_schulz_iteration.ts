import { Problem } from '../../types';

export const DM_172_MUON_OPTIMIZER_UPDATE_WITH_NEWTON_SCHULZ_ITERATION: Problem = {
  "id": "dm_172_muon_optimizer_update_with_newton_schulz_iteration",
  "title": "Muon Optimizer Update with Newton-Schulz Iteration",
  "difficulty": "Medium",
  "tags": [
    "Optimization",
    "Matrix Operations",
    "Linear Algebra",
    "Gradient Descent"
  ],
  "descriptionMarkdown": "Implement a single Muon optimizer update. Given a parameter matrix \u03b8 (theta), its gradient G, a momentum buffer B_prev, momentum coefficient \u03bc (mu), learning rate lr, and a fixed number of Newton\u2013Schulz steps, perform one update step as follows:\n\n- Update the momentum-like preconditioner B using the latest gradient: B_new = \u03bc \u00b7 B_prev + G. (For the first step with B_prev = 0, B_new = G.)\n- Compute a preconditioned update O by applying a 5-step (quintic) Newton\u2013Schulz iteration to B_new for matrix preconditioning. No explicit scaling factor is required for this task.\n- Update parameters: \u03b8_new = \u03b8 \u2212 lr \u00b7 O.\n\nReturn \u03b8_new, B_new, and O.\n\nExample:\n\n- Input:\n  - theta = ones(3\u00d73)\n  - grad = reshape(arange(1,10), (3,3))\n  - B_prev = zeros(3\u00d73)\n  - mu = 0.9, lr = 0.01\n- Output:\n  - B_new equals grad (since B_prev = 0)\n  - O is the result of the 5-step Newton\u2013Schulz iteration applied to B_new\n  - \u03b8_new = \u03b8 \u2212 lr \u00b7 O",
  "solutionExplanation": "The Muon update in this task combines a momentum-like accumulator with a matrix preconditioning step based on the Newton\u2013Schulz iteration. First, we update the buffer B using a simple momentum rule B_new = \u03bc\u00b7B_prev + G so that the first update reduces to B_new = G when B_prev is zero. This buffer tracks recent gradients and stabilizes the direction of updates.\n\nFor the preconditioning, we use a 5-step (quintic) Newton\u2013Schulz iteration that progressively \"orthogonalizes\" or normalizes the matrix by iteratively reducing the deviation of X^T X from the identity. Starting from X_0 = G (or B_new), we set P = I \u2212 X^T X and multiply X by a polynomial in P. Using coefficients that yield a fifth-order method, the iteration is:\nX_{k+1} = X_k \u00b7 [I + 1/2 P + 3/8 P^2 + 5/16 P^3 + 35/128 P^4], with P = I \u2212 X_k^T X.\nAfter a fixed number of steps, the result O acts as a preconditioned direction derived from the current gradient structure. Finally, we update the parameters with a standard gradient descent step using this direction: \u03b8_new = \u03b8 \u2212 lr \u00b7 O.\n\nThis approach keeps the implementation purely in terms of matrix multiplications, is numerically stable for small step counts on typical problems, and does not require explicit scaling for this exercise.",
  "solutionCode": "import torch\nimport torch.nn as nn\n\ndef newtonschulz5(G: torch.Tensor, steps: int = 5, eps: float = 1e-7) -> torch.Tensor:\n    \"\"\"\n    Apply a 5-step (quintic) Newton\u2013Schulz iteration to matrix G for preconditioning.\n    This implements the fifth-order polynomial iteration often used for polar-style\n    normalization:\n        X_{k+1} = X_k * (I + 1/2 P + 3/8 P^2 + 5/16 P^3 + 35/128 P^4),\n    where P = I - X_k^T X_k.\n\n    Args:\n        G: 2D tensor of shape (m, n)\n        steps: number of iteration steps (default 5)\n        eps: small constant for numerical stability in degenerate cases\n\n    Returns:\n        Tensor of same shape as G representing the preconditioned matrix.\n    \"\"\"\n    if G.dim() != 2:\n        raise ValueError(\"G must be a 2D matrix tensor.\")\n\n    # Work in float64 for improved numerical stability, then cast back\n    dtype_orig = G.dtype\n    X = G.to(dtype=torch.float64).clone()\n    m, n = X.shape\n    I = torch.eye(n, dtype=X.dtype, device=X.device)\n\n    for _ in range(steps):\n        # P = I - X^T X\n        XT_X = X.T @ X\n        # Optional stabilization if rank deficient: add tiny eps to diagonal\n        if eps is not None and eps > 0:\n            XT_X = XT_X + eps * I\n        P = I - XT_X\n\n        # Compute polynomial: I + 1/2 P + 3/8 P^2 + 5/16 P^3 + 35/128 P^4\n        P2 = P @ P\n        P3 = P2 @ P\n        P4 = P3 @ P\n        C = I + 0.5 * P + 0.375 * P2 + 0.3125 * P3 + (35.0 / 128.0) * P4\n\n        # Update X\n        X = X @ C\n\n    return X.to(dtype=dtype_orig)\n\n\ndef muon_update(theta: torch.Tensor,\n                grad: torch.Tensor,\n                B_prev: torch.Tensor,\n                mu: float,\n                lr: float,\n                steps: int = 5,\n                eps: float = 1e-7) -> tuple:\n    \"\"\"\n    Perform one Muon optimizer update step.\n\n    Args:\n        theta: parameter matrix (m x n)\n        grad: gradient matrix (m x n)\n        B_prev: previous momentum/preconditioner buffer (m x n)\n        mu: momentum coefficient\n        lr: learning rate\n        steps: number of Newton\u2013Schulz iterations (default 5)\n        eps: small constant for stability in Newton\u2013Schulz\n\n    Returns:\n        (theta_new, B_new, O) where\n          - B_new is the updated buffer\n          - O is the Newton\u2013Schulz5-preconditioned matrix\n          - theta_new = theta - lr * O\n    \"\"\"\n    if not (theta.shape == grad.shape == B_prev.shape):\n        raise ValueError(\"theta, grad, and B_prev must have the same shape.\")\n\n    with torch.no_grad():\n        B_new = mu * B_prev + grad\n        O = newtonschulz5(B_new, steps=steps, eps=eps)\n        theta_new = theta - lr * O\n    return theta_new, B_new, O\n\n\n# Example usage\nif __name__ == \"__main__\":\n    torch.manual_seed(0)\n    theta = torch.ones((3, 3), dtype=torch.float32)\n    grad = torch.arange(1, 10, dtype=torch.float32).reshape(3, 3)\n    B_prev = torch.zeros((3, 3), dtype=torch.float32)\n    mu = 0.9\n    lr = 0.01\n\n    theta_new, B_new, O = muon_update(theta, grad, B_prev, mu, lr)\n\n    # Display rounded results\n    print(B_new.round(decimals=3))\n    print(O.round(decimals=4))\n",
  "timeComplexity": "O(steps * n^3) for n x n matrices (dominated by matrix multiplications)",
  "spaceComplexity": "O(n^2) for storing intermediate matrices",
  "platform": "deepml"
};
