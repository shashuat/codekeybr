import { Problem } from '../../types';

export const DM_84_PHI_TRANSFORMATION_FOR_POLYNOMIAL_FEATURES: Problem = {
  "id": "dm_84_phi_transformation_for_polynomial_features",
  "title": "Phi Transformation for Polynomial Features",
  "difficulty": "Easy",
  "tags": [
    "Linear Algebra",
    "Matrix Operations"
  ],
  "descriptionMarkdown": "Write a Python function to perform a Phi Transformation that maps input features into a higher-dimensional space by generating polynomial features. The transformation allows models like linear regression to fit nonlinear data by introducing new feature dimensions that represent polynomial combinations of the original input features.\n\nThe function should take a list of numerical data and a degree as inputs, and return a nested list where each inner list represents the transformed features of a data point. If the degree is less than 0, the function should return an empty list.\n\nExample:\n- Input: data = [1.0, 2.0], degree = 2\n- Output: [[1.0, 1.0, 1.0], [1.0, 2.0, 4.0]]\n\nReasoning: The Phi Transformation generates polynomial features for each data point up to the specified degree. For data = [1.0, 2.0] and degree = 2, the transformation creates a nested list where each row contains powers of the data point from 0 to 2.",
  "solutionExplanation": "The Phi transformation expands each scalar input x into a polynomial feature vector [x^0, x^1, ..., x^d]. This allows linear models to capture nonlinearity by operating in a higher-dimensional feature space where polynomial relationships can be modeled linearly. For example, with degree d=2, each input becomes [1, x, x^2].\n\nWe implement this efficiently using PyTorch tensor broadcasting. Given a list of values, we convert it to a 1D tensor of shape (N,) and create a powers tensor [0, 1, ..., d]. Using broadcasting, x.unsqueeze(-1) ** degrees.unsqueeze(0) yields an (N, d+1) tensor where each row contains the polynomial features for the corresponding input. Edge cases are handled by returning an empty list if the degree is negative or if the input data is empty.\n\nThis vectorized approach avoids Python loops, leverages fast tensor operations, and directly returns a nested Python list via .tolist() for compatibility with the required output format.",
  "solutionCode": "import torch\nimport torch.nn as nn\nfrom typing import List\n\ndef phi_transform(data: List[float], degree: int) -> List[List[float]]:\n    \"\"\"\n    Perform a Phi Transformation to generate polynomial features up to a given degree.\n\n    Args:\n        data (List[float]): A list of numerical values (scalars) to transform.\n        degree (int): The maximum polynomial degree. If < 0, returns an empty list.\n\n    Returns:\n        List[List[float]]: A nested list where each inner list contains [x^0, x^1, ..., x^degree]\n                            for the corresponding input x. If degree < 0 or data is empty,\n                            returns an empty list.\n    \"\"\"\n    # If degree is negative, return an empty list per specification\n    if degree < 0:\n        return []\n\n    # If no data, return empty list\n    if not data:\n        return []\n\n    # Convert input data to a torch tensor\n    x = torch.tensor(data, dtype=torch.float32)  # shape: (N,)\n\n    # Create degrees tensor [0, 1, 2, ..., degree]\n    degrees = torch.arange(0, degree + 1, dtype=torch.float32)  # shape: (degree+1,)\n\n    # Broadcast to compute all powers at once: result shape (N, degree+1)\n    features = x.unsqueeze(-1) ** degrees.unsqueeze(0)\n\n    # Convert to nested Python lists\n    return features.tolist()\n\n\ndef solution():\n    # Example usage\n    data = [1.0, 2.0]\n    degree = 2\n    transformed = phi_transform(data, degree)\n    print(transformed)  # Expected: [[1.0, 1.0, 1.0], [1.0, 2.0, 4.0]]\n    return transformed\n\nif __name__ == \"__main__\":\n    solution()\n",
  "timeComplexity": "O(N * degree)",
  "spaceComplexity": "O(N * degree)",
  "platform": "deepml"
};
