import { Problem } from '../../types';

export const DM_171_SOFTMAX_CROSS_ENTROPY__FORWARD_AND_GRADIENT: Problem = {
  "id": "dm_171_softmax_cross_entropy_forward_and_gradient",
  "title": "Softmax Cross-Entropy: Forward and Gradient",
  "difficulty": "Medium",
  "tags": [
    "Loss Functions",
    "Probability",
    "Backpropagation",
    "Matrix Operations",
    "Neural Networks"
  ],
  "descriptionMarkdown": "Implement a numerically stable softmax cross-entropy loss and its gradient with respect to the logits.\n\nGiven:\n- logits: a tensor of shape (N, C), where N is the batch size and C is the number of classes\n- targets: a tensor of shape (N,), containing integer class indices in [0, C-1]\n- optional label smoothing parameter epsilon in [0, 1)\n\nTasks:\n1. Compute the softmax probabilities using a numerically stable log-sum-exp trick.\n2. Compute the cross-entropy loss (mean reduction by default). If label smoothing is enabled, construct the smoothed target distribution: y_smooth = (1 - epsilon) * one_hot(target) + epsilon / C.\n3. Derive and return the gradient of the loss with respect to logits. With mean reduction, the gradient is: grad = (p - y_smooth) / N, where p is the softmax probabilities.\n\nReturn:\n- loss: a scalar tensor (mean loss across the batch)\n- grad: a tensor of shape (N, C), the gradient of the loss w.r.t. logits\n\nConstraints:\n- Use numerically stable computations (shift logits by their row-wise max before exponentiation).\n- Use only PyTorch tensor operations.",
  "solutionExplanation": "The softmax cross-entropy combines the softmax normalization with the negative log-likelihood. Directly exponentiating logits can be unstable, so we use the log-sum-exp trick: shift each row of logits by its maximum before exponentiation. Specifically, log_probs = logits - logsumexp(logits), where logsumexp is computed as log(sum(exp(shifted_logits))). The softmax probabilities are p = exp(log_probs).\n\nThe cross-entropy for one example with target distribution y is L = -sum_c y_c * log p_c. With label smoothing (epsilon), the target distribution becomes y_smooth = (1 - epsilon) * one_hot + epsilon / C. For a batch, we compute the mean loss across N samples. The analytical gradient of the mean cross-entropy loss with respect to the logits is grad = (p - y) / N, where y is the (possibly smoothed) target distribution and p is the softmax probabilities. This follows from d/dz logsoftmax(z) = I - softmax(z) and linearity over the target distribution.\n\nThe provided implementation follows these principles: it computes log_probs via a stable log-sum-exp, forms the (possibly smoothed) target distribution, calculates the mean loss as the negative expected log-probability under the target distribution, and finally returns the gradient (p - y) / N. An example demonstrates parity with PyTorch's nn.CrossEntropyLoss (including label_smoothing) and autograd-produced gradients.",
  "solutionCode": "import torch\nimport torch.nn as nn\nfrom typing import Tuple\n\n\ndef softmax_cross_entropy_with_grad(\n    logits: torch.Tensor,\n    targets: torch.Tensor,\n    label_smoothing: float = 0.0,\n    reduction: str = \"mean\",\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Compute numerically stable softmax cross-entropy and its gradient w.r.t. logits.\n\n    Args:\n        logits: Tensor of shape (N, C).\n        targets: Long tensor of shape (N,), class indices in [0, C-1].\n        label_smoothing: Float in [0, 1). 0.0 disables smoothing.\n        reduction: 'mean' or 'sum'. Defaults to 'mean'.\n\n    Returns:\n        loss: Scalar tensor (if reduction='mean' or 'sum') or shape (N,) if reduction=None.\n        grad: Tensor of shape (N, C), gradient of loss w.r.t. logits under the given reduction.\n    \"\"\"\n    if logits.ndim != 2:\n        raise ValueError(f\"logits must be 2D (N, C), got shape {tuple(logits.shape)}\")\n    if targets.ndim != 1 or targets.shape[0] != logits.shape[0]:\n        raise ValueError(\"targets must be 1D of length N, matching logits.shape[0]\")\n    if reduction not in (\"mean\", \"sum\"):\n        raise ValueError(\"reduction must be either 'mean' or 'sum'\")\n\n    N, C = logits.shape\n    if not torch.is_floating_point(logits):\n        logits = logits.float()\n\n    # Clamp label smoothing into valid range [0, 1)\n    eps = float(label_smoothing)\n    if eps < 0.0 or eps >= 1.0:\n        raise ValueError(\"label_smoothing must be in [0, 1)\")\n\n    # Numerically stable log-softmax via log-sum-exp\n    # shift for stability: subtract max per row\n    max_per_row = logits.max(dim=1, keepdim=True).values\n    shifted = logits - max_per_row\n    logsumexp = torch.log(torch.sum(torch.exp(shifted), dim=1, keepdim=True))\n    log_probs = shifted - logsumexp  # shape (N, C)\n    probs = torch.exp(log_probs)     # softmax probabilities\n\n    # Construct target distribution (with optional label smoothing)\n    # One-hot targets\n    y = torch.zeros_like(logits)\n    y.scatter_(1, targets.view(-1, 1), 1.0)\n    if eps > 0.0:\n        y = (1.0 - eps) * y + eps / C\n\n    # Cross-entropy loss: -sum(y * log_probs)\n    per_sample_loss = -(y * log_probs).sum(dim=1)  # shape (N,)\n    if reduction == \"mean\":\n        loss = per_sample_loss.mean()\n        grad = (probs - y) / N  # mean reduction -> divide by N\n    else:  # reduction == 'sum'\n        loss = per_sample_loss.sum()\n        grad = (probs - y)\n\n    return loss, grad\n\n\ndef _example_usage():\n    torch.manual_seed(0)\n    N, C = 4, 5\n    logits = torch.randn(N, C, requires_grad=False)\n    targets = torch.randint(0, C, (N,), dtype=torch.long)\n    eps = 0.1\n\n    # Our implementation\n    loss, grad = softmax_cross_entropy_with_grad(logits, targets, label_smoothing=eps, reduction=\"mean\")\n    print(\"Loss (ours):\", loss.item())\n\n    # Compare with PyTorch's CrossEntropyLoss (which combines LogSoftmax + NLLLoss)\n    logits_autograd = logits.clone().detach().requires_grad_(True)\n    criterion = nn.CrossEntropyLoss(label_smoothing=eps, reduction=\"mean\")\n    loss_torch = criterion(logits_autograd, targets)\n    loss_torch.backward()\n    print(\"Loss (torch):\", loss_torch.item())\n\n    # Check gradient closeness\n    max_abs_diff = (grad - logits_autograd.grad).abs().max().item()\n    print(\"Max abs grad diff:\", max_abs_diff)\n\n\nif __name__ == \"__main__\":\n    _example_usage()\n",
  "timeComplexity": "O(N*C)",
  "spaceComplexity": "O(N*C)",
  "platform": "deepml"
};
