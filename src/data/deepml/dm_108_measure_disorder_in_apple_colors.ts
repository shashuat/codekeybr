import { Problem } from '../../types';

export const DM_108_MEASURE_DISORDER_IN_APPLE_COLORS: Problem = {
  "id": "dm_108_measure_disorder_in_apple_colors",
  "title": "Measure Disorder in Apple Colors",
  "difficulty": "Easy",
  "tags": [
    "Probability"
  ],
  "descriptionMarkdown": "Implement a function that calculates the disorder in a basket of apples based on their colors, where each apple color is represented by an integer. The disorder must be 0 if all apples are the same color and must increase as the variety of colors increases.\n\nRequirements:\n- [0, 0, 0, 0] should yield 0.\n- [1, 1, 0, 0] should have a higher disorder than [0, 0, 0, 0].\n- [0, 1, 2, 3] should have a higher disorder than [1, 1, 0, 0].\n- [0, 0, 1, 1, 2, 2, 3, 3] should have a higher disorder than [0, 0, 0, 0, 0, 1, 2, 3].\n\nYou may use any method to measure disorder as long as these properties are satisfied.\n\nExample:\nInput: disorder([1, 1, 0, 0])\nOutput: 0.5 (or any value from -inf to +inf that preserves the ordering constraints)",
  "solutionExplanation": "A principled way to quantify disorder in categorical data is Shannon entropy of the empirical distribution over colors. Given counts c_i for each distinct color and total N apples, the empirical probabilities are p_i = c_i / N. The Shannon entropy H = -\u2211 p_i log(p_i) (log base is arbitrary) is 0 when all mass is on one color (p = 1 for one class, 0 elsewhere) and increases as the distribution becomes more uniform across more colors.\n\nThis measure satisfies all required comparisons: a uniform split across two colors (e.g., [1,1,0,0]) has higher entropy than a single-color basket; a uniform split across four colors (e.g., [0,1,2,3]) has higher entropy than two colors; and a perfectly balanced 4-color distribution ([0,0,1,1,2,2,3,3]) has higher entropy than an imbalanced distribution with the same number of colors ([0,0,0,0,0,1,2,3]). We implement this using PyTorch tensors and torch.unique with return_counts=True for robust counting over arbitrary integers.",
  "solutionCode": "import torch\nimport torch.nn as nn\nfrom typing import List\n\ndef disorder(apples: List[int]) -> float:\n    \"\"\"\n    Compute the disorder in a basket of apples using Shannon entropy (in bits).\n    - apples: list of integers representing colors (may include negative integers).\n    - returns a non-negative float, 0 when all apples are the same color.\n\n    This implementation uses PyTorch tensor ops only.\n    \"\"\"\n    # Handle empty input gracefully\n    if apples is None or len(apples) == 0:\n        return 0.0\n\n    # Convert to tensor of long for counting\n    x = torch.as_tensor(apples, dtype=torch.long)\n\n    # Count occurrences of each distinct color\n    # torch.unique works for arbitrary integer values (including negatives)\n    unique_vals, counts = torch.unique(x, sorted=False, return_counts=True)\n\n    # Convert counts to probabilities\n    probs = counts.to(dtype=torch.float64) / x.numel()\n\n    # Shannon entropy in bits: -sum p * log2(p); no need for epsilon since counts > 0\n    entropy = -(probs * torch.log2(probs)).sum()\n\n    return float(entropy.item())\n\n\ndef solution():\n    # Example usages demonstrating required ordering\n    examples = [\n        [0, 0, 0, 0],\n        [1, 1, 0, 0],\n        [0, 1, 2, 3],\n        [0, 0, 1, 1, 2, 2, 3, 3],\n        [0, 0, 0, 0, 0, 1, 2, 3],\n    ]\n    results = [disorder(b) for b in examples]\n    for b, r in zip(examples, results):\n        print(f\"basket={b}, disorder={r:.6f}\")\n\n    # Expected qualitative comparisons:\n    # disorder([0,0,0,0]) == 0\n    # disorder([1,1,0,0]) < disorder([0,1,2,3])\n    # disorder([0,0,1,1,2,2,3,3]) > disorder([0,0,0,0,0,1,2,3])\n\nif __name__ == \"__main__\":\n    solution()\n",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(K) where K is the number of distinct colors",
  "platform": "deepml"
};
