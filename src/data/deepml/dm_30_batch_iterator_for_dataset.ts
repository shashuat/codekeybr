import { Problem } from '../../types';

export const DM_30_BATCH_ITERATOR_FOR_DATASET: Problem = {
  "id": "dm_30_batch_iterator_for_dataset",
  "title": "Batch Iterator for Dataset",
  "difficulty": "Easy",
  "tags": [
    "Neural Networks",
    "Matrix Operations"
  ],
  "descriptionMarkdown": "Implement a batch iterable function that takes a NumPy array X and an optional NumPy array y, and returns mini-batches of a specified size. If y is provided, the function should return batches of (X, y) pairs; otherwise, it should return batches of X only.\n\nExample:\n\nInput:\n- X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]\n- y = [1, 2, 3, 4, 5]\n- batch_size = 2\n\nOutput:\n- For batches over (X, y):\n  [[[[1, 2], [3, 4]], [1, 2]],\n   [[[5, 6], [7, 8]], [3, 4]],\n   [[[9, 10]], [5]]]\n\nThe dataset X contains 5 samples, and with a batch size of 2, there are 3 batches. The first two batches contain 2 samples each, and the last batch contains the remaining sample. The corresponding values from y are included in each batch.",
  "solutionExplanation": "A batch iterator can be implemented by slicing the dataset along the first dimension with a fixed stride equal to the batch size. For each step, we create a view of X (and y if provided) between indices [start:end), where start increments by batch_size and end is clamped to the dataset length. This approach naturally handles the final, possibly smaller batch.\n\nUsing PyTorch ensures compatibility with common deep learning workflows. We convert the inputs to tensors via torch.as_tensor, which avoids unnecessary data copies when the inputs are already tensors or NumPy arrays. The function is implemented as a generator, yielding one batch at a time, which keeps memory overhead constant regardless of dataset size. If y is provided, we verify that its first dimension matches X to ensure consistent batching.",
  "solutionCode": "import torch\nfrom typing import Iterator, Optional, Tuple, Union, Iterable\n\nTensorLike = Union[torch.Tensor, 'numpy.ndarray', Iterable]\n\ndef batch_iterator(X: TensorLike, y: Optional[TensorLike] = None, batch_size: int = 64):\n    \"\"\"\n    Yield mini-batches from X (and y if provided).\n\n    Args:\n        X: Input samples as a NumPy array, PyTorch tensor, or any array-like with first-dimension as batch.\n        y: Optional targets/labels aligned with X along the first dimension.\n        batch_size: Number of samples per batch (must be > 0).\n\n    Yields:\n        - If y is None: torch.Tensor of shape (B, ...)\n        - If y is provided: tuple (X_batch, y_batch)\n\n    Notes:\n        - Uses torch.as_tensor to avoid unnecessary copies when possible.\n        - Works on CPU tensors; move batches to device as needed by the caller.\n    \"\"\"\n    if not isinstance(batch_size, int) or batch_size <= 0:\n        raise ValueError(\"batch_size must be a positive integer\")\n\n    X_t = torch.as_tensor(X)\n    if X_t.dim() == 0:\n        raise ValueError(\"X must have at least one dimension with batch along dim 0\")\n\n    n = X_t.shape[0]\n\n    if y is not None:\n        y_t = torch.as_tensor(y)\n        if y_t.shape[0] != n:\n            raise ValueError(f\"X and y must have the same number of samples, got {n} and {y_t.shape[0]}\")\n        for start in range(0, n, batch_size):\n            end = min(start + batch_size, n)\n            yield X_t[start:end], y_t[start:end]\n    else:\n        for start in range(0, n, batch_size):\n            end = min(start + batch_size, n)\n            yield X_t[start:end]\n\n\n# Example usage\nif __name__ == \"__main__\":\n    import numpy as np\n\n    X = np.array([[1, 2],\n                  [3, 4],\n                  [5, 6],\n                  [7, 8],\n                  [9, 10]])\n    y = np.array([1, 2, 3, 4, 5])\n\n    # Collect batches to match the example's printed structure\n    batches = [[[xb.tolist(), yb.tolist()] for xb, yb in [(xb, yb)]][0]\n               for xb, yb in batch_iterator(X, y, batch_size=2)]\n    print(batches)\n\n    # If y is None\n    x_batches = [xb for xb in batch_iterator(X, batch_size=2)]\n    # x_batches contains torch tensors of shape (2, 2), (2, 2), (1, 2)\n",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(1)",
  "platform": "deepml"
};
