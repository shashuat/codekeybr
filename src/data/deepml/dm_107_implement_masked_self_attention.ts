import { Problem } from '../../types';

export const DM_107_IMPLEMENT_MASKED_SELF_ATTENTION: Problem = {
  "id": "dm_107_implement_masked_self_attention",
  "title": "Implement Masked Self-Attention",
  "difficulty": "Medium",
  "tags": [
    "Transformers",
    "Attention",
    "Neural Networks",
    "Matrix Operations"
  ],
  "descriptionMarkdown": "Implement masked self-attention, a variation of the attention mechanism used in sequence modeling tasks such as text generation. Your task is to compute masked self-attention using query (Q), key (K), value (V) matrices and an attention mask.\n\nExample:\n- Input: `masked_attention(Q, K, V, mask)`\n- Output:\n```\n[[547. 490. 399. 495. 485. 439. 645. 393.]\n [547. 490. 399. 495. 485. 439. 645. 393.]\n [471. 472. 429. 538. 377. 450. 531. 362.]\n [471. 472. 429. 538. 377. 450. 531. 362.]\n [471. 472. 429. 538. 377. 450. 531. 362.]\n [471. 472. 429. 538. 377. 450. 531. 362.]]\n```\n\nThe function computes self-attention by applying a mask to restrict information flow (e.g., a causal mask), ensuring dependencies are maintained appropriately.",
  "solutionExplanation": "Masked self-attention uses the standard dot-product attention with a mask to prevent certain positions from attending to others. Given Q in R^{L x d_k}, K in R^{L x d_k}, and V in R^{L x d_v}, the attention scores are computed as S = Q K^T / sqrt(d_k). A mask M in {0,1}^{L x L} (or boolean) is applied to S by setting disallowed positions to a very negative value so that their softmax probability becomes (near) zero. This is commonly used for causal attention where tokens cannot attend to future positions.\n\nAfter masking, the attention weights are obtained by applying the softmax over the last dimension, A = softmax(S_masked). The output is a weighted sum of values, O = A V. The implementation supports broadcasting masks for batched inputs and handles both boolean and 0/1 masks. For numerical stability, we scale by sqrt(d_k) and replace masked positions with a large negative number before softmax, and we optionally guard against NaNs if an entire row is masked.",
  "solutionCode": "import torch\nimport torch.nn as nn\nfrom typing import Tuple, Optional\n\ndef compute_qkv(X: torch.Tensor,\n                 W_q: torch.Tensor,\n                 W_k: torch.Tensor,\n                 W_v: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    Supports input shapes:\n      - X: (..., L, d_model)\n      - W_q, W_k, W_v: (d_model, d_k) / (d_model, d_v)\n    Returns Q, K, V with shapes (..., L, d_k) / (..., L, d_v).\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return Q, K, V\n\n\ndef masked_attention(Q: torch.Tensor,\n                     K: torch.Tensor,\n                     V: torch.Tensor,\n                     mask: Optional[torch.Tensor],\n                     scale: bool = True,\n                     dropout_p: float = 0.0,\n                     dropout: Optional[nn.Dropout] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Compute masked (scaled) dot-product self-attention.\n\n    Args:\n        Q: Query tensor of shape (..., L, d_k)\n        K: Key tensor of shape   (..., L, d_k)\n        V: Value tensor of shape (..., L, d_v)\n        mask: Attention mask broadcastable to scores shape (..., L, L). True/1 = keep, False/0 = mask out.\n        scale: If True, scale scores by sqrt(d_k).\n        dropout_p: Dropout probability applied to attention weights.\n        dropout: Optional pre-created Dropout module to reuse.\n\n    Returns:\n        output: Tensor of shape (..., L, d_v)\n        attn:   Attention probabilities of shape (..., L, L)\n    \"\"\"\n    d_k = Q.size(-1)\n    # Compute raw attention scores: (..., L, L)\n    scores = Q @ K.transpose(-2, -1)\n    if scale:\n        scores = scores / (d_k ** 0.5)\n\n    if mask is not None:\n        # Ensure boolean mask where True means keep/attend\n        attn_mask = mask.bool()\n        # Broadcast mask to scores dims if needed\n        while attn_mask.dim() < scores.dim():\n            attn_mask = attn_mask.unsqueeze(0)\n        # Fill disallowed positions with a large negative number\n        scores = scores.masked_fill(~attn_mask, -1e9)\n\n    attn = torch.softmax(scores, dim=-1)\n    # Guard against potential NaNs if a row is fully masked\n    attn = torch.nan_to_num(attn, nan=0.0)\n\n    if dropout is None and dropout_p > 0.0:\n        dropout = nn.Dropout(dropout_p)\n    if dropout is not None and dropout_p > 0.0:\n        attn = dropout(attn)\n\n    # Weighted sum of values: (..., L, d_v)\n    output = attn @ V\n    return output, attn\n\n\ndef solution():\n    # Example usage\n    torch.manual_seed(0)\n\n    batch = 1\n    L = 6\n    d_model = 8\n    d_k = 8\n    d_v = 8\n\n    # Dummy inputs\n    X = torch.randn(batch, L, d_model)\n    W_q = torch.randn(d_model, d_k)\n    W_k = torch.randn(d_model, d_k)\n    W_v = torch.randn(d_model, d_v)\n\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n\n    # Causal mask: allow attending to current and previous positions\n    mask = torch.tril(torch.ones(L, L, dtype=torch.bool))  # (L, L)\n\n    out, attn = masked_attention(Q, K, V, mask, scale=True)\n\n    print(\"Output shape:\", out.shape)   # (batch, L, d_v)\n    print(\"Attention shape:\", attn.shape)  # (batch, L, L)\n\n    return out, attn\n\nif __name__ == \"__main__\":\n    solution()\n",
  "timeComplexity": "O(B * L^2 * d_k)",
  "spaceComplexity": "O(B * L^2 + B * L * d_v)",
  "platform": "deepml"
};
