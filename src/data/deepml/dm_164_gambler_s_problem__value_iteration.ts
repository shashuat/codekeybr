import { Problem } from '../../types';

export const DM_164_GAMBLER_S_PROBLEM__VALUE_ITERATION: Problem = {
  "id": "dm_164_gamblers_problem_value_iteration",
  "title": "Gambler's Problem: Value Iteration",
  "difficulty": "Hard",
  "tags": [
    "Probability",
    "Optimization"
  ],
  "descriptionMarkdown": "A gambler repeatedly bets on a biased coin. Starting with capital s (0 < s < 100), on each flip the gambler may stake any integer amount a in [1, min(s, 100 \u2212 s)]. If the coin lands heads, the gambler wins the stake (s \u2192 s + a); if tails, the gambler loses it (s \u2192 s \u2212 a). The process terminates upon reaching state 0 (bankruptcy) or 100 (goal). The reward is +1 only on transitions that reach state 100, and 0 otherwise.\n\nTask: Implement gambler_value_iteration(ph, theta=1e-9) that:\n- Computes the optimal state-value function V(s) for s = 1, \u2026, 99 via value iteration.\n- Returns the optimal policy giving, for each state s, the optimal stake (tie-breaking can be arbitrary). For s \u2208 {0, 100}, the policy should be 0.\n\nInputs:\n- ph: probability of heads (float in [0, 1])\n- theta: convergence threshold for value iteration (default 1e\u22129)\n\nOutputs:\n- V: list of length 101, where V[s] is the value of state s\n- policy: list of length 101, where policy[s] is the optimal stake at state s (0 for s \u2208 {0, 100})\n\nExample:\n- Input: ph = 0.4\n- Output example: round(V[50], 4) \u2248 0.0178 and policy[50] = 1. This reflects that from s = 50, the optimal action is to bet 1 and the optimal probability of eventually reaching 100 is \u2248 0.0178 for ph = 0.4.",
  "solutionExplanation": "This is a finite Markov Decision Process with terminal states at 0 and 100. The reward is +1 on transitions that reach 100 and 0 otherwise. Under an undiscounted episodic setting, the optimal value V*(s) equals the maximal probability of eventually reaching 100 starting from s. A standard way to compute V* is value iteration using the Bellman optimality update. A convenient formulation is to set V(0) = 0 and V(100) = 1 and use zero reward everywhere; this is equivalent to the original reward definition under gamma = 1 because the terminal state's value absorbs the +1 reward upon arrival.\n\nFor each nonterminal state s, the action space is the set of stakes a \u2208 {1, \u2026, min(s, 100 \u2212 s)}. The Bellman optimality backup is:\nV_{k+1}(s) = max_a [ ph \u00b7 V_k(s + a) + (1 \u2212 ph) \u00b7 V_k(s \u2212 a) ].\nWe iterate synchronous sweeps over all states until the maximum change across states is below theta. After convergence, we perform a separate greedy policy extraction by selecting, for each s, the stake a that maximizes the same action-value expression. Ties can be resolved by picking the first maximizing action (smallest stake).\n\nThis approach guarantees convergence because the state and action spaces are finite and the updates are monotone contractions under this setup. The resulting V and policy are optimal for the given ph.",
  "solutionCode": "import torch\n\ndef gambler_value_iteration(ph: float, theta: float = 1e-9):\n    \"\"\"\n    Compute the optimal value function and policy for the Gambler's Problem via value iteration.\n\n    Args:\n        ph (float): Probability of heads in [0, 1].\n        theta (float): Convergence threshold for value iteration.\n\n    Returns:\n        (V, policy):\n            V (list of float, len=101): state values V[s].\n            policy (list of int, len=101): optimal stake at state s (0 for s in {0, 100}).\n    \"\"\"\n    if not (0.0 <= ph <= 1.0):\n        raise ValueError(\"ph must be in [0, 1]\")\n\n    # Use double precision for stability\n    V = torch.zeros(101, dtype=torch.double)\n    # Terminal boundary conditions: treat V[100] = 1, V[0] = 0 (absorbing terminal values)\n    V[100] = 1.0\n\n    # Value Iteration\n    while True:\n        delta = 0.0\n        # Sweep over all nonterminal states\n        for s in range(1, 100):\n            max_stake = min(s, 100 - s)\n            if max_stake <= 0:\n                continue\n            stakes = torch.arange(1, max_stake + 1, dtype=torch.long)\n\n            # Expected value of each stake under current V\n            v_heads = V[s + stakes]\n            v_tails = V[s - stakes]\n            q_values = ph * v_heads + (1.0 - ph) * v_tails\n\n            v_new = torch.max(q_values)\n            diff = abs(v_new.item() - V[s].item())\n            if diff > delta:\n                delta = diff\n            V[s] = v_new\n        if delta < theta:\n            break\n\n    # Policy extraction (greedy w.r.t. the converged V)\n    policy = torch.zeros(101, dtype=torch.long)\n    for s in range(1, 100):\n        max_stake = min(s, 100 - s)\n        if max_stake <= 0:\n            policy[s] = 0\n            continue\n        stakes = torch.arange(1, max_stake + 1, dtype=torch.long)\n        v_heads = V[s + stakes]\n        v_tails = V[s - stakes]\n        q_values = ph * v_heads + (1.0 - ph) * v_tails\n        # Pick the first maximizing stake (smallest stake among ties)\n        best_idx = torch.argmax(q_values).item()\n        policy[s] = stakes[best_idx].item()\n\n    # Ensure terminal policies are 0\n    policy[0] = 0\n    policy[100] = 0\n\n    return V.tolist(), policy.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    ph = 0.4\n    V, policy = gambler_value_iteration(ph)\n    # Expected: around 0.0178 for V[50], policy[50] = 1\n    print(round(V[50], 4))\n    print(policy[50])\n",
  "timeComplexity": "O(S^2 * I), where S is the number of states (here 101) and I is the number of value-iteration sweeps until convergence.",
  "spaceComplexity": "O(S)",
  "platform": "deepml"
};
