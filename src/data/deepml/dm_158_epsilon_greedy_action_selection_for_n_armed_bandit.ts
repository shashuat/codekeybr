import { Problem } from '../../types';

export const DM_158_EPSILON_GREEDY_ACTION_SELECTION_FOR_N_ARMED_BANDIT: Problem = {
  "id": "dm_158_epsilon_greedy_action_selection_for_n_armed_bandit",
  "title": "Epsilon-Greedy Action Selection for n-Armed Bandit",
  "difficulty": "Medium",
  "tags": [
    "Probability"
  ],
  "descriptionMarkdown": "Implement the epsilon-greedy method for action selection in an n-armed bandit problem. Given a vector of estimated action values (Q-values), select an action using the epsilon-greedy policy: with probability epsilon, choose a random action uniformly; with probability 1 - epsilon, choose the action with the highest estimated value.\n\nExample:\n- Input:\n  - Q = [0.5, 2.3, 1.7]\n  - epsilon = 0.0\n- Output:\n  - 1\n- Reasoning: With epsilon = 0.0 (always greedy), the highest Q-value is 2.3 at index 1, so the function always returns 1.",
  "solutionExplanation": "The epsilon-greedy policy balances exploration and exploitation in the n-armed bandit setting. With probability epsilon, it chooses an action uniformly at random to explore the action space. With probability 1 - epsilon, it exploits current knowledge by selecting the action with the maximum estimated value.\n\nTo implement this, we draw a uniform random number in [0, 1). If it is less than epsilon, we sample a random action index from [0, n). Otherwise, we compute the argmax of the Q-values and return the corresponding index. Using PyTorch ensures compatibility with tensor-based pipelines and reproducibility via torch.Generator or torch.manual_seed. The implementation validates inputs, converts Q to a 1D float tensor, and performs all computations with PyTorch operations.",
  "solutionCode": "import torch\nimport torch.nn as nn\n\ndef epsilon_greedy(Q, epsilon: float = 0.1, generator: torch.Generator | None = None) -> int:\n    \"\"\"\n    Select an action using the epsilon-greedy policy.\n\n    Args:\n        Q: 1D sequence or torch.Tensor of shape (n,) with estimated action values.\n        epsilon: Exploration probability in [0, 1]. With probability epsilon, choose a random action.\n        generator: Optional torch.Generator for reproducible randomness.\n\n    Returns:\n        int: Selected action index in [0, n-1].\n    \"\"\"\n    # Convert input to a 1D float tensor\n    if not torch.is_tensor(Q):\n        Q = torch.tensor(Q, dtype=torch.float32)\n    else:\n        Q = Q.detach().to(dtype=torch.float32)\n\n    if Q.ndim != 1 or Q.numel() == 0:\n        raise ValueError(\"Q must be a non-empty 1D tensor or sequence of action values.\")\n\n    eps = float(epsilon)\n    if not (0.0 <= eps <= 1.0):\n        raise ValueError(\"epsilon must be in [0, 1].\")\n\n    n = Q.numel()\n\n    # Draw a random number to decide exploration vs. exploitation\n    if torch.rand((), generator=generator).item() < eps:\n        # Exploration: choose an action uniformly at random\n        action = int(torch.randint(low=0, high=n, size=(1,), generator=generator).item())\n    else:\n        # Exploitation: choose the action with the highest Q-value\n        # torch.argmax returns the first index in case of ties (deterministic tie-breaking)\n        action = int(torch.argmax(Q).item())\n\n    return action\n\n\ndef solution():\n    # Example usage\n    Q = torch.tensor([0.5, 2.3, 1.7])\n    epsilon = 0.0  # Always greedy\n    torch.manual_seed(0)  # For reproducibility of random branch, if used\n\n    action = epsilon_greedy(Q, epsilon)\n    print(action)  # Expected: 1\n    return action\n",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(1)",
  "platform": "deepml"
};
