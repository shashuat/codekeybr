import { Problem } from '../../types';

export const DM_127_FIND_CAPTAIN_REDBEARD_S_HIDDEN_TREASURE: Problem = {
  "id": "dm_127_find_captain_redbeard_s_hidden_treasure",
  "title": "Find Captain Redbeard's Hidden Treasure",
  "difficulty": "Medium",
  "tags": [
    "Optimization",
    "Gradient Descent"
  ],
  "descriptionMarkdown": "You are given a 1D \"terrain\" described by the function:\n\n- f(x) = x^4 - 3x^3 + 2\n\nYour task is to implement a function `find_treasure(start_x: float) -> float` that returns the x-coordinate where f(x) is minimized, starting from an arbitrary initial position `start_x`. Beware: there is a stationary point at x = 0 that is not the global minimum.\n\nExample:\n- Input: `start_x = 0.0`\n- Output: the x-value where f(x) reaches its minimum (a float)\n",
  "solutionExplanation": "The function f(x) = x^4 - 3x^3 + 2 is a smooth quartic polynomial. Its derivative is f'(x) = 4x^3 - 9x^2 = x^2(4x - 9). The stationary points are x = 0 (double root) and x = 9/4. Since x^4 dominates for large |x|, the function grows to +infinity at both tails; evaluating f at these critical points shows the global minimum occurs at x = 9/4 (2.25), while x = 0 is a flat stationary point that is not a minimum.\n\nTo emulate a robust optimization routine \"from any start,\" we use PyTorch to perform gradient-based optimization on the scalar parameter x. We create a 1D tensor with `requires_grad=True`, compute the loss f(x), backpropagate to get the gradient, and update x using Adam. A practical caveat is the flat stationary point at x = 0 where the gradient is exactly zero; purely gradient-based methods would stall there. To address this, we apply a tiny deterministic nudge if the initial gradient is ~0, then proceed with Adam and stop early when both the parameter change and gradient are sufficiently small. Using double precision ensures stable convergence.",
  "solutionCode": "import torch\n\n\ndef _f(x: torch.Tensor) -> torch.Tensor:\n    \"\"\"Objective function f(x) = x^4 - 3x^3 + 2 for a 1D tensor x.\"\"\"\n    return x**4 - 3.0 * x**3 + 2.0\n\n\ndef find_treasure(start_x: float, lr: float = 0.05, max_iters: int = 10000,\n                  tol: float = 1e-12, grad_tol: float = 1e-10) -> float:\n    \"\"\"\n    Find the x-coordinate that minimizes f(x) = x^4 - 3x^3 + 2 using PyTorch and gradient-based optimization.\n\n    Args:\n        start_x: Initial guess for x.\n        lr: Learning rate for Adam optimizer.\n        max_iters: Maximum number of optimization steps.\n        tol: Tolerance for change in x to declare convergence.\n        grad_tol: Tolerance on gradient magnitude to declare convergence.\n\n    Returns:\n        The float value of x that approximately minimizes f(x).\n    \"\"\"\n    # Use double precision for numerical stability in a 1D optimization problem.\n    x = torch.tensor([start_x], dtype=torch.float64, requires_grad=True)\n\n    # If we start exactly at a flat stationary point (e.g., x=0), the gradient is 0 and\n    # pure gradient descent would get stuck. Apply a tiny deterministic nudge.\n    (_f(x)).backward()\n    if x.grad is not None and torch.abs(x.grad).item() < 1e-14:\n        with torch.no_grad():\n            x += 1e-3\n    # Clear gradient after the initial check.\n    if x.grad is not None:\n        x.grad.zero_()\n\n    optimizer = torch.optim.Adam([x], lr=lr)\n\n    prev_x_val = float('inf')\n    for _ in range(max_iters):\n        optimizer.zero_grad(set_to_none=True)\n        loss = _f(x)\n        loss.backward()\n\n        grad_mag = torch.abs(x.grad).item()\n        optimizer.step()\n\n        x_val = x.item()\n        if abs(x_val - prev_x_val) < tol and grad_mag < grad_tol:\n            break\n        prev_x_val = x_val\n\n    return float(x.item())\n\n\ndef solution():\n    # Example usage: starting at 0.0 (a flat stationary point). The method nudges and converges to ~2.25.\n    xmin = find_treasure(0.0)\n    print(f\"Estimated minimizer x*: {xmin:.8f}\")\n    return xmin\n",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(1)",
  "platform": "deepml"
};
