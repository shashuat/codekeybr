import { Problem } from '../../types';

export const DM_88_GPT_2_TEXT_GENERATION: Problem = {
  "id": "dm_88_gpt_2_text_generation",
  "title": "GPT-2 Text Generation",
  "difficulty": "Hard",
  "tags": [
    "Transformers",
    "Attention",
    "Embeddings",
    "Neural Networks",
    "Activation Functions"
  ],
  "descriptionMarkdown": "Implement a simplified GPT-2-like text generation function in Python using PyTorch. Your function should capture the core components of a minimal GPT-2 architecture:\n\n- Token Embeddings: map input tokens to dense vectors.\n- Positional Embeddings: add positional information to token embeddings.\n- Multi-head Causal Self-Attention: attend over prior positions only.\n- Feed-Forward Network: position-wise MLP.\n- Layer Normalization: stabilize activations.\n\nUse the helper function `load_encoder_hparams_and_params` to retrieve a dummy encoder, model hyperparameters, and (dummy) model parameters. Build your autoregressive text generation loop around these components.\n\nFunction requirements:\n- Input: `prompt: str`, `n_tokens_to_generate: int` (the total number of tokens in the final output, including any tokens from the prompt; if the prompt already has at least this many tokens, truncate to this length).\n- Output: generated text as a string.\n\nExample:\n- Input: `prompt=\"hello\"`, `n_tokens_to_generate=5`\n- Possible Output: `hello hello hello <UNK> <UNK>`\n\nThis exercise focuses on understanding how GPT-2 performs autoregressive generation with masked self-attention and learned embeddings.",
  "solutionExplanation": "We implement a compact GPT-2-style Transformer with token and positional embeddings, pre-layer-normalized residual blocks, masked multi-head self-attention, and a position-wise feed-forward network. The attention uses a causal (triangular) mask so that, at time step t, the model only attends to positions \u2264 t. We tie the output projection weights to the token embedding matrix, a common practice in language models that reduces parameters and typically improves performance.\n\nFor generation, we first encode the prompt using a dummy byte-pair-like tokenizer that maps space-separated tokens to IDs from a tiny vocabulary. We then iteratively append tokens: at each step we run a forward pass on the current sequence (truncated to the maximum context length), take the logits for the last position, and greedily pick the argmax token. We repeat until the output length reaches `n_tokens_to_generate`. Finally, we decode token IDs back into text by reversing the dummy vocabulary mapping. While the parameters here are randomly initialized (for didactic purposes), the structure matches the core of GPT-2's autoregressive inference.\n\nThis setup illustrates how embeddings, positional encodings, masked multi-head attention, layer normalization, and MLPs combine to produce next-token distributions in a Transformer decoder-only architecture.",
  "solutionCode": "import math\nfrom typing import Dict, Tuple, Any, List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\") -> Tuple[Any, Dict[str, int], Dict[str, torch.Tensor]]:\n    \"\"\"\n    Returns:\n        encoder: a dummy encoder/decoder with a tiny vocab\n        hparams: minimal hyperparameters for a tiny GPT-2-like model\n        params: placeholder for model params (not used here; model initializes its own weights)\n    \"\"\"\n    class DummyBPE:\n        def __init__(self):\n            # Tiny vocab with three tokens\n            self.encoder_dict = {\"<UNK>\": 0, \"hello\": 1, \"world\": 2}\n\n        def encode(self, text: str) -> List[int]:\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(tok, self.encoder_dict[\"<UNK>\"]) for tok in tokens]\n\n        def decode(self, token_ids: List[int]) -> str:\n            inv = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join(inv.get(tid, \"<UNK>\") for tid in token_ids)\n\n    encoder = DummyBPE()\n\n    hparams = {\n        \"n_vocab\": len(encoder.encoder_dict),\n        \"n_embd\": 64,\n        \"n_head\": 4,\n        \"n_layer\": 2,\n        \"n_ctx\": 64,\n    }\n\n    params: Dict[str, torch.Tensor] = {}\n    return encoder, hparams, params\n\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, n_embd: int, n_head: int, n_ctx: int):\n        super().__init__()\n        assert n_embd % n_head == 0, \"Embedding dim must be divisible by number of heads\"\n        self.n_head = n_head\n        self.n_ctx = n_ctx\n        self.head_dim = n_embd // n_head\n\n        self.qkv = nn.Linear(n_embd, 3 * n_embd, bias=False)\n        self.proj = nn.Linear(n_embd, n_embd, bias=False)\n\n        # Causal mask (lower triangular)\n        mask = torch.tril(torch.ones(n_ctx, n_ctx, dtype=torch.bool))\n        self.register_buffer(\"attn_mask\", mask.view(1, 1, n_ctx, n_ctx), persistent=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # x: (B, T, C)\n        B, T, C = x.shape\n        qkv = self.qkv(x)  # (B, T, 3C)\n        q, k, v = qkv.split(C, dim=-1)\n\n        # Reshape to (B, n_head, T, head_dim)\n        def shape_proj(t):\n            return t.view(B, T, self.n_head, self.head_dim).permute(0, 2, 1, 3)\n\n        q = shape_proj(q)\n        k = shape_proj(k)\n        v = shape_proj(v)\n\n        # Scaled dot-product attention with causal masking\n        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (B, n_head, T, T)\n        att = att.masked_fill(self.attn_mask[:, :, :T, :T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        y = att @ v  # (B, n_head, T, head_dim)\n\n        # Merge heads\n        y = y.permute(0, 2, 1, 3).contiguous().view(B, T, C)\n        y = self.proj(y)\n        return y\n\n\nclass MLP(nn.Module):\n    def __init__(self, n_embd: int):\n        super().__init__()\n        self.fc1 = nn.Linear(n_embd, 4 * n_embd)\n        self.fc2 = nn.Linear(4 * n_embd, n_embd)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.fc2(F.gelu(self.fc1(x)))\n\n\nclass Block(nn.Module):\n    def __init__(self, n_embd: int, n_head: int, n_ctx: int):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.attn = CausalSelfAttention(n_embd, n_head, n_ctx)\n        self.ln2 = nn.LayerNorm(n_embd)\n        self.mlp = MLP(n_embd)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.attn(self.ln1(x))\n        x = x + self.mlp(self.ln2(x))\n        return x\n\n\nclass GPT2Mini(nn.Module):\n    def __init__(self, n_vocab: int, n_embd: int, n_head: int, n_layer: int, n_ctx: int):\n        super().__init__()\n        self.n_ctx = n_ctx\n        self.wte = nn.Embedding(n_vocab, n_embd)  # token embeddings\n        self.wpe = nn.Embedding(n_ctx, n_embd)    # positional embeddings\n        self.h = nn.ModuleList([Block(n_embd, n_head, n_ctx) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd)\n        self.lm_head = nn.Linear(n_embd, n_vocab, bias=False)\n\n        # Weight tying between token embeddings and output head\n        self.lm_head.weight = self.wte.weight\n\n    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n        # idx: (B, T)\n        B, T = idx.shape\n        assert T <= self.n_ctx, \"Sequence length exceeds context window\"\n\n        pos = torch.arange(0, T, dtype=torch.long, device=idx.device).unsqueeze(0)  # (1, T)\n        x = self.wte(idx) + self.wpe(pos)\n        for block in self.h:\n            x = block(x)\n        x = self.ln_f(x)\n        logits = self.lm_head(x)  # (B, T, vocab)\n        return logits\n\n\n@torch.no_grad()\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40) -> str:\n    \"\"\"\n    Generate text using a minimal GPT-2-like model.\n\n    Args:\n        prompt: initial text prompt\n        n_tokens_to_generate: total number of tokens in the final output (including prompt tokens).\n\n    Returns:\n        Generated text as a string.\n    \"\"\"\n    torch.manual_seed(0)\n    device = torch.device(\"cpu\")\n\n    encoder, hparams, _ = load_encoder_hparams_and_params()\n\n    model = GPT2Mini(\n        n_vocab=hparams[\"n_vocab\"],\n        n_embd=hparams[\"n_embd\"],\n        n_head=hparams[\"n_head\"],\n        n_layer=hparams[\"n_layer\"],\n        n_ctx=hparams[\"n_ctx\"],\n    ).to(device)\n    model.eval()\n\n    # Encode prompt\n    tokens: List[int] = encoder.encode(prompt)\n    if len(tokens) == 0:\n        tokens = [0]  # start with <UNK> if empty\n\n    # Ensure the output has exactly n_tokens_to_generate tokens total\n    target_len = max(1, n_tokens_to_generate)\n\n    # If prompt is longer than target, truncate\n    tokens = tokens[:target_len]\n\n    while len(tokens) < target_len:\n        # Prepare input (respect context window by truncating left side)\n        idx = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)  # (1, T)\n        if idx.size(1) > hparams[\"n_ctx\"]:\n            idx = idx[:, -hparams[\"n_ctx\"]:]\n\n        logits = model(idx)  # (1, T, V)\n        next_logits = logits[:, -1, :]  # (1, V)\n        next_token = int(torch.argmax(next_logits, dim=-1).item())\n        tokens.append(next_token)\n\n    # Decode exactly target_len tokens\n    out_text = encoder.decode(tokens[:target_len])\n    return out_text\n\n\nif __name__ == \"__main__\":\n    # Example usage\n    print(gen_text(prompt=\"hello\", n_tokens_to_generate=5))\n",
  "timeComplexity": "O(n_layers * T * L^2), where L <= n_ctx is the (capped) sequence length during generation and T is the number of new tokens generated",
  "spaceComplexity": "O(L * d + L^2) during inference (per layer), for hidden states and attention weights",
  "platform": "deepml"
};
