import { Problem } from '../../types';

export const DM_54_IMPLEMENTING_A_SIMPLE_RNN: Problem = {
  "id": "dm_54_implementing_a_simple_rnn",
  "title": "Implementing a Simple RNN",
  "difficulty": "Medium",
  "tags": [
    "Neural Networks",
    "RNNs",
    "Activation Functions",
    "Matrix Operations"
  ],
  "descriptionMarkdown": "Write a Python function that implements a simple Recurrent Neural Network (RNN) cell. The function should process a sequence of input vectors and produce the final hidden state. Use the tanh activation function for the hidden state updates. The function should take as inputs the sequence of input vectors, the initial hidden state, the weight matrices for input-to-hidden and hidden-to-hidden connections, and the bias vector. The function should return the final hidden state after processing the entire sequence, rounded to four decimal places.\n\nExample:\n\nInput:\n\n```python\ninput_sequence = [[1.0], [2.0], [3.0]]\ninitial_hidden_state = [0.0]\nWx = [[0.5]]  # Input to hidden weights\nWh = [[0.8]]  # Hidden to hidden weights\nb = [0.0]     # Bias\n```\n\nOutput:\n\n```python\nfinal_hidden_state = [0.9993]\n```\n\nReasoning:\nThe RNN processes each input in the sequence, updating the hidden state at each step using the tanh activation function.",
  "solutionExplanation": "A simple RNN cell updates its hidden state sequentially for each time step t using the recurrence: h_t = tanh(x_t Wx + h_{t-1} Wh + b), where x_t is the input at time t, h_{t-1} is the previous hidden state, Wx is the input-to-hidden weight matrix, Wh is the hidden-to-hidden (recurrent) weight matrix, and b is the bias vector. The tanh activation bounds the hidden state between -1 and 1 and introduces nonlinearity.\n\nTo implement this in PyTorch, we convert the provided lists into tensors with appropriate shapes: x_t has shape [input_dim], h has shape [hidden_dim], Wx is [input_dim, hidden_dim], Wh is [hidden_dim, hidden_dim], and b is [hidden_dim]. We iterate over the sequence, applying the RNN update at each step using vectorized tensor operations. After processing the entire sequence, we round the final hidden state to four decimal places and return it as a Python list.\n\nThis approach is efficient and numerically stable, leveraging PyTorch's optimized operations. It is easy to extend to batch processing or multiple layers, but here we focus on a single sequence and a single RNN cell as required.",
  "solutionCode": "import torch\nfrom typing import List\n\n\ndef rnn_forward(\n    input_sequence: List[List[float]],\n    initial_hidden_state: List[float],\n    Wx: List[List[float]],\n    Wh: List[List[float]],\n    b: List[float],\n) -> List[float]:\n    \"\"\"\n    Run a simple RNN over a sequence using tanh activation.\n\n    Args:\n        input_sequence: List of T input vectors, each of length input_dim -> shape [T, input_dim].\n        initial_hidden_state: Initial hidden state vector of length hidden_dim -> shape [hidden_dim].\n        Wx: Input-to-hidden weight matrix -> shape [input_dim, hidden_dim].\n        Wh: Hidden-to-hidden weight matrix -> shape [hidden_dim, hidden_dim].\n        b: Bias vector -> shape [hidden_dim].\n\n    Returns:\n        final_hidden_state: Final hidden state after processing the sequence, rounded to 4 decimals, as a list[float].\n    \"\"\"\n    # Convert to tensors\n    x_seq = torch.tensor(input_sequence, dtype=torch.float32)\n    h = torch.tensor(initial_hidden_state, dtype=torch.float32)\n    Wx_t = torch.tensor(Wx, dtype=torch.float32)\n    Wh_t = torch.tensor(Wh, dtype=torch.float32)\n    b_t = torch.tensor(b, dtype=torch.float32)\n\n    # Validate shapes\n    if x_seq.dim() != 2:\n        raise ValueError(f\"input_sequence must be 2D [T, input_dim], got shape {tuple(x_seq.shape)}\")\n    if h.dim() != 1:\n        raise ValueError(f\"initial_hidden_state must be 1D [hidden_dim], got shape {tuple(h.shape)}\")\n    T, input_dim = x_seq.shape\n    hidden_dim = h.shape[0]\n    if Wx_t.shape != (input_dim, hidden_dim):\n        raise ValueError(f\"Wx shape must be ({input_dim}, {hidden_dim}), got {tuple(Wx_t.shape)}\")\n    if Wh_t.shape != (hidden_dim, hidden_dim):\n        raise ValueError(f\"Wh shape must be ({hidden_dim}, {hidden_dim}), got {tuple(Wh_t.shape)}\")\n    if b_t.shape != (hidden_dim,):\n        raise ValueError(f\"b shape must be ({hidden_dim},), got {tuple(b_t.shape)}\")\n\n    # RNN forward pass\n    for t in range(T):\n        x_t_cur = x_seq[t]                    # [input_dim]\n        h = torch.tanh(x_t_cur @ Wx_t + h @ Wh_t + b_t)  # [hidden_dim]\n\n    # Round to 4 decimals and return as list\n    h_rounded = torch.round(h * 1_0000) / 1_0000\n    return h_rounded.tolist()\n\n\ndef solution():\n    # Example usage based on the prompt\n    input_sequence = [[1.0], [2.0], [3.0]]\n    initial_hidden_state = [0.0]\n    Wx = [[0.5]]  # Input to hidden weights\n    Wh = [[0.8]]  # Hidden to hidden weights\n    b = [0.0]     # Bias\n\n    final_hidden_state = rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b)\n    print(final_hidden_state)\n\n\nif __name__ == \"__main__\":\n    solution()\n",
  "timeComplexity": "O(T * (I * H + H^2)) where T is sequence length, I is input dimension, and H is hidden dimension",
  "spaceComplexity": "O(H) additional space for the hidden state",
  "platform": "deepml"
};
