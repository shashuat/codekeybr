import { Problem } from '../../types';

export const DM_91_CALCULATE_F1_SCORE_FROM_PREDICTED_AND_TRUE_LABELS: Problem = {
  "id": "dm_91_calculate_f1_score_from_predicted_and_true_labels",
  "title": "Calculate F1 Score from Predicted and True Labels",
  "difficulty": "Easy",
  "tags": [
    "Probability"
  ],
  "descriptionMarkdown": "Implement a function to compute the F1 score given binary predicted and true labels. The F1 score is the harmonic mean of precision and recall and is widely used to balance false positives and false negatives.\n\n- Input: two equal-length sequences of binary labels (0/1), `y_true` and `y_pred`.\n- Output: the F1 score rounded to three decimal places.\n\nExample:\n- Input: `y_true = [1, 0, 1, 1, 0]`, `y_pred = [1, 0, 0, 1, 1]`\n- Output: `0.667`\n\nNotes:\n- Use standard definitions: precision = TP / (TP + FP), recall = TP / (TP + FN), F1 = 2 * precision * recall / (precision + recall).\n- If any denominator is zero, treat the corresponding metric as 0 to avoid division-by-zero.",
  "solutionExplanation": "To compute the F1 score for binary classification, we first count true positives (TP), false positives (FP), and false negatives (FN) from the elementwise comparison of predictions and ground-truth labels. Precision quantifies correctness among predicted positives (TP / (TP + FP)), while recall quantifies coverage of actual positives (TP / (TP + FN)). The F1 score is the harmonic mean of precision and recall: F1 = 2 * precision * recall / (precision + recall).\n\nIn a robust implementation, we must handle edge cases where denominators are zero. If TP + FP is zero, precision is set to 0; if TP + FN is zero, recall is set to 0; and if precision + recall is zero, F1 is set to 0. Using PyTorch tensors and vectorized logical operations makes the computation concise and efficient. Finally, we round the resulting scalar score to three decimal places, as required.",
  "solutionCode": "import torch\nimport torch.nn as nn\n\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score for binary classification using PyTorch operations.\n\n    Args:\n        y_true (list/tuple/torch.Tensor): Ground truth binary labels (0/1).\n        y_pred (list/tuple/torch.Tensor): Predicted binary labels (0/1).\n\n    Returns:\n        float: F1 score rounded to three decimal places.\n    \"\"\"\n    # Convert inputs to boolean tensors for logical operations\n    t = torch.as_tensor(y_true).to(dtype=torch.bool)\n    p = torch.as_tensor(y_pred).to(dtype=torch.bool)\n\n    if t.numel() != p.numel():\n        raise ValueError(\"y_true and y_pred must have the same number of elements.\")\n\n    # Compute confusion components\n    tp = torch.logical_and(p, t).sum().to(torch.float64)\n    fp = torch.logical_and(p, torch.logical_not(t)).sum().to(torch.float64)\n    fn = torch.logical_and(torch.logical_not(p), t).sum().to(torch.float64)\n\n    # Precision and recall with safe division\n    denom_p = tp + fp\n    denom_r = tp + fn\n\n    precision = torch.where(denom_p > 0, tp / denom_p, torch.tensor(0.0, dtype=torch.float64))\n    recall = torch.where(denom_r > 0, tp / denom_r, torch.tensor(0.0, dtype=torch.float64))\n\n    # F1 with safe handling when precision + recall == 0\n    denom_f1 = precision + recall\n    f1 = torch.where(denom_f1 > 0, 2.0 * precision * recall / denom_f1, torch.tensor(0.0, dtype=torch.float64))\n\n    return round(float(f1.item()), 3)\n\n\ndef solution():\n    # Example usage\n    y_true = [1, 0, 1, 1, 0]\n    y_pred = [1, 0, 0, 1, 1]\n    score = calculate_f1_score(y_true, y_pred)\n    print(score)  # Expected: 0.667\n    return score\n",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(1)",
  "platform": "deepml"
};
