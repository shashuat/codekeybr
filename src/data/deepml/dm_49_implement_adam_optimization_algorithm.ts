import { Problem } from '../../types';

export const DM_49_IMPLEMENT_ADAM_OPTIMIZATION_ALGORITHM: Problem = {
  "id": "dm_49_implement_adam_optimization_algorithm",
  "title": "Implement Adam Optimization Algorithm",
  "difficulty": "Medium",
  "tags": [
    "Optimization",
    "Gradient Descent",
    "Neural Networks"
  ],
  "descriptionMarkdown": "Implement the Adam (Adaptive Moment Estimation) optimization algorithm in Python. Adam is an optimization algorithm that adapts the learning rate for each parameter.\n\nWrite a function `adam_optimizer` that updates the parameters of a given function using the Adam algorithm.\n\nParameters:\n- f: The objective function to be optimized.\n- grad: A function that computes the gradient of `f`.\n- x0: Initial parameter values.\n- learning_rate: The step size (default: 0.001).\n- beta1: Exponential decay rate for the first moment estimates (default: 0.9).\n- beta2: Exponential decay rate for the second moment estimates (default: 0.999).\n- epsilon: A small constant for numerical stability (default: 1e-8).\n- num_iterations: Number of iterations to run the optimizer (default: 1000).\n\nThe function should return the optimized parameters.\n\nExample:\n\n```python\n# Objective: minimize sum of squares\n# Expected behavior: parameters move toward zero\nimport torch\n\ndef objective_function(x):\n    return (x**2).sum()\n\ndef gradient(x):\n    return 2 * x\n\nx0 = torch.tensor([1.0, 1.0])\nx_opt = adam_optimizer(objective_function, gradient, x0)\nprint(\"Optimized parameters:\", x_opt)\n```",
  "solutionExplanation": "Adam maintains exponentially decaying moving averages of both the gradients (first moment) and the squared gradients (second moment). At each step t, it updates m_t = beta1 * m_{t-1} + (1 - beta1) * g_t and v_t = beta2 * v_{t-1} + (1 - beta2) * g_t^2, where g_t is the gradient at the current parameters. These moments estimate the mean and uncentered variance of the gradients to adapt step sizes per-parameter.\n\nBecause m_t and v_t are biased toward zero at early steps, Adam applies bias correction: m\u0302_t = m_t / (1 - beta1^t) and v\u0302_t = v_t / (1 - beta2^t). The parameter update is then x_{t+1} = x_t - lr * m\u0302_t / (sqrt(v\u0302_t) + eps). This combines the benefits of momentum (via the first moment) and RMS scaling (via the second moment), providing robust, adaptive learning rates that typically converge faster and more stably than vanilla SGD on many problems.\n",
  "solutionCode": "import torch\nfrom typing import Callable, Union\n\ndef adam_optimizer(\n    f: Callable[[torch.Tensor], torch.Tensor],\n    grad: Callable[[torch.Tensor], torch.Tensor],\n    x0: Union[torch.Tensor, list, tuple],\n    learning_rate: float = 1e-3,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    epsilon: float = 1e-8,\n    num_iterations: int = 1000,\n) -> torch.Tensor:\n    \"\"\"\n    Minimize objective f using the Adam optimizer and a provided gradient function.\n\n    Args:\n        f: Objective function mapping parameters x -> scalar tensor loss.\n        grad: Function computing gradient of f at x, returns tensor of same shape as x.\n        x0: Initial parameters (torch.Tensor, list, or tuple).\n        learning_rate: Step size.\n        beta1: Exponential decay rate for first moment.\n        beta2: Exponential decay rate for second moment.\n        epsilon: Numerical stability term.\n        num_iterations: Number of optimization steps.\n\n    Returns:\n        Optimized parameters as a torch.Tensor.\n    \"\"\"\n    # Initialize parameter tensor\n    if not isinstance(x0, torch.Tensor):\n        x = torch.tensor(x0, dtype=torch.float32)\n    else:\n        x = x0.clone().detach().to(dtype=torch.float32)\n\n    device = x.device\n    m = torch.zeros_like(x, device=device)\n    v = torch.zeros_like(x, device=device)\n\n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        if not isinstance(g, torch.Tensor):\n            g = torch.tensor(g, dtype=torch.float32, device=device)\n        else:\n            g = g.to(device=device, dtype=torch.float32)\n\n        # Update biased first and second moment estimates\n        m = beta1 * m + (1.0 - beta1) * g\n        v = beta2 * v + (1.0 - beta2) * (g * g)\n\n        # Compute bias-corrected moments\n        m_hat = m / (1.0 - beta1 ** t)\n        v_hat = v / (1.0 - beta2 ** t)\n\n        # Parameter update\n        x = x - learning_rate * m_hat / (torch.sqrt(v_hat) + epsilon)\n\n    return x\n\n\ndef solution():\n    # Example usage: minimize sum of squares\n    def objective_function(x: torch.Tensor) -> torch.Tensor:\n        return (x ** 2).sum()\n\n    def gradient(x: torch.Tensor) -> torch.Tensor:\n        return 2.0 * x\n\n    x0 = torch.tensor([1.0, 1.0])\n    x_opt = adam_optimizer(\n        objective_function,\n        gradient,\n        x0,\n        learning_rate=1e-2,\n        num_iterations=200,\n    )\n    print(\"Optimized parameters:\", x_opt)\n",
  "timeComplexity": "O(T * D)",
  "spaceComplexity": "O(D)",
  "platform": "deepml"
};
