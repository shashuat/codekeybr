import { Problem } from '../../types';

export const DM_122_POLICY_GRADIENT_WITH_REINFORCE: Problem = {
  "id": "dm_122_policy_gradient_with_reinforce",
  "title": "Policy Gradient with REINFORCE",
  "difficulty": "Hard",
  "tags": [
    "Optimization",
    "Probability",
    "Gradient Descent",
    "Matrix Operations"
  ],
  "descriptionMarkdown": "Implement the policy gradient estimator using the REINFORCE algorithm. The policy is parameterized by a 2D array `theta` of shape `(num_states, num_actions)`. For each state `s`, the policy over actions is computed via a softmax over `theta[s, :]`.\n\nGiven a list of episodes, where each episode is a list of `(state, action, reward)` tuples, compute the average policy gradient of the log-policy weighted by the return at each time step. Concretely, for each time step `t` in an episode, compute the return `G_t = \\sum_{k=t}^{T-1} r_k`, and accumulate `G_t * \u2207_\u03b8 log \u03c0_\u03b8(a_t | s_t)`.\n\nReturn the gradient averaged across episodes (i.e., divide the total accumulated gradient by the number of episodes). The output should have the same shape as `theta`.\n\nExample:\n- Input: `theta = np.zeros((2,2)); episodes = [[(0,1,0), (1,0,1)], [(0,0,0)]]`\n- Output: `[[ -0.25, 0.25 ], [ 0.25, -0.25 ]]`\n- Explanation: Episode 1 contributes a positive gradient from reward 1 at `t=1`; episode 2 adds zero. The result is averaged across the two episodes.",
  "solutionExplanation": "REINFORCE estimates the gradient of the expected return with respect to policy parameters by using sampled trajectories. For a softmax policy \u03c0_\u03b8(a|s) = softmax(\u03b8[s, :])_a, the score function (gradient of log-policy) with respect to the logits for a given state s is \u2207_\u03b8 log \u03c0_\u03b8(a|s) = one_hot(a) \u2212 \u03c0_\u03b8(\u00b7|s) (non-zero only on row s).\n\nFor each episode, we compute the return G_t at each time step t by a backward cumulative sum of rewards. The gradient contribution at step t is then G_t multiplied by the score function at (s_t, a_t). We sum these contributions over all steps and episodes, and finally average by the number of episodes to obtain an unbiased Monte Carlo estimate of the policy gradient. Using log-softmax/softmax ensures numerical stability, and since \u03b8 is state-action logits, the probabilities can be computed once per state for a given \u03b8.\n\nThis implementation avoids autograd and computes the analytical gradient for the softmax policy directly, which is efficient and clear for tabular policies. If desired, a baseline (e.g., average return) could be subtracted from G_t to reduce variance, but the problem statement requests the plain REINFORCE estimator.",
  "solutionCode": "import torch\nimport torch.nn as nn\nfrom typing import List, Tuple\n\n\ndef compute_policy_gradient(theta: torch.Tensor,\n                             episodes: List[List[Tuple[int, int, float]]]) -> torch.Tensor:\n    \"\"\"\n    Estimate the policy gradient using REINFORCE for a tabular softmax policy.\n\n    Args:\n        theta: Tensor of shape (num_states, num_actions) representing policy logits per state.\n        episodes: List of episodes; each episode is a list of (state, action, reward) tuples.\n\n    Returns:\n        Tensor of same shape as theta: average policy gradient across episodes.\n    \"\"\"\n    if not isinstance(theta, torch.Tensor):\n        theta = torch.tensor(theta, dtype=torch.float32)\n    else:\n        theta = theta.detach().clone().to(dtype=torch.float32)\n\n    num_states, num_actions = theta.shape\n\n    # Softmax policy probabilities per state (fixed for given theta)\n    probs = torch.softmax(theta, dim=1)  # (S, A)\n\n    grad = torch.zeros_like(theta)\n    num_episodes = len(episodes)\n    if num_episodes == 0:\n        return grad\n\n    for ep in episodes:\n        if len(ep) == 0:\n            continue\n        # Compute returns G_t via backward cumulative sum\n        T = len(ep)\n        returns = [0.0] * T\n        G = 0.0\n        for t in range(T - 1, -1, -1):\n            _, _, r = ep[t]\n            G += float(r)\n            returns[t] = G\n\n        # Accumulate gradient contributions per time step\n        for t, (s, a, _r) in enumerate(ep):\n            s = int(s)\n            a = int(a)\n            G_t = returns[t]\n\n            # Score function: one_hot(a) - probs[s]\n            one_hot = torch.zeros(num_actions, dtype=torch.float32)\n            one_hot[a] = 1.0\n            row_grad = one_hot - probs[s]\n\n            grad[s] += G_t * row_grad\n\n    # Average across episodes\n    grad = grad / float(num_episodes)\n    return grad\n\n\ndef solution():\n    # Example usage matching the problem statement\n    theta = torch.zeros((2, 2), dtype=torch.float32)\n    episodes = [\n        [(0, 1, 0.0), (1, 0, 1.0)],\n        [(0, 0, 0.0)]\n    ]\n\n    grad = compute_policy_gradient(theta, episodes)\n    # For demonstration, print the result; expected:\n    # tensor([[-0.2500,  0.2500],\n    #         [ 0.2500, -0.2500]])\n    print(grad)\n\n\nif __name__ == \"__main__\":\n    solution()\n",
  "timeComplexity": "O(E * T * A), where E is the number of episodes, T is average episode length, and A is the number of actions",
  "spaceComplexity": "O(S * A), where S is the number of states and A is the number of actions",
  "platform": "deepml"
};
