import { Problem } from '../../types';

export const DM_160_MIXED_PRECISION_TRAINING: Problem = {
  "id": "dm_160_mixed_precision_training",
  "title": "Mixed Precision Training",
  "difficulty": "Medium",
  "tags": [
    "Optimization",
    "Loss Functions",
    "Backpropagation",
    "Matrix Operations"
  ],
  "descriptionMarkdown": "Write a Python class to implement Mixed Precision Training that uses both float32 and float16 data types to optimize memory usage and speed.\n\nRequirements:\n- Implement a class MixedPrecision with an __init__(self, loss_scale=1024.0) method to initialize a loss scaling factor.\n- Implement forward(self, weights, inputs, targets) to perform the forward pass in float16 and return the Mean Squared Error (MSE) loss scaled by loss_scale as a float32 Python float.\n- Implement backward(self, gradients) to unscale gradients by loss_scale, check for overflow (inf/NaN), and return gradients in float32. If overflow is detected, return zero gradients.\n- Use float16 for computations but float32 for gradient accumulation.\n\nExample:\n\nInput:\n\nimport numpy as np\nmp = MixedPrecision(loss_scale=1024.0)\nweights = np.array([0.5, -0.3], dtype=np.float32)\ninputs = np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32)\ntargets = np.array([1.0, 0.0], dtype=np.float32)\nloss = mp.forward(weights, inputs, targets)\nprint(f\"Loss: {loss:.4f}\")\nprint(f\"Loss dtype: {type(loss).__name__}\")\ngrads = np.array([512.0, -256.0], dtype=np.float32)\nresult = mp.backward(grads)\nprint(f\"Gradients: {result}\")\nprint(f\"Grad dtype: {result.dtype}\")\n\nOutput:\n\nLoss: 665.0000\nLoss dtype: float\nGradients: [0.5 -0.25]\nGrad dtype: float32",
  "solutionExplanation": "Mixed precision training accelerates computations and reduces memory use by performing most arithmetic in float16 while keeping numerically sensitive quantities (like loss scaling and gradient accumulation) in float32. Because float16 has a narrower dynamic range, it is common to scale the loss by a large factor before backpropagation to avoid gradient underflow; later, gradients are unscaled by dividing by the same factor.\n\nIn the forward pass, we cast inputs, weights, and targets to float16, compute predictions with a matrix-vector product, and evaluate the MSE loss in float16. We then cast the loss to float32 and multiply by a configurable loss_scale, returning a Python float via .item(). In the backward pass, provided gradients are first converted to float32, then unscaled by dividing by loss_scale. We check for overflow by testing if any unscaled gradients are non-finite (inf or NaN). If overflow is detected, we return zero gradients; otherwise, we return the unscaled float32 gradients. This design mirrors practical mixed-precision workflows: compute-heavy ops in fp16, while critical accumulations and checks remain in fp32.",
  "solutionCode": "import torch\nimport torch.nn as nn\nfrom typing import Union\n\nclass MixedPrecision:\n    \"\"\"\n    Minimal mixed-precision helper that:\n    - Performs forward computations in float16\n    - Returns scaled MSE loss as a Python float (from float32)\n    - Unscales provided gradients in backward and checks for overflow\n    - Returns gradients in float32, zeroing them if overflow is detected\n    \"\"\"\n    def __init__(self, loss_scale: float = 1024.0):\n        self.loss_scale = float(loss_scale)\n        self.overflow = False\n        self.last_loss = None\n\n    def forward(\n        self,\n        weights: Union[torch.Tensor, list, tuple],\n        inputs: Union[torch.Tensor, list, tuple],\n        targets: Union[torch.Tensor, list, tuple],\n    ) -> float:\n        # Cast to float16 for computations\n        w16 = torch.as_tensor(weights, dtype=torch.float16)\n        x16 = torch.as_tensor(inputs, dtype=torch.float16)\n        y16 = torch.as_tensor(targets, dtype=torch.float16)\n\n        # Prediction (matrix-vector product)\n        # Supports shapes: (B, D) @ (D,) -> (B,)\n        preds16 = x16.matmul(w16)\n\n        # MSE in float16\n        err16 = preds16 - y16\n        mse16 = (err16 * err16).mean()\n\n        # Scale and cast to float32, return Python float\n        scaled_loss32 = mse16.to(torch.float32) * self.loss_scale\n        self.last_loss = float(scaled_loss32.item())\n        return self.last_loss\n\n    def backward(self, gradients: Union[torch.Tensor, list, tuple]) -> torch.Tensor:\n        # Ensure float32 for gradient accumulation\n        g32 = torch.as_tensor(gradients, dtype=torch.float32)\n\n        # Unscale gradients\n        unscaled = g32 / self.loss_scale\n\n        # Overflow/NaN/Inf check\n        if not torch.isfinite(unscaled).all():\n            self.overflow = True\n            return torch.zeros_like(unscaled, dtype=torch.float32)\n\n        self.overflow = False\n        return unscaled\n\n\ndef solution():\n    # Example usage mirroring the problem's example but in PyTorch\n    mp = MixedPrecision(loss_scale=1024.0)\n\n    weights = torch.tensor([0.5, -0.3], dtype=torch.float32)\n    inputs = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32)\n    targets = torch.tensor([1.0, 0.0], dtype=torch.float32)\n\n    loss = mp.forward(weights, inputs, targets)\n    print(f\"Loss: {loss:.4f}\")\n    print(f\"Loss dtype: {type(loss).__name__}\")\n\n    grads = torch.tensor([512.0, -256.0], dtype=torch.float32)\n    result = mp.backward(grads)\n    print(f\"Gradients: {result}\")\n    print(f\"Grad dtype: {result.dtype}\")\n\n    # Demonstrate overflow handling\n    bad_grads = torch.tensor([float('inf'), 1.0], dtype=torch.float32)\n    result_overflow = mp.backward(bad_grads)\n    print(f\"Overflow detected: {mp.overflow}\")\n    print(f\"Gradients after overflow: {result_overflow}\")\n",
  "timeComplexity": "O(MN) for a (M x N) inputs times N weights matrix-vector product",
  "spaceComplexity": "O(M + N) for intermediate tensors; O(1) extra beyond inputs/outputs",
  "platform": "deepml"
};
