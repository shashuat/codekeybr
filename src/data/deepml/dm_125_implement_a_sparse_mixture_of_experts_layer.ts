import { Problem } from '../../types';

export const DM_125_IMPLEMENT_A_SPARSE_MIXTURE_OF_EXPERTS_LAYER: Problem = {
  "id": "dm_125_implement_a_sparse_mixture_of_experts_layer",
  "title": "Implement a Sparse Mixture of Experts Layer",
  "difficulty": "Hard",
  "tags": [
    "Neural Networks",
    "Transformers",
    "Matrix Operations",
    "Probability"
  ],
  "descriptionMarkdown": "Implement a Mixture-of-Experts (MoE) layer using softmax gating and top-k routing.\n\nGiven:\n- x: input tensor of shape (n_batch, l_seq, d_model)\n- We: expert weight matrices of shape (n_experts, d_model, d_model)\n- Wg: gating weight matrix of shape (d_model, n_experts)\n- n_experts: number of experts\n- top_k: number of experts to route each token to\n\nTask:\n1. Compute gating logits for each token via x @ Wg, apply softmax over experts to obtain probabilities.\n2. For each token, select the top-k experts by probability.\n3. Renormalize the selected top-k probabilities so they sum to 1 per token.\n4. Apply the selected experts' linear transforms to the token and aggregate the outputs as a weighted sum using the renormalized top-k probabilities.\n\nReturn the final tensor of shape (n_batch, l_seq, d_model).\n\nExample:\n- x = np.arange(12).reshape(2, 3, 2)\n- We = np.ones((4, 2, 2))\n- Wg = np.ones((2, 4))\n- top_k = 1\nOutput:\n[[[1, 1], [5, 5], [9, 9]], [[13, 13], [17, 17], [21, 21]]]\n",
  "solutionExplanation": "A sparse Mixture-of-Experts (MoE) layer routes each token to only a few experts, reducing compute compared to dense MoE. We first compute gating logits per token using a linear projection x @ Wg that maps d_model to n_experts. Applying a softmax over experts yields a probability distribution per token. We then select the top-k experts and renormalize the selected probabilities so they sum to 1 for each token, a standard practice to maintain the expected scale and to match typical MoE routing behavior.\n\nFor computation, we avoid evaluating all experts by gathering only the top-k expert weight matrices per token. We then apply the corresponding linear transforms to the token and aggregate their outputs with the renormalized probabilities. This preserves differentiability through the gating softmax for the selected experts while reducing cost from O(E) expert transforms to O(K) per token.\n\nThe implementation uses vectorized PyTorch operations: softmax for gating, topk for routing, advanced indexing to gather expert weights, and einsum for the batched token\u2013expert matrix multiplications. The example provided on the page is reproduced by renormalizing the top-1 probability to 1, yielding the expected output.",
  "solutionCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef moe(x: torch.Tensor, We: torch.Tensor, Wg: torch.Tensor, n_experts: int, top_k: int) -> torch.Tensor:\n    \"\"\"\n    Sparse Mixture-of-Experts forward pass with softmax gating and top-k routing.\n\n    Args:\n        x: Input tensor of shape (B, L, D)\n        We: Expert weights of shape (E, D, D)\n        Wg: Gating weights of shape (D, E)\n        n_experts: Number of experts (E). Must match We.shape[0].\n        top_k: Number of experts to route each token to (K).\n\n    Returns:\n        Output tensor of shape (B, L, D)\n    \"\"\"\n    assert x.dim() == 3, \"x must be (B, L, D)\"\n    B, L, D = x.shape\n    E = n_experts\n    assert We.shape == (E, D, D), f\"We must be (E, D, D); got {tuple(We.shape)}\"\n    assert Wg.shape == (D, E), f\"Wg must be (D, E); got {tuple(Wg.shape)}\"\n    assert 1 <= top_k <= E, \"top_k must be in [1, n_experts]\"\n\n    # Gating: logits -> probabilities over experts\n    # (B, L, D) @ (D, E) -> (B, L, E)\n    logits = torch.matmul(x, Wg)\n    gate_probs = F.softmax(logits, dim=-1)\n\n    # Top-k selection and renormalization among selected experts\n    topk_vals, topk_idx = torch.topk(gate_probs, k=top_k, dim=-1)  # (B, L, K), (B, L, K)\n    # Renormalize so selected probabilities sum to 1 per token\n    denom = topk_vals.sum(dim=-1, keepdim=True).clamp_min(1e-12)\n    topk_probs = topk_vals / denom  # (B, L, K)\n\n    # Gather selected expert matrices per token: (B, L, K, D, D)\n    W_sel = We[topk_idx]\n\n    # Apply selected experts to tokens using batched einsum\n    # x_exp: (B, L, 1, D) -> broadcast to (B, L, K, D)\n    x_exp = x.unsqueeze(2)\n    # y_sel: (B, L, K, D)\n    y_sel = torch.einsum('blkd,blkdh->blkh', x_exp, W_sel)\n\n    # Weight by normalized top-k probabilities and sum over K\n    out = (y_sel * topk_probs.unsqueeze(-1)).sum(dim=2)\n    return out\n\n\nclass SparseMoE(nn.Module):\n    \"\"\"Sparse Mixture-of-Experts layer with softmax gating and top-k routing.\"\"\"\n    def __init__(self, d_model: int, n_experts: int, top_k: int):\n        super().__init__()\n        self.d_model = d_model\n        self.n_experts = n_experts\n        self.top_k = top_k\n        # Expert matrices: (E, D, D)\n        self.We = nn.Parameter(torch.empty(n_experts, d_model, d_model))\n        # Gating matrix: (D, E)\n        self.Wg = nn.Parameter(torch.empty(d_model, n_experts))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        # Xavier init for stability\n        nn.init.xavier_uniform_(self.We)\n        nn.init.xavier_uniform_(self.Wg)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return moe(x, self.We, self.Wg, self.n_experts, self.top_k)\n\n\ndef solution():\n    # Example from the prompt reproduced in PyTorch\n    x = torch.arange(12, dtype=torch.float32).view(2, 3, 2)\n    We = torch.ones(4, 2, 2, dtype=torch.float32)\n    Wg = torch.ones(2, 4, dtype=torch.float32)\n    top_k = 1\n\n    y = moe(x, We, Wg, n_experts=4, top_k=top_k)\n    print(y)\n    # Expected:\n    # tensor([[[ 1.,  1.],\n    #          [ 5.,  5.],\n    #          [ 9.,  9.]],\n    #         [[13., 13.],\n    #          [17., 17.],\n    #          [21., 21.]]])\n\nif __name__ == \"__main__\":\n    solution()\n",
  "timeComplexity": "O(B * L * (D * E + K * D^2))",
  "spaceComplexity": "O(E * D^2 + B * L * K * D)",
  "platform": "deepml"
};
