import { Problem } from '../../types';

export const DM_86_DETECT_OVERFITTING_OR_UNDERFITTING: Problem = {
  "id": "dm_86_detect_overfitting_or_underfitting",
  "title": "Detect Overfitting or Underfitting",
  "difficulty": "Easy",
  "tags": [
    "Regularization"
  ],
  "descriptionMarkdown": "Write a Python function to determine whether a machine learning model is overfitting, underfitting, or performing well based on training and test accuracy values. The function should take two inputs: `training_accuracy` and `test_accuracy`. It should return one of three values: `1` if Overfitting, `-1` if Underfitting, or `0` if a Good fit.\n\nRules:\n- Overfitting: The training accuracy is significantly higher than the test accuracy (difference > 0.2).\n- Underfitting: Both training and test accuracy are below 0.7.\n- Good fit: Neither of the above conditions is true.\n\nExample:\n- Input: `training_accuracy = 0.95`, `test_accuracy = 0.65`\n- Output: `1` (Overfitting)\n- Reasoning: The training accuracy is much higher than the test accuracy (difference = 0.30 > 0.2), indicating overfitting.",
  "solutionExplanation": "We classify the model's fit quality using two simple heuristics derived from generalization behavior. If the gap between training and test accuracy exceeds 0.2, we flag the model as overfitting because it performs much better on seen data than on unseen data. If both training and test accuracies are below 0.7, we mark it as underfitting since the model fails to achieve acceptable performance even on the training set.\n\nIf neither condition is met, we consider the model a good fit. In the rare case where both conditions could be true (e.g., both accuracies are low and the gap is also large), we prioritize the overfitting rule first, then underfitting, and finally good fit. The implementation uses PyTorch tensors for computations, includes input validation to ensure accuracies lie in [0, 1], and returns an integer label as specified.",
  "solutionCode": "import torch\nfrom typing import Union\n\n\ndef model_fit_quality(training_accuracy: Union[float, torch.Tensor],\n                      test_accuracy: Union[float, torch.Tensor]) -> int:\n    \"\"\"\n    Determine if a model is overfitting, underfitting, or a good fit based on\n    training and test accuracies.\n\n    Returns:\n        1  -> Overfitting (training - test > 0.2)\n        -1 -> Underfitting (training < 0.7 and test < 0.7)\n        0  -> Good fit (otherwise)\n\n    Precedence when multiple conditions are true:\n        Overfitting > Underfitting > Good fit\n    \"\"\"\n    # Convert inputs to 0-D PyTorch tensors for consistent tensor operations\n    train = torch.as_tensor(training_accuracy, dtype=torch.float32)\n    test = torch.as_tensor(test_accuracy, dtype=torch.float32)\n\n    # Validate input ranges\n    if torch.any((train < 0) | (train > 1)) or torch.any((test < 0) | (test > 1)):\n        raise ValueError(\"Accuracies must be within [0, 1].\")\n\n    # Compute conditions using PyTorch ops\n    diff = train - test\n    overfit = diff > 0.2\n    underfit = (train < 0.7) & (test < 0.7)\n\n    # Apply precedence: Overfitting first, then Underfitting, else Good fit\n    if bool(overfit):\n        return 1\n    if bool(underfit):\n        return -1\n    return 0\n\n\ndef solution(training_accuracy: float, test_accuracy: float) -> int:\n    \"\"\"Wrapper to match the required interface.\"\"\"\n    return model_fit_quality(training_accuracy, test_accuracy)\n\n\nif __name__ == \"__main__\":\n    # Example usages\n    print(solution(0.95, 0.65))  # Expected: 1 (Overfitting)\n    print(solution(0.60, 0.55))  # Expected: -1 (Underfitting)\n    print(solution(0.85, 0.80))  # Expected: 0 (Good fit)\n",
  "timeComplexity": "O(1)",
  "spaceComplexity": "O(1)",
  "platform": "deepml"
};
