import { Problem } from '../../types';

export const DM_151_DROPOUT_LAYER: Problem = {
  "id": "dm_151_dropout_layer",
  "title": "Dropout Layer",
  "difficulty": "Medium",
  "tags": [
    "Neural Networks",
    "Regularization",
    "Backpropagation",
    "Probability"
  ],
  "descriptionMarkdown": "Implement a dropout layer that applies random neuron deactivation during training to prevent overfitting in neural networks. The layer should randomly zero out a proportion of input elements based on a dropout rate p, scale the remaining values by 1/(1-p) to maintain expected values, and pass inputs unchanged during inference.\n\nDuring backpropagation, gradients must be masked with the same dropout pattern and scaled by the same factor to ensure proper gradient flow.\n\nExample:\n- Input: x = [1.0, 2.0, 3.0, 4.0], grad = [0.1, 0.2, 0.3, 0.4], p = 0.5\n- Output: output = [2., 0., 6., 0.], grad = [0.2, 0., 0.6, 0.]\n\nReasoning:\nThe Dropout layer randomly zeroes out elements of the input tensor with probability p during training. To maintain the expected value of the activations, the remaining elements are scaled by a factor of 1 / (1 - p). During inference, Dropout is disabled and the input is passed through unchanged. During backpropagation, the same dropout mask and scaling are applied to the gradients, ensuring the expected gradient magnitude is preserved.",
  "solutionExplanation": "Dropout is a regularization technique that randomly deactivates units with probability p during training. For each input element x_i, we draw a Bernoulli mask m_i ~ Bernoulli(1 - p). The forward output is y_i = (m_i * x_i) / (1 - p) during training, and y_i = x_i during inference (no randomness). The division by (1 - p) ensures the expected value of activations remains unchanged.\n\nFor backpropagation, the gradient must be consistent with the forward mask: only the surviving units receive gradients, and those gradients are scaled by the same factor 1 / (1 - p). Thus, if g is the upstream gradient, the gradient wrt input is dy/dx = m / (1 - p) element-wise, so grad_input = g * m / (1 - p). This can be implemented cleanly in PyTorch either by relying on autograd through masked/scaled multiplication or by explicitly defining a custom autograd Function that saves the mask and reuses it in backward.\n\nThe provided PyTorch solution implements Dropout as a custom torch.autograd.Function to explicitly control both forward and backward, and wraps it in an nn.Module for convenient use. It supports toggling training behavior via an argument or the module's training flag, validates p in [0, 1), and uses vectorized operations for efficiency.",
  "solutionCode": "import torch\nimport torch.nn as nn\nfrom typing import Optional\n\nclass _ManualDropoutFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input: torch.Tensor, p: float, training: bool) -> torch.Tensor:\n        if p < 0.0 or p >= 1.0:\n            raise ValueError(f\"Dropout probability p must be in [0, 1), got {p}.\")\n        if not training or p == 0.0:\n            # In inference mode or p==0, pass through unchanged; no mask needed.\n            ctx.save_for_backward(None)\n            ctx.p = p\n            ctx.training = False\n            return input\n        # Training mode: sample Bernoulli mask and scale activations\n        mask = (torch.rand_like(input) > p).to(input.dtype)\n        scale = 1.0 / (1.0 - p)\n        output = input * mask * scale\n        # Save mask and metadata for backward\n        ctx.save_for_backward(mask)\n        ctx.p = p\n        ctx.training = True\n        ctx.scale = scale\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output: torch.Tensor):\n        # Gradients w.r.t. input and (non-tensor) args\n        if not ctx.training or ctx.p == 0.0:\n            grad_input = grad_output\n        else:\n            (mask,) = ctx.saved_tensors\n            grad_input = grad_output * mask * ctx.scale\n        # None for non-tensor args (p, training)\n        return grad_input, None, None\n\n\nclass DropoutLayer(nn.Module):\n    \"\"\"Dropout layer with explicit forward/backward semantics via a custom autograd Function.\n\n    Args:\n        p (float): Dropout probability in [0, 1).\n    \"\"\"\n    def __init__(self, p: float):\n        super().__init__()\n        if not (0.0 <= p < 1.0):\n            raise ValueError(f\"Dropout probability p must be in [0, 1), got {p}.\")\n        self.p = float(p)\n\n    def forward(self, x: torch.Tensor, training: Optional[bool] = None) -> torch.Tensor:\n        # Respect explicit flag if provided; otherwise use module's training flag\n        if training is None:\n            training = self.training\n        return _ManualDropoutFunction.apply(x, self.p, bool(training))\n\n\ndef solution():\n    \"\"\"Demonstration of the custom DropoutLayer with forward and backward.\n\n    Returns:\n        A tuple (output, input_grad) for a fixed random seed to make behavior reproducible.\n    \"\"\"\n    torch.manual_seed(0)\n    x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n    upstream_grad = torch.tensor([0.1, 0.2, 0.3, 0.4])\n\n    layer = DropoutLayer(p=0.5)\n    # Force training=True to apply dropout; in practice you can also call layer.train()/layer.eval()\n    out = layer(x, training=True)\n    # Backpropagate a custom upstream gradient\n    out.backward(upstream_grad)\n\n    # Detach for clean return/printing\n    return out.detach(), x.grad.detach()\n\n\nif __name__ == \"__main__\":\n    out, grad_in = solution()\n    print(\"Dropout output:\", out)\n    print(\"Gradient w.r.t input:\", grad_in)\n",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(N) during training to store the dropout mask; O(1) additional during inference",
  "platform": "deepml"
};
