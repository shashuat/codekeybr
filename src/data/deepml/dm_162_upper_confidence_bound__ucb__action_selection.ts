import { Problem } from '../../types';

export const DM_162_UPPER_CONFIDENCE_BOUND__UCB__ACTION_SELECTION: Problem = {
  "id": "dm_162_upper_confidence_bound_ucb_action_selection",
  "title": "Upper Confidence Bound (UCB) Action Selection",
  "difficulty": "Easy",
  "tags": [
    "Probability",
    "Optimization"
  ],
  "descriptionMarkdown": "Implement the Upper Confidence Bound (UCB) action selection strategy for the multi-armed bandit problem.\n\nGiven:\n- counts: number of times each action has been selected (length N)\n- values: average rewards for each action (length N)\n- t: current timestep (t >= 1)\n- c: exploration coefficient\n\nReturn the index of the action to select using the UCB1 rule:\n- If any action has not been tried yet (count == 0), select one of them (e.g., the first).\n- Otherwise select argmax_i [ values[i] + c * sqrt(ln(t) / counts[i]) ].\n\nExample:\n- counts = [1, 1, 1, 1]\n- values = [1.0, 2.0, 1.5, 0.5]\n- t = 4, c = 2.0\n- All actions have the same confidence bonus; action 1 has the highest average reward, so it is chosen.",
  "solutionExplanation": "Upper Confidence Bound (UCB1) is a principled way to balance exploration and exploitation in multi-armed bandits. It augments each action's empirical mean reward with a confidence bonus that shrinks as the action is sampled more often and grows with the total number of trials. Concretely, the score for an action i is values[i] + c * sqrt(ln(t) / counts[i]). The logarithmic term encourages occasional exploration, while the 1/sqrt(counts[i]) term reduces the uncertainty the more we sample an arm.\n\nA practical detail is handling untried actions: when counts[i] == 0, the UCB bonus would be undefined. The common strategy is to immediately select any untried action first, ensuring each action is explored at least once. After every action has been tried, we compute the UCB scores and pick the action with the maximal score. The exploration coefficient c tunes the exploration-exploitation balance; larger c encourages more exploration.",
  "solutionCode": "import torch\n\ndef ucb_action(counts, values, t, c=2.0):\n    \"\"\"\n    Select an action using the UCB1 formula.\n\n    Args:\n        counts: 1D array-like of length N; number of times each action has been chosen\n        values: 1D array-like of length N; average reward of each action\n        t: int, current timestep (starts from 1)\n        c: float, exploration coefficient\n\n    Returns:\n        int: index of action to select\n    \"\"\"\n    counts_t = torch.as_tensor(counts)\n    values_t = torch.as_tensor(values, dtype=torch.float32)\n\n    if counts_t.ndim != 1 or values_t.ndim != 1 or counts_t.numel() != values_t.numel():\n        raise ValueError(\"counts and values must be 1D tensors of the same length\")\n\n    # If any action has not been tried, select the first such action\n    zero_mask = counts_t == 0\n    if torch.any(zero_mask):\n        return int(torch.nonzero(zero_mask, as_tuple=False)[0].item())\n\n    counts_f = counts_t.to(dtype=torch.float32)\n    # Guard t to be at least 1 to avoid log(0)\n    t_f = torch.clamp(torch.tensor(float(t), dtype=torch.float32), min=1.0)\n\n    exploration = torch.sqrt(torch.log(t_f) / counts_f)\n    ucb_scores = values_t + float(c) * exploration\n\n    return int(torch.argmax(ucb_scores).item())\n\n\ndef solution():\n    # Example usage\n    counts = torch.tensor([1, 1, 1, 1])\n    values = torch.tensor([1.0, 2.0, 1.5, 0.5])\n    t = 4\n    c = 2.0\n    action = ucb_action(counts, values, t, c)\n    print(action)\n\nif __name__ == \"__main__\":\n    solution()\n",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(N)",
  "platform": "deepml"
};
