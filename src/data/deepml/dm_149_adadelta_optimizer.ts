import { Problem } from '../../types';

export const DM_149_ADADELTA_OPTIMIZER: Problem = {
  "id": "dm_149_adadelta_optimizer",
  "title": "Adadelta Optimizer",
  "difficulty": "Medium",
  "tags": [
    "Optimization",
    "Gradient Descent",
    "Neural Networks"
  ],
  "descriptionMarkdown": "Implement the Adadelta optimizer update step function. The function should take the current parameter value, its gradient, and the moving averages of squared gradients (u) and squared parameter updates (v). It should return the updated parameter value along with the new moving averages.\n\nRequirements:\n- Support both scalar and array/tensor inputs (broadcasting allowed).\n- Include proper input validation (e.g., rho in [0, 1), epsilon > 0, broadcastable shapes).\n\nExample:\n- Input: parameter = 1.0, grad = 0.1, u = 1.0, v = 1.0, rho = 0.95, epsilon = 1e-6\n- Output: (0.89743, 0.9505, 0.95053)\n\nReturn a function that performs one Adadelta update step and demonstrates usage with the example.",
  "solutionExplanation": "AdaDelta adapts learning rates per-parameter by maintaining two exponentially decaying averages: u for squared gradients and v for squared parameter updates. At each step, we update u with the current gradient, compute a scale-invariant step using the root-mean-square (RMS) of past updates over the RMS of recent gradients, and then update the parameter. The update magnitude is further tracked in v.\n\nMathematically, given gradient g, decay rho in [0, 1), and small epsilon > 0 for numerical stability:\n- u_new = rho * u + (1 - rho) * g^2\n- step = - (sqrt(v + epsilon) / sqrt(u_new + epsilon)) * g\n- param_new = param + step\n- v_new = rho * v + (1 - rho) * step^2\n\nThis eliminates the need for a global learning rate and provides units-consistent updates. The implementation validates inputs, supports scalar and tensor inputs via broadcasting, and returns updated parameter and moving averages.",
  "solutionCode": "import torch\nimport torch.nn as nn\nfrom typing import Tuple, Union\n\nNumber = Union[int, float]\n\n\ndef adadelta_update(\n    parameter: Union[torch.Tensor, Number],\n    grad: Union[torch.Tensor, Number],\n    u: Union[torch.Tensor, Number],\n    v: Union[torch.Tensor, Number],\n    rho: float = 0.95,\n    epsilon: float = 1e-6,\n) -> Tuple[Union[torch.Tensor, float], Union[torch.Tensor, float], Union[torch.Tensor, float]]:\n    \"\"\"\n    Perform one AdaDelta update step.\n\n    Args:\n        parameter: Current parameter value(s), scalar or tensor.\n        grad: Current gradient(s), scalar or tensor.\n        u: Running average of squared gradients, same shape or broadcastable with parameter.\n        v: Running average of squared parameter updates, same shape or broadcastable with parameter.\n        rho: Decay rate for moving averages (0 <= rho < 1).\n        epsilon: Small constant for numerical stability (> 0).\n\n    Returns:\n        A tuple (param_new, u_new, v_new). If all inputs are Python scalars, returns Python floats.\n    \"\"\"\n    # Validate rho and epsilon\n    if not (0.0 <= float(rho) < 1.0):\n        raise ValueError(\"rho must satisfy 0 <= rho < 1\")\n    if not (float(epsilon) > 0.0):\n        raise ValueError(\"epsilon must be > 0\")\n\n    default_dtype = torch.get_default_dtype()\n\n    # Convert inputs to tensors with a consistent dtype\n    p_t = torch.as_tensor(parameter, dtype=default_dtype)\n    g_t = torch.as_tensor(grad, dtype=default_dtype)\n    u_t = torch.as_tensor(u, dtype=default_dtype)\n    v_t = torch.as_tensor(v, dtype=default_dtype)\n\n    # Ensure broadcastability\n    try:\n        torch.broadcast_shapes(p_t.shape, g_t.shape, u_t.shape, v_t.shape)\n    except RuntimeError as e:\n        raise ValueError(f\"parameter, grad, u, and v must be broadcastable: {e}\")\n\n    rho_t = torch.as_tensor(rho, dtype=default_dtype)\n    one_minus_rho = 1.0 - rho_t\n\n    # Update running average of squared gradients\n    u_new = rho_t * u_t + one_minus_rho * (g_t * g_t)\n\n    # Compute adaptive step\n    rms_delta = torch.sqrt(v_t + epsilon)\n    rms_grad = torch.sqrt(u_new + epsilon)\n    step = - (rms_delta / rms_grad) * g_t\n\n    # Update parameter\n    p_new = p_t + step\n\n    # Update running average of squared parameter updates\n    v_new = rho_t * v_t + one_minus_rho * (step * step)\n\n    # If all inputs were Python numbers and result is scalar, return Python floats\n    if all(not isinstance(x, torch.Tensor) for x in (parameter, grad, u, v)) and p_new.numel() == 1:\n        return p_new.item(), u_new.item(), v_new.item()\n\n    return p_new, u_new, v_new\n\n\ndef solution():\n    # Example usage based on the prompt\n    param, grad, u, v = 1.0, 0.1, 1.0, 1.0\n    p_new, u_new, v_new = adadelta_update(param, grad, u, v, rho=0.95, epsilon=1e-6)\n    # For the given example, expected approximate outputs:\n    # p_new ~ 0.89743, u_new ~ 0.9505, v_new ~ 0.95053\n    return p_new, u_new, v_new\n",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(1)",
  "platform": "deepml"
};
