import { Problem } from '../../types';

export const DM_140_BERNOULLI_NAIVE_BAYES_CLASSIFIER: Problem = {
  "id": "dm_140_bernoulli_naive_bayes_classifier",
  "title": "Bernoulli Naive Bayes Classifier",
  "difficulty": "Medium",
  "tags": [
    "Probability",
    "Matrix Operations"
  ],
  "descriptionMarkdown": "Write a Python class to implement the Bernoulli Naive Bayes classifier for binary (0/1) feature data. Your class should have two methods: `forward(self, X, y)` to train on the input data (`X`: 2D NumPy array of binary features, `y`: 1D NumPy array of class labels) and `predict(self, X)` to output predicted labels for a 2D test matrix `X`. Use Laplace smoothing (parameter: `smoothing=1.0`). Return predictions as a NumPy array. Predictions must be binary (0 or 1) and you must handle cases where the training data contains only one class. All log/likelihood calculations should use log probabilities for numerical stability.\n\nExample:\n\nInput:\n```\nX = np.array([[1, 0, 1], [1, 1, 0], [0, 0, 1], [0, 1, 0], [1, 1, 1]]); y = np.array([1, 1, 0, 0, 1])\nmodel = NaiveBayes(smoothing=1.0)\nmodel.forward(X, y)\nprint(model.predict(np.array([[1, 0, 1]])))\n```\n\nReasoning:\nThe model learns class priors and feature probabilities with Laplace smoothing. For `[1, 0, 1]`, the posterior for class 1 is higher, so the model predicts 1.",
  "solutionExplanation": "Bernoulli Naive Bayes models each binary feature independently given the class label. For class c in {0,1} and feature j, we estimate \u03b8_cj = P(x_j=1 | y=c). With Laplace smoothing \u03b1, \u03b8_cj = (count of ones for feature j in class c + \u03b1) / (N_c + 2\u03b1), where N_c is the number of samples in class c. Smoothing ensures probabilities never become 0 or 1, avoiding log(0) and improving numerical stability. We also smooth the class prior: P(y=c) = (N_c + \u03b1) / (N + 2\u03b1), which naturally handles the case when the training data contains only one class.\n\nFor prediction, we compute the log-posterior for each class using log probabilities for stability: log P(y=c) + \u03a3_j [x_j log \u03b8_cj + (1 \u2212 x_j) log(1 \u2212 \u03b8_cj)]. The predicted label is the class with the larger log-posterior. All operations can be vectorized efficiently across samples and classes using tensor operations.\n\nHandling edge cases: If a class is absent in training (N_c = 0), the smoothed estimates reduce to \u03b8_cj = 0.5 and a finite prior, allowing valid predictions rather than degenerate behavior. Using double precision and clamping probabilities before taking logs further improves numerical robustness.",
  "solutionCode": "import torch\nimport torch.nn as nn\nfrom typing import Optional\n\nclass NaiveBayes(nn.Module):\n    \"\"\"\n    Bernoulli Naive Bayes for binary features using Laplace smoothing.\n    Methods:\n      - forward(X, y): fit model parameters from training data\n      - predict(X): predict binary class labels for test data\n    Notes:\n      - Accepts NumPy arrays; internally uses PyTorch tensors.\n      - Returns predictions as a NumPy array of ints (0/1).\n    \"\"\"\n    def __init__(self, smoothing: float = 1.0):\n        super().__init__()\n        if smoothing <= 0:\n            raise ValueError(\"smoothing must be positive\")\n        self.smoothing = float(smoothing)\n        self.register_buffer('log_prior', None)\n        self.register_buffer('log_theta', None)\n        self.register_buffer('log_1_minus_theta', None)\n        self.n_features: Optional[int] = None\n\n    def forward(self, X, y):\n        \"\"\"Fit the model.\n        Args:\n            X: 2D NumPy array of shape (N, D) with binary (0/1) features.\n            y: 1D NumPy array of shape (N,) with labels in {0,1}.\n        \"\"\"\n        # Convert to torch tensors (double precision for numeric stability)\n        X_t = torch.as_tensor(X, dtype=torch.float64)\n        y_t = torch.as_tensor(y, dtype=torch.int64)\n\n        if X_t.dim() != 2:\n            raise ValueError(\"X must be a 2D array\")\n        if y_t.dim() != 1 or y_t.shape[0] != X_t.shape[0]:\n            raise ValueError(\"y must be a 1D array with same number of samples as X\")\n        if torch.any((X_t != 0) & (X_t != 1)):\n            raise ValueError(\"X must contain only binary values 0/1 for Bernoulli NB\")\n        if torch.any((y_t != 0) & (y_t != 1)):\n            raise ValueError(\"y must contain only class labels 0 or 1\")\n\n        N, D = X_t.shape\n        self.n_features = D\n        alpha = self.smoothing\n\n        # Class counts and smoothed priors\n        counts = torch.bincount(y_t, minlength=2).to(dtype=torch.float64)\n        priors = (counts + alpha) / (N + 2.0 * alpha)\n        eps = 1e-12\n        self.log_prior = torch.log(priors.clamp_min(eps))\n\n        # Conditional probabilities P(x_j=1 | y=c) with Laplace smoothing\n        theta = torch.empty((2, D), dtype=torch.float64)\n        for c in (0, 1):\n            mask = (y_t == c)\n            # Sum over features for the class; if no samples, sum is zeros\n            if mask.any():\n                sum_c = X_t[mask].sum(dim=0)\n            else:\n                sum_c = torch.zeros(D, dtype=torch.float64, device=X_t.device)\n            denom = counts[c] + 2.0 * alpha  # Nc + 2*alpha\n            theta[c] = (sum_c + alpha) / denom\n\n        # Clamp to avoid log(0) and log(1)\n        theta = theta.clamp(min=eps, max=1.0 - eps)\n        self.log_theta = torch.log(theta)\n        self.log_1_minus_theta = torch.log(1.0 - theta)\n\n        return self\n\n    @torch.no_grad()\n    def predict(self, X):\n        \"\"\"Predict labels for test data X.\n        Args:\n            X: 2D NumPy array of shape (M, D) with binary features.\n        Returns:\n            NumPy array of shape (M,) with predicted labels (0 or 1).\n        \"\"\"\n        if self.log_prior is None or self.log_theta is None:\n            raise RuntimeError(\"Model is not trained. Call forward(X, y) first.\")\n\n        X_t = torch.as_tensor(X, dtype=torch.float64)\n        if X_t.dim() != 2:\n            raise ValueError(\"X must be a 2D array\")\n        if self.n_features is not None and X_t.shape[1] != self.n_features:\n            raise ValueError(f\"Expected input with {self.n_features} features, got {X_t.shape[1]}\")\n        if torch.any((X_t != 0) & (X_t != 1)):\n            raise ValueError(\"X must contain only binary values 0/1 for Bernoulli NB\")\n\n        # Compute log-likelihoods for each class using vectorized operations\n        # shape manipulations yield (M, 2, D) then sum over D -> (M, 2)\n        term_on = X_t.unsqueeze(1) * self.log_theta\n        term_off = (1.0 - X_t).unsqueeze(1) * self.log_1_minus_theta\n        log_likelihood = (term_on + term_off).sum(dim=2)\n        log_posterior = log_likelihood + self.log_prior  # broadcast over batch\n\n        preds = torch.argmax(log_posterior, dim=1).to(torch.int64)\n        return preds.cpu().numpy()\n\n\ndef solution():\n    # Example usage\n    import numpy as np\n    X = np.array([[1, 0, 1], [1, 1, 0], [0, 0, 1], [0, 1, 0], [1, 1, 1]])\n    y = np.array([1, 1, 0, 0, 1])\n    model = NaiveBayes(smoothing=1.0)\n    model.forward(X, y)\n    preds = model.predict(np.array([[1, 0, 1]]))\n    # Return predictions so the caller can verify\n    return preds\n",
  "timeComplexity": "Training: O(N * D); Inference: O(M * D)",
  "spaceComplexity": "O(D)",
  "platform": "deepml"
};
