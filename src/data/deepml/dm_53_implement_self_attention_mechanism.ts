import { Problem } from '../../types';

export const DM_53_IMPLEMENT_SELF_ATTENTION_MECHANISM: Problem = {
  "id": "dm_53_implement_self_attention_mechanism",
  "title": "Implement Self-Attention Mechanism",
  "difficulty": "Medium",
  "tags": [
    "Transformers",
    "Attention",
    "Neural Networks",
    "Matrix Operations",
    "Linear Algebra"
  ],
  "descriptionMarkdown": "Task: Implement the Self-Attention Mechanism\n\nYour task is to implement the self-attention mechanism, a core component of transformer models used across NLP and CV. Self-attention allows a model to dynamically focus on different parts of an input sequence to build contextualized representations.\n\nYour function should return the self-attention output as a NumPy array.\n\nExample:\n\nInput:\n\n```\nimport numpy as np\nX = np.array([[1, 0], [0, 1]])\nW_q = np.array([[1, 0], [0, 1]])\nW_k = np.array([[1, 0], [0, 1]])\nW_v = np.array([[1, 2], [3, 4]])\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\noutput = self_attention(Q, K, V)\nprint(output)\n```\n\nOutput:\n\n```\n[[1.660477 2.660477]\n [2.339523 3.339523]]\n```\n\nReasoning:\nThe self-attention mechanism computes attention scores between all pairs of positions and forms a weighted sum of value vectors based on these scores, producing contextualized outputs.",
  "solutionExplanation": "Self-attention computes how much each position (token) should attend to every other position in the sequence. Given an input X and parameter matrices W_q, W_k, W_v, we first form queries Q = XW_q, keys K = XW_k, and values V = XW_v.\n\nThe attention scores are computed via scaled dot-product attention: scores = QK^T / sqrt(d_k), where d_k is the dimensionality of the key vectors. The scaling by sqrt(d_k) stabilizes gradients and prevents overly sharp softmax distributions as dimensions grow. We then apply a softmax over the last dimension of the scores to obtain attention weights that sum to 1 for each query position.\n\nFinally, the output is obtained by multiplying the attention weights with the values: output = softmax(scores) V. This yields a contextualized representation for each position as a weighted sum of all value vectors. The implementation below uses efficient PyTorch tensor operations throughout and returns a NumPy array for compatibility with the task requirement.",
  "solutionCode": "import torch\nimport torch.nn.functional as F\nimport numpy as np\n\n\ndef compute_qkv(X, W_q, W_k, W_v):\n    \"\"\"\n    Compute Q, K, V using matrix multiplications with provided weights.\n    Accepts NumPy arrays or PyTorch tensors and returns PyTorch tensors.\n    Shapes (2D expected):\n      - X:  (L, d_in)\n      - Wq: (d_in, d_k)\n      - Wk: (d_in, d_k)\n      - Wv: (d_in, d_v)\n    Returns:\n      - Q: (L, d_k)\n      - K: (L, d_k)\n      - V: (L, d_v)\n    \"\"\"\n    X_t = torch.as_tensor(X, dtype=torch.float32)\n    Wq_t = torch.as_tensor(W_q, dtype=torch.float32)\n    Wk_t = torch.as_tensor(W_k, dtype=torch.float32)\n    Wv_t = torch.as_tensor(W_v, dtype=torch.float32)\n\n    Q = X_t @ Wq_t\n    K = X_t @ Wk_t\n    V = X_t @ Wv_t\n    return Q, K, V\n\n\ndef self_attention(Q, K, V):\n    \"\"\"\n    Compute scaled dot-product self-attention.\n    Accepts Q, K, V as NumPy arrays or PyTorch tensors.\n    Returns a NumPy array with shape (L, d_v).\n    \"\"\"\n    Q_t = torch.as_tensor(Q, dtype=torch.float32)\n    K_t = torch.as_tensor(K, dtype=torch.float32)\n    V_t = torch.as_tensor(V, dtype=torch.float32)\n\n    if Q_t.dim() != 2 or K_t.dim() != 2 or V_t.dim() != 2:\n        raise ValueError(\"Q, K, V must be 2D tensors of shapes (L, d_k), (L, d_k), (L, d_v)\")\n    if Q_t.shape[0] != K_t.shape[0] or Q_t.shape[0] != V_t.shape[0]:\n        raise ValueError(\"Sequence lengths of Q, K, V must match in dimension 0\")\n    if Q_t.shape[1] != K_t.shape[1]:\n        raise ValueError(\"Key and Query dimensionalities (d_k) must match\")\n\n    d_k = Q_t.shape[-1]\n\n    # Scaled dot-product attention: softmax((QK^T) / sqrt(d_k)) V\n    scores = (Q_t @ K_t.transpose(0, 1)) / (d_k ** 0.5)\n    attn = torch.softmax(scores, dim=-1)\n    output = attn @ V_t\n\n    return output.detach().cpu().numpy()\n\n\ndef solution():\n    # Example usage matching the problem statement\n    X = np.array([[1, 0], [0, 1]], dtype=np.float32)\n    W_q = np.array([[1, 0], [0, 1]], dtype=np.float32)\n    W_k = np.array([[1, 0], [0, 1]], dtype=np.float32)\n    W_v = np.array([[1, 2], [3, 4]], dtype=np.float32)\n\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    output = self_attention(Q, K, V)\n\n    # Example print; function returns the NumPy array as required\n    # Expected (approximately):\n    # [[1.660477 2.660477]\n    #  [2.339523 3.339523]]\n    print(output)\n    return output\n",
  "timeComplexity": "O(L^2 * d)",
  "spaceComplexity": "O(L^2 + L * d)",
  "platform": "deepml"
};
