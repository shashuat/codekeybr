import { Problem } from '../../types';

export const DM_63_IMPLEMENT_THE_CONJUGATE_GRADIENT_METHOD_FOR_SOLVING_LINEAR_SYSTEMS: Problem = {
  "id": "dm_63_implement_the_conjugate_gradient_method_for_solving_linear_systems",
  "title": "Implement the Conjugate Gradient Method for Solving Linear Systems",
  "difficulty": "Hard",
  "tags": [
    "Linear Algebra",
    "Optimization",
    "Matrix Operations"
  ],
  "descriptionMarkdown": "Task: Implement the Conjugate Gradient (CG) method, an efficient iterative algorithm for solving large, sparse, symmetric, positive-definite linear systems. Given a matrix A and a vector b, the algorithm will solve for x in the system (Ax = b).\n\nImplement a function:\n\n- conjugate_gradient(A, b, n, x0=None, tol=1e-8)\n\nArguments:\n- A: A symmetric, positive-definite matrix representing the linear system.\n- b: The vector on the right side of the equation.\n- n: Maximum number of iterations.\n- x0: Initial guess for the solution vector (default is the zero vector).\n- tol: Tolerance for the stopping criterion based on the residual norm.\n\nThe function should return the solution vector x.\n\nExample:\n\nInput:\n- A = [[4, 1], [1, 3]]\n- b = [1, 2]\n- n = 5\n\nOutput:\n- [0.09090909, 0.63636364]\n\nReasoning:\nThe Conjugate Gradient method is applied to the linear system Ax = b with the given matrix A and vector b. The algorithm iteratively refines the solution to converge to the exact solution for symmetric positive-definite matrices.",
  "solutionExplanation": "The Conjugate Gradient (CG) method solves Ax = b for symmetric positive-definite (SPD) matrices by iteratively minimizing the quadratic function f(x) = 0.5 x^T A x \u2212 b^T x. Instead of performing a full matrix factorization, CG builds a sequence of search directions that are A-conjugate (mutually orthogonal under the A-inner product), ensuring efficient progress toward the solution.\n\nStarting from an initial guess x0 (default zero), the algorithm computes the residual r = b \u2212 A x and sets the first search direction p = r. At each iteration it computes a step size alpha = (r^T r) / (p^T A p), updates the solution x \u2190 x + alpha p, and the residual r \u2190 r \u2212 alpha A p. The next direction is formed as p \u2190 r + beta p where beta = (r_new^T r_new) / (r^T r). The process repeats until the residual norm falls below a tolerance (relative to ||b||) or the maximum number of iterations is reached. For exact arithmetic, CG converges in at most n steps on an n\u00d7n SPD system, but in practice it often converges much faster, especially for well-conditioned problems.\n\nThis implementation uses efficient PyTorch tensor operations (torch.mv for matrix-vector products and torch.dot for inner products), handles dtype/device consistently, and uses a relative residual stopping criterion for robustness. It returns the approximated solution vector x.",
  "solutionCode": "import torch\nimport torch.nn as nn\n\ndef conjugate_gradient(A: torch.Tensor, b: torch.Tensor, n: int, x0: torch.Tensor = None, tol: float = 1e-8) -> torch.Tensor:\n    \"\"\"\n    Solve Ax = b using the Conjugate Gradient method for symmetric positive-definite A.\n\n    Args:\n        A (torch.Tensor): [m, m] symmetric positive-definite matrix.\n        b (torch.Tensor): [m] right-hand side vector.\n        n (int): Maximum number of iterations.\n        x0 (torch.Tensor, optional): Initial guess [m]. If None, use zeros.\n        tol (float): Relative residual tolerance for convergence.\n\n    Returns:\n        torch.Tensor: Solution vector x of shape [m].\n    \"\"\"\n    if A.dim() != 2:\n        raise ValueError(\"A must be a 2D tensor (matrix).\")\n    if b.dim() != 1:\n        raise ValueError(\"b must be a 1D tensor (vector).\")\n    if A.size(0) != A.size(1) or A.size(0) != b.size(0):\n        raise ValueError(\"Incompatible shapes: A must be square and match b's size.\")\n\n    device = A.device\n    # Ensure floating dtype for computations; default to float64 for stability if needed\n    dtype = A.dtype if A.dtype.is_floating_point else torch.float64\n    A = A.to(device=device, dtype=dtype)\n    b = b.to(device=device, dtype=dtype)\n\n    # Initialize x\n    x = torch.zeros_like(b) if x0 is None else x0.to(device=device, dtype=dtype).clone()\n\n    # Initial residual and direction\n    r = b - torch.mv(A, x)\n    p = r.clone()\n\n    b_norm = torch.norm(b)\n    if b_norm == 0:\n        return torch.zeros_like(b)\n\n    rs_old = torch.dot(r, r)\n    if torch.sqrt(rs_old) <= tol * b_norm:\n        return x\n\n    for _ in range(int(n)):\n        Ap = torch.mv(A, p)\n        denom = torch.dot(p, Ap)\n        # Protect against breakdown (shouldn't happen for SPD A unless numerical issues)\n        if denom.abs() <= 1e-30:\n            break\n        alpha = rs_old / denom\n\n        x = x + alpha * p\n        r = r - alpha * Ap\n\n        rs_new = torch.dot(r, r)\n        if torch.sqrt(rs_new) <= tol * b_norm:\n            break\n\n        beta = rs_new / rs_old\n        p = r + beta * p\n        rs_old = rs_new\n\n    return x\n\n\ndef solution():\n    # Example usage\n    A = torch.tensor([[4.0, 1.0],\n                      [1.0, 3.0]])\n    b = torch.tensor([1.0, 2.0])\n    n = 5\n    x = conjugate_gradient(A, b, n, tol=1e-12)\n    print(x.tolist())  # Expected: [0.09090909..., 0.63636363...]\n",
  "timeComplexity": "O(k * nnz(A)) for sparse A; O(k * n^2) for dense A, where k is the number of iterations",
  "spaceComplexity": "O(n) additional memory for a few vectors (x, r, p, Ap), excluding storage for A",
  "platform": "deepml"
};
