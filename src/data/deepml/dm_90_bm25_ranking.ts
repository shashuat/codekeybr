import { Problem } from '../../types';

export const DM_90_BM25_RANKING: Problem = {
  "id": "dm_90_bm25_ranking",
  "title": "BM25 Ranking",
  "difficulty": "Medium",
  "tags": [
    "Probability"
  ],
  "descriptionMarkdown": "Implement the BM25 ranking function to calculate document scores for a query in an information retrieval context. BM25 is an advanced variation of TF-IDF that incorporates term frequency saturation, document length normalization, and a configurable penalty for document length effects.\n\nExample:\n- Input: corpus = [['the', 'cat', 'sat'], ['the', 'dog', 'ran'], ['the', 'bird', 'flew']], query = ['the', 'cat']\n- Output: [0.693, 0., 0.]\n\nBM25 calculates scores for each document in the corpus by evaluating how well the query terms match each document while considering term frequency saturation and document length normalization.",
  "solutionExplanation": "BM25 ranks documents by summing contributions from each query term. For a document d and query q, the score is the sum over terms t in q of IDF(t) multiplied by a saturation-normalized term frequency. The saturation is controlled by k1, and document-length normalization is controlled by b. This makes BM25 more robust than raw TF-IDF by preventing extremely frequent terms from dominating and by correcting for document length.\n\nFormally, score(d, q) = sum_t IDF(t) * ((tf(t, d) * (k1 + 1)) / (tf(t, d) + k1 * (1 - b + b * |d| / avgdl))). Here, tf(t, d) is the term frequency of t in d, |d| is the document length, and avgdl is the average document length across the corpus. IDF can be computed as log((N - df + 0.5) / (df + 0.5)), optionally clamped to avoid negative values for very common terms, or with a +1 inside the log to ensure positivity. We precompute per-document term counts to achieve efficient scoring over all query terms.",
  "solutionCode": "import torch\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75, epsilon=1e-12, idf_add_one=False, clamp_idf_min=0.0):\n    \"\"\"\n    Compute BM25 scores for a corpus of tokenized documents given a tokenized query.\n\n    Args:\n        corpus: List[List[str]] tokenized documents.\n        query: List[str] tokenized query terms.\n        k1: float, term frequency saturation parameter.\n        b: float, document length normalization parameter.\n        epsilon: small constant to avoid division by zero.\n        idf_add_one: if True, use IDF = log((N - df + 0.5)/(df + 0.5) + 1.0); else use standard IDF and optionally clamp.\n        clamp_idf_min: if not None, clamp IDF at this minimum (e.g., 0.0) to avoid negative contributions.\n\n    Returns:\n        torch.Tensor of shape (N,) with BM25 scores per document.\n    \"\"\"\n    N = len(corpus)\n    if N == 0:\n        return torch.empty(0, dtype=torch.float32)\n\n    # Precompute document lengths and average length\n    doc_lengths = torch.tensor([len(doc) for doc in corpus], dtype=torch.float32)\n    avgdl = doc_lengths.mean() if N > 0 else torch.tensor(0.0, dtype=torch.float32)\n\n    # Precompute doc-side counters for O(N*L) preprocessing and O(N) per-term scoring\n    doc_counters = [Counter(doc) for doc in corpus]\n\n    # Precompute K component for each document: k1 * (1 - b + b * |d| / avgdl)\n    K = k1 * (1.0 - b + b * (doc_lengths / (avgdl + epsilon)))\n\n    # Initialize scores\n    scores = torch.zeros(N, dtype=torch.float32)\n\n    # Query term counts (BM25 can weight repeated query terms)\n    q_counts = Counter(query)\n\n    for term, qf in q_counts.items():\n        # Term frequencies across documents for this term\n        tfs = torch.tensor([dc.get(term, 0) for dc in doc_counters], dtype=torch.float32)\n        # Document frequency\n        df = int((tfs > 0).sum().item())\n        if df == 0:\n            continue\n\n        # Inverse Document Frequency (IDF)\n        if idf_add_one:\n            idf = torch.log(((N - df + 0.5) / (df + 0.5)) + 1.0)\n        else:\n            idf = torch.log(((N - df + 0.5) / (df + 0.5)) + epsilon)\n            if clamp_idf_min is not None:\n                idf = torch.clamp(idf, min=clamp_idf_min)\n\n        # BM25 term contribution per document\n        numerator = tfs * (k1 + 1.0)\n        denominator = tfs + K + epsilon\n        contrib = idf * (numerator / denominator)\n\n        if qf > 1:\n            contrib = contrib * float(qf)\n\n        scores += contrib\n\n    return scores\n\n\ndef solution():\n    # Example usage\n    corpus = [['the', 'cat', 'sat'], ['the', 'dog', 'ran'], ['the', 'bird', 'flew']]\n    query = ['the', 'cat']\n\n    scores = calculate_bm25_scores(corpus, query, k1=1.5, b=0.75, idf_add_one=False, clamp_idf_min=0.0)\n    # Round to 3 decimals to mimic the problem's example formatting\n    rounded = torch.round(scores, decimals=3)\n    print(rounded.tolist())\n",
  "timeComplexity": "O(N * L + N * |Q|), where N is #documents, L is average document length, and |Q| is #unique query terms",
  "spaceComplexity": "O(N + V), where V is the number of unique terms stored in per-document counters",
  "platform": "deepml"
};
