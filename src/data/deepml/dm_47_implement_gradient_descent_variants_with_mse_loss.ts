import { Problem } from '../../types';

export const DM_47_IMPLEMENT_GRADIENT_DESCENT_VARIANTS_WITH_MSE_LOSS: Problem = {
  "id": "dm_47_implement_gradient_descent_variants_with_mse_loss",
  "title": "Implement Gradient Descent Variants with MSE Loss",
  "difficulty": "Medium",
  "tags": [
    "Optimization",
    "Loss Functions",
    "Gradient Descent",
    "Matrix Operations",
    "Linear Algebra"
  ],
  "descriptionMarkdown": "In this problem, you need to implement a single function that performs three variants of gradient descent\u2014Stochastic Gradient Descent (SGD), Batch Gradient Descent, and Mini-Batch Gradient Descent\u2014using Mean Squared Error (MSE) as the loss function for a linear model. The function should accept an additional parameter to specify which variant to use. Do not shuffle the data.\n\nExample:\n\n- Input:\n  - X = [[1, 1], [2, 1], [3, 1], [4, 1]]\n  - y = [2, 3, 4, 5]\n  - learning_rate = 0.01\n  - n_iterations = 1000\n  - batch_size = 2\n  - weights initialized to zeros\n\n- Calls:\n  - gradient_descent(X, y, weights, learning_rate, n_iterations, method='batch')\n  - gradient_descent(X, y, weights, learning_rate, n_iterations, method='stochastic')\n  - gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method='mini_batch')\n\nThe function should return the final weights after performing the specified variant of gradient descent.",
  "solutionExplanation": "We optimize a linear model y\u0302 = Xw using Mean Squared Error (MSE) loss: L = (1/m) \u03a3 (y\u0302 \u2212 y)^2. The gradient of the MSE with respect to the weights w for a batch of size m is \u2207w = (2/m) X\u1d40(Xw \u2212 y). The update rule is w \u2190 w \u2212 \u03b7 \u2207w, where \u03b7 is the learning rate. We implement this gradient and update using PyTorch tensor operations.\n\nThe three variants differ by how they choose the data subset per update: (1) Batch Gradient Descent computes the gradient on the full dataset each step for n_iterations steps. (2) Stochastic Gradient Descent (SGD) updates the weights using a single sample at a time, iterating sequentially over the dataset for n_iterations epochs, without shuffling. (3) Mini-Batch Gradient Descent splits the dataset into contiguous chunks of size batch_size and performs updates for each chunk per epoch for n_iterations epochs, also without shuffling. All computations use PyTorch tensors to ensure correct and efficient linear algebra operations.",
  "solutionCode": "import torch\nimport torch.nn as nn\nfrom typing import Union, Sequence\n\ndef _to_tensor(x, dtype=torch.float32):\n    return torch.as_tensor(x, dtype=dtype)\n\n\ndef _mse_grad(Xb: torch.Tensor, yb: torch.Tensor, w: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Compute gradient of MSE for linear model y_hat = Xb @ w.\n    Xb: (m, d), yb: (m,), w: (d,)\n    Returns: grad shape (d,)\n    \"\"\"\n    m = Xb.shape[0]\n    preds = Xb.matmul(w)  # (m,)\n    err = preds - yb      # (m,)\n    grad = (2.0 / m) * Xb.t().matmul(err)  # (d,)\n    return grad\n\n\ndef gradient_descent(\n    X: Union[torch.Tensor, Sequence[Sequence[float]]],\n    y: Union[torch.Tensor, Sequence[float]],\n    weights: Union[torch.Tensor, Sequence[float]],\n    learning_rate: float,\n    n_iterations: int,\n    batch_size: int = 1,\n    method: str = 'batch'\n) -> list:\n    \"\"\"\n    Perform gradient descent variants (batch, stochastic, mini_batch) with MSE loss.\n\n    Args:\n        X: Features of shape (N, D). Can be NumPy array, list, or torch.Tensor.\n        y: Targets of shape (N,). Can be NumPy array, list, or torch.Tensor.\n        weights: Initial weights of shape (D,).\n        learning_rate: Learning rate (eta).\n        n_iterations: Number of steps (batch) or epochs (stochastic/mini-batch).\n        batch_size: Batch size for mini-batch method (ignored for other methods).\n        method: One of {'batch', 'stochastic', 'mini_batch'}.\n\n    Returns:\n        Final weights as a Python list of floats.\n\n    Notes:\n        - Data is NOT shuffled (processed in given order).\n        - Uses PyTorch tensor operations under the hood.\n    \"\"\"\n    if method not in {\"batch\", \"stochastic\", \"mini_batch\"}:\n        raise ValueError(\"method must be one of {'batch', 'stochastic', 'mini_batch'}\")\n    if learning_rate <= 0:\n        raise ValueError(\"learning_rate must be positive\")\n    if n_iterations <= 0:\n        raise ValueError(\"n_iterations must be positive\")\n    if method == 'mini_batch' and batch_size <= 0:\n        raise ValueError(\"batch_size must be positive for mini_batch\")\n\n    # Convert to tensors\n    X_t = _to_tensor(X)\n    y_t = _to_tensor(y).view(-1)\n    w = _to_tensor(weights).view(-1).clone()  # clone to avoid mutating the input\n\n    N, D = X_t.shape\n    if w.shape[0] != D:\n        raise ValueError(f\"weights must have shape ({D},), got {tuple(w.shape)}\")\n\n    if method == 'batch':\n        for _ in range(n_iterations):\n            grad = _mse_grad(X_t, y_t, w)\n            w -= learning_rate * grad\n\n    elif method == 'stochastic':\n        # Iterate over data in order for each epoch, no shuffling\n        for _ in range(n_iterations):\n            for i in range(N):\n                Xi = X_t[i:i+1, :]         # (1, D)\n                yi = y_t[i:i+1]            # (1,)\n                grad = _mse_grad(Xi, yi, w)\n                w -= learning_rate * grad\n\n    else:  # mini_batch\n        for _ in range(n_iterations):\n            for start in range(0, N, batch_size):\n                end = min(start + batch_size, N)\n                Xb = X_t[start:end, :]\n                yb = y_t[start:end]\n                grad = _mse_grad(Xb, yb, w)\n                w -= learning_rate * grad\n\n    return w.detach().cpu().tolist()\n\n\ndef solution():\n    # Example usage matching the prompt\n    import numpy as np\n    X = np.array([[1, 1], [2, 1], [3, 1], [4, 1]], dtype=float)\n    y = np.array([2, 3, 4, 5], dtype=float)\n\n    learning_rate = 0.01\n    n_iterations = 1000\n    batch_size = 2\n\n    weights = np.zeros(X.shape[1], dtype=float)\n\n    w_batch = gradient_descent(X, y, weights, learning_rate, n_iterations, method='batch')\n    w_stoch = gradient_descent(X, y, weights, learning_rate, n_iterations, method='stochastic')\n    w_mini = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=batch_size, method='mini_batch')\n\n    # Example prints (will look like [float, float])\n    print(w_batch)\n    print(w_stoch)\n    print(w_mini)\n\n    return w_batch\n",
  "timeComplexity": "O(T * N * D) where T is the number of iterations/epochs, N is the number of samples, and D is the number of features",
  "spaceComplexity": "O(D) for storing the weight vector and intermediate gradients",
  "platform": "deepml"
};
