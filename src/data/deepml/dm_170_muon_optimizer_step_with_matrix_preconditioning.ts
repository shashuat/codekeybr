import { Problem } from '../../types';

export const DM_170_MUON_OPTIMIZER_STEP_WITH_MATRIX_PRECONDITIONING: Problem = {
  "id": "dm_170_muon_optimizer_step_with_matrix_preconditioning",
  "title": "Muon Optimizer Step with Matrix Preconditioning",
  "difficulty": "Medium",
  "tags": [
    "Optimization",
    "Gradient Descent",
    "Matrix Operations",
    "Linear Algebra"
  ],
  "descriptionMarkdown": "Implement a single update step of the Muon optimizer for a 2D parameter matrix. The Muon optimizer combines momentum with a matrix preconditioning step based on the Newton\u2013Schulz iteration to approximately whiten (or orthogonalize) the update direction.\n\nRequirements:\n- Update the momentum using the current gradient and previous momentum.\n- Apply a Newton\u2013Schulz matrix iteration (order 5; use a fixed number of steps) to precondition the update direction. This involves:\n  - Normalizing the update for numerical stability.\n  - Transposing when the matrix is wide to operate on the smaller Gram matrix.\n  - Running the fixed matrix iteration for a specified number of steps.\n- Use a scale factor based on the RMS operator norm for stability.\n- Update the parameters using the preconditioned direction, learning rate, and scale.\n- Return both the updated parameter matrix and momentum.\n\nExample:\n- Input:\n  - theta = I (2x2 identity), B = zeros(2x2), grad = ones(2x2), eta = 0.1, mu = 0.9, ns_steps=2\n- Output (rounded):\n  - [[ 0.944 -0.056]\n     [-0.056  0.944]]\n\nAfter the momentum and Newton\u2013Schulz preconditioning, the parameters are updated in the direction of the scaled, preconditioned matrix.",
  "solutionExplanation": "Muon augments standard momentum with a matrix-wise preconditioner that approximates whitening of the update directions. First, we update the momentum buffer B using the current gradient. A common and effective choice is B \u2190 \u03bcB + g, which accumulates gradients with exponential decay \u03bc.\n\nTo precondition the update, we approximate X \u00b7 (X^T X)^{-1/2} (for tall matrices) or (X X^T)^{-1/2} \u00b7 X (for wide matrices), where X is the (normalized) update matrix. Directly computing an inverse square root is expensive, so we use the Newton\u2013Schulz iteration. Given a positive semi-definite matrix A, the Newton\u2013Schulz iteration produces A^{-1/2} via a sequence of matrix multiplies using Y and Z updates: T = 0.5(3I \u2212 ZY), Y \u2190 YT, Z \u2190 TZ, with an initial normalization to keep the spectrum in a numerically stable range. We then apply the resulting inverse square root to the update.\n\nFinally, we scale the preconditioned direction by a stability factor based on the RMS operator norm (we use RMS of singular values, i.e., Frobenius norm divided by sqrt(min(M, N))). The parameter update is \u03b8 \u2190 \u03b8 \u2212 \u03b7 \u00b7 (scaled, preconditioned update). This yields a normalized, shape-aware step that adapts to correlations in the update matrix without requiring an explicit eigendecomposition.",
  "solutionCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef _matrix_inv_sqrt_newton_schulz(A: torch.Tensor, num_iters: int = 5, eps: float = 1e-7) -> torch.Tensor:\n    \"\"\"Compute A^{-1/2} for a symmetric positive semidefinite matrix A using\n    the Newton\u2013Schulz iteration with input normalization.\n\n    Args:\n        A: (..., n, n) SPD/PSD matrix (batched or single).\n        num_iters: number of Newton\u2013Schulz iterations.\n        eps: numerical stability epsilon.\n\n    Returns:\n        (..., n, n) tensor approximating A^{-1/2}.\n    \"\"\"\n    assert A.dim() >= 2 and A.size(-1) == A.size(-2), \"A must be square\"\n    n = A.size(-1)\n    I = torch.eye(n, device=A.device, dtype=A.dtype).expand(A.shape)\n\n    # Normalize A to improve convergence\n    # Use Frobenius norm; add eps to avoid division by zero\n    fro_norm = torch.linalg.norm(A, ord='fro', dim=(-2, -1), keepdim=True)\n    Y = A / (fro_norm + eps)\n    Z = I.clone()\n\n    for _ in range(num_iters):\n        T = 0.5 * (3.0 * I - Z @ Y)\n        Y = Y @ T\n        Z = T @ Z\n\n    inv_sqrt_A = Z / torch.sqrt(fro_norm + eps)\n    return inv_sqrt_A\n\n\ndef _precondition_ns(update: torch.Tensor, ns_steps: int = 5, eps: float = 1e-7) -> torch.Tensor:\n    \"\"\"Precondition a 2D update matrix using Newton\u2013Schulz inverse-square-root whitening.\n\n    For tall matrices (M >= N):  P = X @ (X^T X)^{-1/2}\n    For wide matrices (M < N):   P = (X X^T)^{-1/2} @ X\n\n    The update is normalized before forming the Gram matrix; the result is then\n    rescaled by an RMS operator-norm proxy (Frobenius norm / sqrt(min(M, N))).\n\n    Args:\n        update: (M, N) tensor\n        ns_steps: Newton\u2013Schulz iterations\n        eps: stability term\n\n    Returns:\n        (M, N) preconditioned update tensor\n    \"\"\"\n    assert update.dim() == 2, \"update must be a 2D tensor\"\n    M, N = update.shape\n\n    # RMS operator-norm proxy: sqrt(mean of squared singular values))\n    # = Frobenius norm / sqrt(rank bound) ~ Fro / sqrt(min(M, N))\n    fro = torch.linalg.norm(update, ord='fro')\n    rms_op = fro / (float(min(M, N)) ** 0.5 + eps)\n\n    # Normalize the update prior to NS to stabilize the Gram spectrum\n    X = update / (rms_op + eps)\n\n    if M >= N:\n        # Use right-whitening on the smaller Gram (N x N)\n        G = X.T @ X + eps * torch.eye(N, device=update.device, dtype=update.dtype)\n        inv_sqrt = _matrix_inv_sqrt_newton_schulz(G, num_iters=ns_steps, eps=eps)\n        P = X @ inv_sqrt\n    else:\n        # Use left-whitening on the smaller Gram (M x M)\n        G = X @ X.T + eps * torch.eye(M, device=update.device, dtype=update.dtype)\n        inv_sqrt = _matrix_inv_sqrt_newton_schulz(G, num_iters=ns_steps, eps=eps)\n        P = inv_sqrt @ X\n\n    # Rescale back by RMS operator norm for a stable step magnitude\n    return P * rms_op\n\n\ndef muon_step(theta: torch.Tensor,\n              B: torch.Tensor,\n              grad: torch.Tensor,\n              eta: float,\n              mu: float,\n              ns_steps: int = 5,\n              eps: float = 1e-7):\n    \"\"\"Perform one Muon optimizer step on a 2D parameter matrix.\n\n    Args:\n        theta: (M, N) parameter matrix\n        B: (M, N) momentum buffer\n        grad: (M, N) current gradient\n        eta: learning rate (float)\n        mu: momentum coefficient in [0, 1)\n        ns_steps: Newton\u2013Schulz iterations for preconditioning\n        eps: numerical stability epsilon\n\n    Returns:\n        theta_new, B_new\n    \"\"\"\n    assert theta.shape == grad.shape == B.shape, \"theta, grad, B must have same shape\"\n\n    # Momentum update: accumulate gradients with decay\n    # (Using the common convention B <- mu * B + grad)\n    B_new = mu * B + grad\n\n    # Matrix preconditioning via Newton\u2013Schulz inverse square root whitening\n    precond = _precondition_ns(B_new, ns_steps=ns_steps, eps=eps)\n\n    # Parameter update\n    theta_new = theta - eta * precond\n    return theta_new, B_new\n\n\ndef solution():\n    # Example usage\n    device = 'cpu'\n    dtype = torch.float64\n\n    theta = torch.eye(2, dtype=dtype, device=device)\n    B = torch.zeros((2, 2), dtype=dtype, device=device)\n    grad = torch.ones((2, 2), dtype=dtype, device=device)\n\n    eta = 0.1\n    mu = 0.9\n    ns_steps = 2\n\n    theta_new, B_new = muon_step(theta, B, grad, eta, mu, ns_steps=ns_steps)\n    print(torch.round(theta_new, decimals=3))\n\n\nif __name__ == '__main__':\n    solution()\n",
  "timeComplexity": "O(ns_steps * M * N * min(M, N))",
  "spaceComplexity": "O(M * N + min(M, N)^2)",
  "platform": "deepml"
};
