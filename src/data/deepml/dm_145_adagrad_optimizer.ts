import { Problem } from '../../types';

export const DM_145_ADAGRAD_OPTIMIZER: Problem = {
  "id": "dm_145_adagrad_optimizer",
  "title": "Adagrad Optimizer",
  "difficulty": "Easy",
  "tags": [
    "Optimization",
    "Gradient Descent",
    "Backpropagation",
    "Neural Networks"
  ],
  "descriptionMarkdown": "Implement the Adagrad optimizer update step function. Your function should take the current parameter value, gradient, and accumulated squared gradients (G) as inputs, and return the updated parameter value and the new accumulated squared gradients. The function must handle both scalar and array/tensor inputs and include proper input validation.\n\nExample:\n- Input: parameter = 1.0, grad = 0.1, G = 1.0\n- Output: (0.999, 1.01)\n\nReasoning:\nAdagrad updates the accumulated squared gradients as G_new = G + grad^2 and updates the parameter using parameter_new = parameter - learning_rate * grad / (sqrt(G_new) + epsilon). With parameter=1.0, grad=0.1, and G=1.0 (learning_rate=0.01, epsilon=1e-8), we obtain parameter_new \u2248 0.999 and G_new = 1.01.",
  "solutionExplanation": "Adagrad adapts the learning rate per-parameter by accumulating the sum of squared gradients over time. This accumulation increases the denominator in the update rule for parameters that frequently receive large gradients, effectively shrinking their future update magnitudes. The update equations are:\n\n- G_new = G + grad^2\n- parameter_new = parameter \u2212 learning_rate \u00d7 grad / (sqrt(G_new) + epsilon)\n\nThe small constant epsilon ensures numerical stability to avoid division by zero. To support both scalar and array inputs, we convert inputs to PyTorch tensors and rely on broadcasting so that parameters, gradients, and accumulators can differ in shape but remain broadcast-compatible. We also validate hyperparameters and input finiteness. The function returns Python floats for scalar inputs and torch.Tensor objects otherwise.",
  "solutionCode": "import torch\nfrom typing import Any, Tuple\n\ndef adagrad_optimizer(parameter: Any,\n                      grad: Any,\n                      G: Any,\n                      learning_rate: float = 0.01,\n                      epsilon: float = 1e-8) -> Tuple[Any, Any]:\n    \"\"\"\n    Update parameters using the Adagrad optimizer.\n\n    Adapts the learning rate for each parameter based on historical gradients.\n\n    Args:\n        parameter: Current parameter value (Python scalar, list/tuple, numpy array, or torch.Tensor)\n        grad: Current gradient (broadcastable to parameter)\n        G: Accumulated squared gradients (broadcastable to parameter)\n        learning_rate: Learning rate (default=0.01, must be > 0)\n        epsilon: Small constant for numerical stability (default=1e-8, must be > 0)\n\n    Returns:\n        tuple: (updated_parameter, updated_G)\n            - If all inputs were Python scalars, returns Python floats.\n            - Otherwise, returns torch.Tensor objects with broadcasted shape.\n    \"\"\"\n    # Validate hyperparameters\n    if not isinstance(learning_rate, (int, float)):\n        raise TypeError(\"learning_rate must be a float or int\")\n    if learning_rate <= 0:\n        raise ValueError(\"learning_rate must be > 0\")\n    if not isinstance(epsilon, (int, float)):\n        raise TypeError(\"epsilon must be a float or int\")\n    if epsilon <= 0:\n        raise ValueError(\"epsilon must be > 0\")\n\n    # Convert inputs to tensors (CPU by default), preserving numeric types as float32\n    p_t = torch.as_tensor(parameter, dtype=torch.float32)\n    g_t = torch.as_tensor(grad, dtype=torch.float32)\n    G_t = torch.as_tensor(G, dtype=torch.float32)\n\n    # Validate numerical finiteness\n    if not torch.isfinite(p_t).all():\n        raise ValueError(\"parameter contains non-finite values\")\n    if not torch.isfinite(g_t).all():\n        raise ValueError(\"grad contains non-finite values\")\n    if not torch.isfinite(G_t).all():\n        raise ValueError(\"G contains non-finite values\")\n\n    # Broadcast to a common shape\n    try:\n        p_b, g_b, G_b = torch.broadcast_tensors(p_t, g_t, G_t)\n    except Exception as e:\n        raise ValueError(\"parameter, grad, and G must be broadcastable to a common shape\") from e\n\n    # Adagrad update\n    G_new = G_b + g_b.pow(2)\n    denom = torch.sqrt(G_new) + epsilon\n    p_new = p_b - (learning_rate * g_b) / denom\n\n    # Preserve scalar return type if all inputs were Python scalars\n    inputs_are_python_scalars = all(isinstance(x, (int, float)) for x in (parameter, grad, G))\n    if inputs_are_python_scalars and p_new.numel() == 1:\n        return (p_new.item(), G_new.item())\n\n    return (p_new, G_new)\n\n\ndef solution():\n    # Example usage\n    parameter = 1.0\n    grad = 0.1\n    G = 1.0\n    updated_param, updated_G = adagrad_optimizer(parameter, grad, G, learning_rate=0.01, epsilon=1e-8)\n    # For demonstration, return the results\n    return updated_param, updated_G\n\nif __name__ == \"__main__\":\n    up, uG = solution()\n    # Example expected: approximately (0.999, 1.01)\n    print(\"Updated parameter:\", up)\n    print(\"Updated G:\", uG)\n",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(N)",
  "platform": "deepml"
};
