import { Problem } from '../../types';

export const DM_157_IMPLEMENT_THE_BELLMAN_EQUATION_FOR_VALUE_ITERATION: Problem = {
  "id": "dm_157_implement_the_bellman_equation_for_value_iteration",
  "title": "Implement the Bellman Equation for Value Iteration",
  "difficulty": "Medium",
  "tags": [
    "Probability",
    "Optimization"
  ],
  "descriptionMarkdown": "Write a function that performs one step of value iteration for a given Markov Decision Process (MDP) using the Bellman equation. The function should update the state-value function V(s) for each state based on possible actions, transition probabilities, rewards, and the discount factor gamma.\n\nExample:\n\nInput:\n\n```\nimport numpy as np\ntransitions = [\n  {0: [(1.0, 0, 0.0, False)], 1: [(1.0, 1, 1.0, False)]},\n  {0: [(1.0, 0, 0.0, False)], 1: [(1.0, 1, 1.0, True)]}\n]\nV = np.array([0.0, 0.0])\ngamma = 0.9\nnew_V = bellman_update(V, transitions, gamma)\nprint(np.round(new_V, 2))\n```\n\nOutput:\n\n```\n[1. 1.]\n```\n\nReasoning:\n- For state 0, the best action is to go to state 1 and get a reward of 1.\n- For state 1, taking action 1 gives a reward of 1 and ends the episode, so its value is 1.",
  "solutionExplanation": "Value iteration relies on the Bellman optimality equation. For each state s, we compute the value as the maximum over actions of the expected return: V_{k+1}(s) = max_a sum_{s'} P(s'|s,a) [R(s,a,s') + gamma * V_k(s')]. If a transition leads to a terminal state, the bootstrap term gamma * V_k(s') is omitted (i.e., treated as zero), since the episode ends there.\n\nThe implementation iterates over all states and their available actions. For each action, it aggregates the expected value across all possible (probability, next_state, reward, done) outcomes, applying the discount factor gamma and masking out the next-state value when done is True. It then selects the maximum action-value for the state. Using PyTorch tensors ensures consistent tensor operations and makes the function easily extensible for GPU tensors if needed.",
  "solutionCode": "import torch\nfrom typing import List, Dict, Tuple\n\ndef bellman_update(V: torch.Tensor,\n                   transitions: List[Dict[int, List[Tuple[float, int, float, bool]]]],\n                   gamma: float) -> torch.Tensor:\n    \"\"\"Perform one step of value iteration using the Bellman optimality equation.\n\n    Args:\n        V: 1D torch.Tensor of shape (n_states,), current state values.\n        transitions: List of dicts; transitions[s][a] is a list of tuples\n            (prob, next_state, reward, done) describing the dynamics.\n        gamma: Discount factor in [0, 1].\n\n    Returns:\n        torch.Tensor: Updated state-value tensor of shape (n_states,).\n    \"\"\"\n    n_states = len(transitions)\n    assert V.dim() == 1 and V.shape[0] == n_states, \"V must be 1D and match number of states\"\n\n    # Allocate output on the same device/dtype as V\n    new_V = torch.empty_like(V)\n\n    for s in range(n_states):\n        action_dict = transitions[s]\n        if not action_dict:  # If a state has no actions, keep its value (or set to 0); here we keep it.\n            new_V[s] = V[s]\n            continue\n\n        action_values = []\n        for _, outcomes in action_dict.items():\n            ev = torch.tensor(0.0, dtype=V.dtype, device=V.device)\n            for prob, next_state, reward, done in outcomes:\n                # If done is True, no bootstrap from next state\n                ns_val = (0.0 if done else 1.0) * V[next_state]\n                ev = ev + prob * (reward + gamma * ns_val)\n            action_values.append(ev)\n\n        # Max over actions\n        max_val = torch.max(torch.stack(action_values))\n        new_V[s] = max_val\n\n    return new_V\n\nif __name__ == \"__main__\":\n    # Example usage (matches the problem's example semantics)\n    transitions = [\n        {0: [(1.0, 0, 0.0, False)], 1: [(1.0, 1, 1.0, False)]},\n        {0: [(1.0, 0, 0.0, False)], 1: [(1.0, 1, 1.0, True)]}\n    ]\n    V = torch.tensor([0.0, 0.0])\n    gamma = 0.9\n    new_V = bellman_update(V, transitions, gamma)\n    print(new_V.round(decimals=2))  # Expected: tensor([1., 1.])\n",
  "timeComplexity": "O(S * A * T) where S is the number of states, A the average actions per state, and T the transitions per action",
  "spaceComplexity": "O(S) for the updated value vector",
  "platform": "deepml"
};
