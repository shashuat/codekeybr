import { Problem } from '../../types';

export const DM_59_IMPLEMENT_LONG_SHORT_TERM_MEMORY__LSTM__NETWORK: Problem = {
  "id": "dm_59_implement_long_short_term_memory_lstm_network",
  "title": "Implement Long Short-Term Memory (LSTM) Network",
  "difficulty": "Medium",
  "tags": [
    "Neural Networks",
    "RNNs",
    "Activation Functions",
    "Backpropagation"
  ],
  "descriptionMarkdown": "Task: Implement Long Short-Term Memory (LSTM) Network\n\nYour task is to implement an LSTM network that processes a sequence of inputs and produces the final hidden state and cell state after processing all inputs.\n\nWrite a class `LSTM` with the following methods:\n- `__init__(self, input_size, hidden_size)`: Initializes the LSTM with random weights and zero biases.\n- `forward(self, x, initial_hidden_state, initial_cell_state)`: Processes a sequence of inputs and returns the hidden states at each time step, as well as the final hidden state and cell state.\n\nThe LSTM should compute the forget gate, input gate, candidate cell state, and output gate at each time step to update the hidden state and cell state.\n\nExample:\n\nInput:\n- `input_sequence = np.array([[1.0], [2.0], [3.0]])`\n- `initial_hidden_state = np.zeros((1, 1))`\n- `initial_cell_state = np.zeros((1, 1))`\n- `lstm = LSTM(input_size=1, hidden_size=1)`\n- `outputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)`\n- `print(final_h)`\n\nOutput (approximate):\n- `[[0.73698596]]`\n\nReasoning:\nThe LSTM processes the input sequence `[1.0, 2.0, 3.0]` and produces the final hidden state close to `[[0.73698596]]`.",
  "solutionExplanation": "An LSTM augments a standard RNN with gating mechanisms to better preserve long-term dependencies. At each time step t, it computes four quantities from the current input x_t and previous hidden state h_{t-1}: the forget gate f_t, input gate i_t, candidate cell g_t, and output gate o_t. These are formed by an affine transform on the concatenation [h_{t-1}, x_t], followed by nonlinearities: sigmoid for gates and tanh for the candidate.\n\nThe core update equations are:\n- f_t = \u03c3(W_f [h_{t-1}, x_t] + b_f)\n- i_t = \u03c3(W_i [h_{t-1}, x_t] + b_i)\n- g_t = tanh(W_c [h_{t-1}, x_t] + b_c)\n- o_t = \u03c3(W_o [h_{t-1}, x_t] + b_o)\n- c_t = f_t \u2299 c_{t-1} + i_t \u2299 g_t\n- h_t = o_t \u2299 tanh(c_t)\n\nWe implement these computations explicitly in PyTorch, initializing weights with a normal distribution and biases to zero as requested. The forward method iterates over the sequence, updating the cell and hidden states and collecting hidden states for all time steps. It accepts NumPy arrays or PyTorch tensors and returns: (1) the stacked hidden states for each time step, (2) the final hidden state, and (3) the final cell state.",
  "solutionCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass LSTM(nn.Module):\n    def __init__(self, input_size: int, hidden_size: int):\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        # Weights for gates: each maps concatenated [h_{t-1}, x_t] -> hidden_size\n        self.Wf = nn.Parameter(torch.randn(hidden_size, input_size + hidden_size))\n        self.Wi = nn.Parameter(torch.randn(hidden_size, input_size + hidden_size))\n        self.Wc = nn.Parameter(torch.randn(hidden_size, input_size + hidden_size))\n        self.Wo = nn.Parameter(torch.randn(hidden_size, input_size + hidden_size))\n        # Biases initialized to zero\n        self.bf = nn.Parameter(torch.zeros(hidden_size))\n        self.bi = nn.Parameter(torch.zeros(hidden_size))\n        self.bc = nn.Parameter(torch.zeros(hidden_size))\n        self.bo = nn.Parameter(torch.zeros(hidden_size))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Args:\n            x: Tensor or numpy array of shape (seq_len, input_size) or (seq_len, batch, input_size)\n            initial_hidden_state: Tensor or numpy array of shape (batch, hidden_size)\n            initial_cell_state: Tensor or numpy array of shape (batch, hidden_size)\n        Returns:\n            outputs: Tensor of shape (seq_len, batch, hidden_size)\n            final_h: Tensor of shape (batch, hidden_size)\n            final_c: Tensor of shape (batch, hidden_size)\n        \"\"\"\n        # Convert inputs to tensors if needed\n        if not isinstance(x, torch.Tensor):\n            try:\n                import numpy as np  # optional dependency for conversion\n                if isinstance(x, np.ndarray):\n                    x = torch.from_numpy(x.astype(\"float32\"))\n                else:\n                    x = torch.tensor(x, dtype=torch.float32)\n            except Exception:\n                x = torch.tensor(x, dtype=torch.float32)\n        if not isinstance(initial_hidden_state, torch.Tensor):\n            initial_hidden_state = torch.tensor(initial_hidden_state, dtype=torch.float32)\n        if not isinstance(initial_cell_state, torch.Tensor):\n            initial_cell_state = torch.tensor(initial_cell_state, dtype=torch.float32)\n\n        # Normalize input shape to (seq_len, batch, input_size)\n        if x.dim() == 2:\n            # (seq_len, input_size) -> (seq_len, 1, input_size)\n            x = x.unsqueeze(1)\n        elif x.dim() != 3:\n            raise ValueError(\"Input x must have shape (seq_len, input_size) or (seq_len, batch, input_size)\")\n\n        seq_len, batch_size, input_size = x.shape\n        assert input_size == self.input_size, f\"Expected input_size={self.input_size}, got {input_size}\"\n\n        h_t = initial_hidden_state\n        c_t = initial_cell_state\n        # Ensure shapes (batch, hidden_size)\n        if h_t.dim() == 1:\n            h_t = h_t.unsqueeze(0)\n        if c_t.dim() == 1:\n            c_t = c_t.unsqueeze(0)\n        assert h_t.shape == (batch_size, self.hidden_size), f\"h_0 shape must be {(batch_size, self.hidden_size)}, got {tuple(h_t.shape)}\"\n        assert c_t.shape == (batch_size, self.hidden_size), f\"c_0 shape must be {(batch_size, self.hidden_size)}, got {tuple(c_t.shape)}\"\n\n        outputs = []\n        for t in range(seq_len):\n            x_t = x[t]  # (batch, input_size)\n            concat = torch.cat([h_t, x_t], dim=1)  # (batch, hidden_size + input_size)\n\n            f_t = torch.sigmoid(concat @ self.Wf.T + self.bf)  # (batch, hidden_size)\n            i_t = torch.sigmoid(concat @ self.Wi.T + self.bi)\n            g_t = torch.tanh(concat @ self.Wc.T + self.bc)\n            o_t = torch.sigmoid(concat @ self.Wo.T + self.bo)\n\n            c_t = f_t * c_t + i_t * g_t\n            h_t = o_t * torch.tanh(c_t)\n\n            outputs.append(h_t.unsqueeze(0))\n\n        outputs = torch.cat(outputs, dim=0)  # (seq_len, batch, hidden_size)\n        return outputs, h_t, c_t\n\n\ndef solution():\n    \"\"\"Example usage returning the outputs and final states.\"\"\"\n    import numpy as np\n    # Example input sequence and initial states\n    input_sequence = np.array([[1.0], [2.0], [3.0]], dtype=np.float32)\n    initial_hidden_state = np.zeros((1, 1), dtype=np.float32)\n    initial_cell_state = np.zeros((1, 1), dtype=np.float32)\n\n    # Optional: set a seed for reproducibility (not required by the task)\n    torch.manual_seed(0)\n\n    lstm = LSTM(input_size=1, hidden_size=1)\n    outputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\n    return outputs, final_h, final_c\n",
  "timeComplexity": "O(T * H * (I + H)), where T is sequence length, I is input_size, and H is hidden_size",
  "spaceComplexity": "O(H * (I + H) + T * H) for parameters plus stored outputs; O(H) additional working memory during the forward pass",
  "platform": "deepml"
};
