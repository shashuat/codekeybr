import { Problem } from '../../types';

export const DM_60_IMPLEMENT_TF_IDF__TERM_FREQUENCY_INVERSE_DOCUMENT_FREQUENCY_: Problem = {
  "id": "dm_60_implement_tf_idf_term_frequency_inverse_document_frequency",
  "title": "Implement TF-IDF (Term Frequency-Inverse Document Frequency)",
  "difficulty": "Medium",
  "tags": [
    "Embeddings",
    "Matrix Operations"
  ],
  "descriptionMarkdown": "Task: Implement TF-IDF (Term Frequency-Inverse Document Frequency)\n\nYour task is to implement a function that computes the TF-IDF scores for a query against a given corpus of documents.\n\nFunction Signature\n- compute_tf_idf(corpus, query)\n  - corpus: A list of documents, where each document is a list of words.\n  - query: A list of words for which you want to compute the TF-IDF scores.\n\nOutput\n- Return a list of lists containing the TF-IDF scores for the query words in each document, rounded to five decimal places.\n\nImportant Considerations\n- Handling Division by Zero (IDF Smoothing): When computing IDF, account for terms that do not appear in any document (df = 0). Use smoothing, e.g., add 1 to both numerator and denominator. A robust choice is: idf = log((N + 1) / (df + 1)) + 1.\n- Empty Corpus: If the corpus is empty, return an empty list (or raise an error). This ensures robustness.\n- Edge Cases: Handle query terms not present in the corpus, documents with no words (avoid division by zero in TF), and extremely large/small term or document frequencies.\n\nExample\nInput:\n\ncorpus = [\n  [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n  [\"the\", \"dog\", \"chased\", \"the\", \"cat\"],\n  [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"]\n]\nquery = [\"cat\"]\n\nprint(compute_tf_idf(corpus, query))\n\nOutput:\n\n[[0.21461], [0.25754], [0.0]]\n\nReasoning: The TF-IDF scores for the word \"cat\" in each document are computed and rounded to five decimal places.",
  "solutionExplanation": "We compute TF-IDF per query term for each document. Term Frequency (TF) is the frequency of a term in a document normalized by document length: tf(t, d) = count(t in d) / |d|. This ensures documents of different lengths are comparable and avoids inflating scores for longer documents. For empty documents (|d| = 0), TF is taken as 0 to prevent division by zero.\n\nInverse Document Frequency (IDF) captures how informative a term is across the corpus. To avoid division by zero and handle unseen terms, we use a smoothed variant: idf(t) = log((N + 1) / (df(t) + 1)) + 1, where N is the number of documents and df(t) is the number of documents containing term t. This smoothing matches the example outputs and ensures stability for df = 0.\n\nFor each document, we build a small vector of TF values for the query terms, multiply elementwise by the precomputed IDF vector, and round the results to five decimals. We implement the arithmetic using PyTorch tensors for clarity and efficiency, while leveraging lightweight Python structures (e.g., Counter) to compute counts.",
  "solutionCode": "import torch\nfrom collections import Counter\n\n\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Compute TF-IDF scores for the given query terms across a corpus of documents.\n\n    Args:\n        corpus (List[List[str]]): List of documents, each a list of tokens.\n        query (List[str]): List of query terms to compute TF-IDF for.\n\n    Returns:\n        List[List[float]]: For each document, a list of TF-IDF scores for the query terms,\n                           rounded to five decimals.\n    \"\"\"\n    # Handle empty corpus: return empty result for robustness\n    if corpus is None or len(corpus) == 0:\n        return []\n\n    N = len(corpus)\n\n    # Precompute document sets to efficiently get document frequency (df)\n    doc_sets = []\n    for doc in corpus:\n        # Ensure each document is a list; if empty or not iterable, treat as empty\n        try:\n            doc_sets.append(set(doc))\n        except Exception:\n            doc_sets.append(set())\n\n    # Compute document frequency for each query term\n    df_list = []\n    for term in query:\n        df = sum(1 for ds in doc_sets if term in ds)\n        df_list.append(df)\n\n    # Convert to PyTorch tensors and compute smoothed IDF\n    # idf = log((N + 1) / (df + 1)) + 1\n    df_tensor = torch.tensor(df_list, dtype=torch.float32)\n    N_scalar = torch.tensor(float(N), dtype=torch.float32)\n    idf = torch.log((N_scalar + 1.0) / (df_tensor + 1.0)) + 1.0\n\n    results = []\n    for doc in corpus:\n        # Safe handling of potentially invalid docs\n        try:\n            counts = Counter(doc)\n            doc_len = len(doc)\n        except Exception:\n            counts = Counter()\n            doc_len = 0\n\n        # Build TF vector for the query terms\n        if doc_len > 0:\n            tf_vals = [counts.get(term, 0) / doc_len for term in query]\n        else:\n            tf_vals = [0.0 for _ in query]\n\n        tf = torch.tensor(tf_vals, dtype=torch.float32)\n\n        # TF-IDF for this document (vectorized over query terms)\n        tfidf = tf * idf\n\n        # Round to five decimals as required\n        tfidf_row = [round(float(v), 5) for v in tfidf.tolist()]\n        results.append(tfidf_row)\n\n    return results\n\n\ndef solution():\n    # Example usage\n    corpus = [\n        [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n        [\"the\", \"dog\", \"chased\", \"the\", \"cat\"],\n        [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"]\n    ]\n    query = [\"cat\"]\n    scores = compute_tf_idf(corpus, query)\n    print(scores)  # Expected: [[0.21461], [0.25754], [0.0]]\n\n\nif __name__ == \"__main__\":\n    solution()\n",
  "timeComplexity": "O(T + N*Q)",
  "spaceComplexity": "O(N + Q)",
  "platform": "deepml"
};
