import { Problem } from '../../types';

export const DM_148_ADAMAX_OPTIMIZER: Problem = {
  "id": "dm_148_adamax_optimizer",
  "title": "Adamax Optimizer",
  "difficulty": "Easy",
  "tags": [
    "Optimization",
    "Gradient Descent",
    "Neural Networks"
  ],
  "descriptionMarkdown": "Implement the Adamax optimizer update step function. The function should take the current parameter value, gradient, and moving averages (first moment m and infinity norm u) as inputs, along with the timestep t. It must return the updated parameter value and the new moving averages. The implementation should:\n\n- Support both scalar and array (vector/tensor) inputs\n- Include bias correction for the first moment\n- Use the Adamax specific infinity norm update\n\nExample:\n\n- Input: parameter = 1.0, grad = 0.1, m = 0.0, u = 0.0, t = 1\n- Output: (0.998, 0.01, 0.1)\n\nAdamax uses the infinity norm of past gradients and bias-corrected first moment for robust updates.",
  "solutionExplanation": "Adamax is a variant of Adam that replaces the second moment (exponential moving average of squared gradients) with the infinity norm (the maximum of the exponentially decayed past infinity norm and the absolute value of the current gradient). The updates are:\n\n- First moment: m_t = \u03b21 \u00b7 m_{t\u22121} + (1 \u2212 \u03b21) \u00b7 g_t\n- Infinity norm: u_t = max(\u03b22 \u00b7 u_{t\u22121}, |g_t|)\n\nTo correct the initialization bias of m_t towards zero, Adamax applies bias correction using m\u0302_t = m_t / (1 \u2212 \u03b21^t). The parameter update then becomes:\n\n\u03b8_t = \u03b8_{t\u22121} \u2212 \u03b1 \u00b7 m\u0302_t / (u_t + \u03b5)\n\nThis formulation is numerically stable due to \u03b5 and robust to large gradient spikes because of the infinity norm u_t. The function below implements these steps in PyTorch, supports scalars and tensors, and returns the updated parameter, m, and u.",
  "solutionCode": "import torch\\nimport torch.nn as nn\\nfrom typing import Tuple, Union\\n\\n@torch.no_grad()\\ndef adamax_update(parameter: Union[float, torch.Tensor],\\n                  grad: Union[float, torch.Tensor],\\n                  m: Union[float, torch.Tensor],\\n                  u: Union[float, torch.Tensor],\\n                  t: int,\\n                  learning_rate: float = 0.002,\\n                  beta1: float = 0.9,\\n                  beta2: float = 0.999,\\n                  epsilon: float = 1e-8) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\\n    \\\"\\\"\\\"\\n    Perform one Adamax optimizer update step.\\n\\n    Args:\\n        parameter: Current parameter(s), scalar or tensor.\\n        grad: Current gradient(s), scalar or tensor broadcastable to parameter.\\n        m: First moment estimate, same shape as parameter.\\n        u: Infinity norm estimate, same shape as parameter.\\n        t: Current timestep (1-indexed).\\n        learning_rate: Step size (alpha).\\n        beta1: Exponential decay rate for the first moment.\\n        beta2: Exponential decay rate for the infinity norm.\\n        epsilon: Small constant for numerical stability.\\n\\n    Returns:\\n        A tuple: (updated_parameter, new_m, new_u), all as torch.Tensors.\\n    \\\"\\\"\\\"\\n    # Helper to convert inputs to tensors on the same device/dtype as parameter\\n    def _to_tensor(x, ref):\\n        if isinstance(x, torch.Tensor):\\n            return x\\n        if isinstance(ref, torch.Tensor):\\n            return torch.as_tensor(x, dtype=ref.dtype, device=ref.device)\\n        return torch.as_tensor(x, dtype=torch.float32)\\n\\n    param_t = _to_tensor(parameter, parameter)\\n    grad_t = _to_tensor(grad, param_t)\\n    m_t = _to_tensor(m, param_t)\\n    u_t = _to_tensor(u, param_t)\\n\\n    # First moment update\\n    new_m = beta1 * m_t + (1.0 - beta1) * grad_t\\n\\n    # Infinity norm update (element-wise max)\\n    # new_u = max(beta2 * u, |g|)\\n    new_u = torch.maximum(beta2 * u_t, grad_t.abs())\\n\\n    # Bias correction for first moment\\n    bias_correction = 1.0 - (beta1 ** float(t))\\n    m_hat = new_m / bias_correction\\n\\n    # Parameter update\\n    updated_param = param_t - learning_rate * (m_hat / (new_u + epsilon))\\n\\n    return updated_param, new_m, new_u\\n\\n\\ndef solution():\\n    # Example usage with scalars\\n    p, m, u = adamax_update(parameter=1.0, grad=0.1, m=0.0, u=0.0, t=1)\\n    # p ~= 0.998, m = 0.01, u = 0.1\\n    # Example usage with tensors\\n    param = torch.tensor([1.0, -2.0])\\n    grad = torch.tensor([0.1, -0.3])\\n    m0 = torch.zeros_like(param)\\n    u0 = torch.zeros_like(param)\\n    p2, m2, u2 = adamax_update(param, grad, m0, u0, t=1)\\n    return (p, m, u), (p2, m2, u2)\\n",
  "timeComplexity": "O(N)",
  "spaceComplexity": "O(N)",
  "platform": "deepml"
};
